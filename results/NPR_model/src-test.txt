public class IsoCamTest extends GdxTest { private static final int TARGET_WIDTH = 480 ; private static final float UNIT_TO_PIXEL = ( IsoCamTest . TARGET_WIDTH ) * 0.15F ; Texture texture ; OrthographicCamera cam ; SpriteBatch batch ; final Sprite [ ] [ ] sprites = new Sprite [ 10 ] [ 10 ] ; final Matrix4 matrix = new Matrix4 ( ) ; @ Override public void create ( ) { texture = new Texture ( files . internal ( "data/badlogicsmall.jpg" ) ) ; float unitsOnX = ( ( ( float ) ( Math . sqrt ( 2 ) ) ) * ( IsoCamTest . TARGET_WIDTH ) ) / ( IsoCamTest . UNIT_TO_PIXEL ) ; float pixelsOnX = ( graphics . getWidth ( ) ) / unitsOnX ; float unitsOnY = ( graphics . getHeight ( ) ) / pixelsOnX ; cam = new OrthographicCamera ( unitsOnX , unitsOnY , 25 ) ; <START_BUG> cam . position . mul ( 30 ) ; <END_BUG> cam . near = 1 ; cam . far = 1000 ; matrix . setToRotation ( new Vector3 ( 1 , 0 , 0 ) , 90 ) ; for ( int z = 0 ; z < 10 ; z ++ ) { for ( int x = 0 ; x < 10 ; x ++ ) { sprites [ x ] [ z ] = new Sprite ( texture ) ; sprites [ x ] [ z ] . setPosition ( x , z ) ; sprites [ x ] [ z ] . setSize ( 1 , 1 ) ; } } batch = new SpriteBatch ( ) ; input . setInputProcessor ( new IsoCamTest . IsoCamController ( cam ) ) ; } @ Override public void dispose ( ) { } @ Override public void render ( ) { } final Plane xzPlane = new Plane ( new Vector3 ( 0 , 1 , 0 ) , 0 ) ; final Vector3 intersection = new Vector3 ( ) ; Sprite lastSelectedTile = null ; private void checkTileTouched ( ) { } public class IsoCamController extends InputAdapter { final Plane xzPlane = new Plane ( new Vector3 ( 0 , 1 , 0 ) , 0 ) ; final Vector3 intersection = new Vector3 ( ) ; final Vector3 curr = new Vector3 ( ) ; final Vector3 last = new Vector3 ( ( - 1 ) , ( - 1 ) , ( - 1 ) ) ; final Vector3 delta = new Vector3 ( ) ; final Camera camera ; public IsoCamController ( Camera camera ) { } @ Override public boolean touchDragged ( int x , int y , int pointer ) { } @ Override public boolean touchUp ( int x , int y , int pointer , int button ) { } } @ Override public boolean needsGL20 ( ) { } }
public class MetagunDesktop { public static void main ( String [ ] argv ) { <START_BUG> new com . badlogic . gdx . backends . lwjgl . LwjglApplication ( new Metagun ( ) , "Metagun" , 320 , 240 , false ) ; <END_BUG> } }
public class SimpleFacetsTests extends AbstractNodesTests { private Client client ; @ BeforeClass public void createNodes ( ) throws Exception { } protected int numberOfShards ( ) { } protected int numberOfNodes ( ) { } protected int numberOfRuns ( ) { } @ AfterClass public void closeNodes ( ) { } protected Client getClient ( ) { } @ Test public void testBinaryFacet ( ) throws Exception { try { client . admin ( ) . indices ( ) . prepareDelete ( "test" ) . execute ( ) . actionGet ( ) ; } catch ( Exception e ) { } client . admin ( ) . indices ( ) . prepareCreate ( "test" ) . execute ( ) . actionGet ( ) ; client . admin ( ) . cluster ( ) . prepareHealth ( ) . setWaitForGreenStatus ( ) . execute ( ) . actionGet ( ) ; client . admin ( ) . cluster ( ) . prepareHealth ( ) . setWaitForGreenStatus ( ) . execute ( ) . actionGet ( ) ; client . prepareIndex ( "test" , "type1" ) . setSource ( jsonBuilder ( ) . startObject ( ) . field ( "tag" , "green" ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; client . admin ( ) . indices ( ) . prepareFlush ( ) . setRefresh ( true ) . execute ( ) . actionGet ( ) ; client . prepareIndex ( "test" , "type1" ) . setSource ( jsonBuilder ( ) . startObject ( ) . field ( "tag" , "blue" ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; client . admin ( ) . indices ( ) . prepareRefresh ( ) . execute ( ) . actionGet ( ) ; for ( int i = 0 ; i < ( numberOfRuns ( ) ) ; i ++ ) { <START_BUG> SearchResponse searchResponse = client . prepareSearch ( ) . setSearchType ( COUNT ) . setFacets ( XContentFactory . jsonBuilder ( ) . startObject ( ) . startObject ( "facet1" ) . startObject ( "terms" ) . field ( "field" , "tag" ) . endObject ( ) . endObject ( ) . endObject ( ) . copiedBytes ( ) ) . execute ( ) . actionGet ( ) ; <END_BUG> assertThat ( searchResponse . hits ( ) . totalHits ( ) , equalTo ( 2L ) ) ; assertThat ( searchResponse . hits ( ) . hits ( ) . length , equalTo ( 0 ) ) ; TermsFacet facet = searchResponse . facets ( ) . facet ( "facet1" ) ; assertThat ( facet . name ( ) , equalTo ( "facet1" ) ) ; assertThat ( facet . entries ( ) . size ( ) , equalTo ( 2 ) ) ; assertThat ( facet . entries ( ) . get ( 0 ) . term ( ) , anyOf ( equalTo ( "green" ) , equalTo ( "blue" ) ) ) ; assertThat ( facet . entries ( ) . get ( 0 ) . count ( ) , equalTo ( 1 ) ) ; assertThat ( facet . entries ( ) . get ( 1 ) . term ( ) , anyOf ( equalTo ( "green" ) , equalTo ( "blue" ) ) ) ; assertThat ( facet . entries ( ) . get ( 1 ) . count ( ) , equalTo ( 1 ) ) ; } } @ Test public void testSearchFilter ( ) throws Exception { } @ Test public void testFacetsWithSize0 ( ) throws Exception { } @ Test public void testTermsIndexFacet ( ) throws Exception { } @ Test public void testFilterFacets ( ) throws Exception { } @ Test public void testTermsFacetsMissing ( ) throws Exception { } @ Test public void testTermsFacetsNoHint ( ) throws Exception { } @ Test public void testTermsFacetsMapHint ( ) throws Exception { } private void testTermsFacets ( String executionHint ) throws Exception { } @ Test public void testTermFacetWithEqualTermDistribution ( ) throws Exception { } @ Test public void testStatsFacets ( ) throws Exception { } @ Test public void testHistoFacetEdge ( ) throws Exception { } @ Test public void testHistoFacets ( ) throws Exception { } @ Test public void testRangeFacets ( ) throws Exception { } @ Test public void testDateHistoFacets ( ) throws Exception { } @ Test public void testTermsStatsFacets ( ) throws Exception { } @ Test public void testNumericTermsStatsFacets ( ) throws Exception { } @ Test public void testTermsStatsFacets2 ( ) throws Exception { } @ Test public void testQueryFacet ( ) throws Exception { } private long utcTimeInMillis ( String time ) { } private long timeInMillis ( String time , DateTimeZone zone ) { } }
public class JoglDebugStarter { public static void main ( String [ ] argv ) { <START_BUG> new com . badlogic . gdx . backends . jogl . JoglApplication ( new com . badlogic . gdx . tests . Box2DTest ( ) , "Debug<seq2seq4repair_space>Test" , 480 , 320 , false ) ; <END_BUG> } }
public class TransportShardMultiGetAction extends TransportShardSingleOperationAction < MultiGetShardRequest , MultiGetShardResponse > { private final IndicesService indicesService ; private final boolean realtime ; @ Inject public TransportShardMultiGetAction ( Settings settings , ClusterService clusterService , TransportService transportService , IndicesService indicesService , ThreadPool threadPool ) { } @ Override protected String executor ( ) { } @ Override protected String transportAction ( ) { } @ Override protected MultiGetShardRequest newRequest ( ) { } @ Override protected MultiGetShardResponse newResponse ( ) { } @ Override protected ClusterBlockException checkGlobalBlock ( ClusterState state , MultiGetShardRequest request ) { } @ Override protected ClusterBlockException checkRequestBlock ( ClusterState state , MultiGetShardRequest request ) { } @ Override protected ShardIterator shards ( ClusterState state , MultiGetShardRequest request ) { } @ Override protected void resolveRequest ( ClusterState state , MultiGetShardRequest request ) { } @ Override protected MultiGetShardResponse shardOperation ( MultiGetShardRequest request , int shardId ) throws ElasticSearchException { IndexService indexService = indicesService . indexServiceSafe ( request . index ( ) ) ; IndexShard indexShard = indexService . shardSafe ( shardId ) ; if ( ( request . refresh ( ) ) && ( ! ( request . realtime ( ) ) ) ) { <START_BUG> indexShard . refresh ( new Engine . Refresh ( false ) ) ; <END_BUG> } MultiGetShardResponse response = new MultiGetShardResponse ( ) ; for ( int i = 0 ; i < ( request . locations . size ( ) ) ; i ++ ) { String type = request . types . get ( i ) ; String id = request . ids . get ( i ) ; String [ ] fields = request . fields . get ( i ) ; long version = request . versions . get ( i ) ; VersionType versionType = request . versionTypes . get ( i ) ; if ( versionType == null ) { versionType = VersionType . INTERNAL ; } FetchSourceContext fetchSourceContext = request . fetchSourceContexts . get ( i ) ; try { GetResult getResult = indexShard . getService ( ) . get ( type , id , fields , request . realtime ( ) , version , versionType , fetchSourceContext ) ; response . add ( request . locations . get ( i ) , new GetResponse ( getResult ) ) ; } catch ( Exception e ) { logger . debug ( "[{}][{}]<seq2seq4repair_space>failed<seq2seq4repair_space>to<seq2seq4repair_space>execute<seq2seq4repair_space>multi_get<seq2seq4repair_space>for<seq2seq4repair_space>[{}]/[{}]" , e , request . index ( ) , shardId , type , id ) ; response . add ( request . locations . get ( i ) , new MultiGetResponse . Failure ( request . index ( ) , type , id , ExceptionsHelper . detailedMessage ( e ) ) ) ; } } return response ; } }
public class ByteFieldMapper extends NumberFieldMapper < Byte > { public static final String CONTENT_TYPE = "byte" ; public static class Defaults extends NumberFieldMapper . Defaults { public static final FieldType BYTE_FIELD_TYPE = new FieldType ( NumberFieldMapper . Defaults . NUMBER_FIELD_TYPE ) ; public static final Byte NULL_VALUE = null ; } public static class Builder extends NumberFieldMapper . Builder < ByteFieldMapper . Builder , ByteFieldMapper > { protected Byte nullValue = ByteFieldMapper . Defaults . NULL_VALUE ; public Builder ( String name ) { } public ByteFieldMapper . Builder nullValue ( byte nullValue ) { } @ Override public ByteFieldMapper build ( BuilderContext context ) { <START_BUG> fieldType . setOmitNorms ( ( ( fieldType . omitNorms ( ) ) || ( ( boost ) != 1.0F ) ) ) ; <END_BUG> ByteFieldMapper fieldMapper = new ByteFieldMapper ( buildNames ( context ) , precisionStep , fuzzyFactor , boost , fieldType , nullValue , ignoreMalformed ( context ) ) ; fieldMapper . includeInAll ( includeInAll ) ; return fieldMapper ; } } public static class TypeParser implements Mapper . TypeParser { @ Override public Mapper . Builder parse ( String name , Map < String , Object > node , ParserContext parserContext ) throws MapperParsingException { } } private Byte nullValue ; private String nullValueAsString ; protected ByteFieldMapper ( Names names , int precisionStep , String fuzzyFactor , float boost , FieldType fieldType , Byte nullValue , Explicit < Boolean > ignoreMalformed ) { } @ Override protected int maxPrecisionStep ( ) { } @ Override public Byte value ( Field field ) { } @ Override public Byte valueFromString ( String value ) { } @ Override public String indexedValue ( String value ) { } @ Override public Query fuzzyQuery ( String value , String minSim , int prefixLength , int maxExpansions , boolean transpositions ) { } @ Override public Query fuzzyQuery ( String value , double minSim , int prefixLength , int maxExpansions , boolean transpositions ) { } @ Override public Query fieldQuery ( String value , @ Nullable QueryParseContext context ) { } @ Override public Query rangeQuery ( String lowerTerm , String upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter fieldFilter ( String value , @ Nullable QueryParseContext context ) { } @ Override public Filter rangeFilter ( String lowerTerm , String upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter rangeFilter ( FieldDataCache fieldDataCache , String lowerTerm , String upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter nullValueFilter ( ) { } @ Override protected boolean customBoost ( ) { } @ Override protected Field innerParseCreateField ( ParseContext context ) throws IOException { } @ Override public FieldDataType fieldDataType ( ) { } @ Override protected String contentType ( ) { } @ Override public void merge ( Mapper mergeWith , MergeContext mergeContext ) throws MergeMappingException { } @ Override protected void doXContentBody ( XContentBuilder builder ) throws IOException { } public static class CustomByteNumericField extends CustomNumericField { private final byte number ; private final NumberFieldMapper mapper ; public CustomByteNumericField ( NumberFieldMapper mapper , byte number , FieldType fieldType ) { } @ Override public TokenStream tokenStream ( Analyzer analyzer ) { } @ Override public String numericAsString ( ) { } } }
public class GameOver extends InvadersScreen { private final SpriteBatch spriteBatch ; private final Texture background ; private final Texture logo ; private final BitmapFont font ; private boolean isDone = false ; private final Matrix4 viewMatrix = new Matrix4 ( ) ; private final Matrix4 transformMatrix = new Matrix4 ( ) ; public GameOver ( ) { } @ Override public void dispose ( ) { } @ Override public boolean isDone ( ) { } @ Override public void draw ( float delta ) { } @ Override public void update ( float delta ) { <START_BUG> if ( input . isTouched ( ) ) { <END_BUG> isDone = true ; } } }
public class FilterAggregationBuilder extends AggregationBuilder < FilterAggregationBuilder > { private FilterBuilder filter ; public FilterAggregationBuilder ( String name ) { } public FilterAggregationBuilder filter ( FilterBuilder filter ) { } @ Override protected XContentBuilder internalXContent ( XContentBuilder builder , Params params ) throws IOException { if ( ( filter ) == null ) { <START_BUG> throw new org . elasticsearch . search . builder . SearchSourceBuilderException ( ( ( "filter<seq2seq4repair_space>must<seq2seq4repair_space>be<seq2seq4repair_space>set<seq2seq4repair_space>on<seq2seq4repair_space>filter<seq2seq4repair_space>aggregation<seq2seq4repair_space>[" + ( getName ( ) ) ) + "]" ) ) ; <END_BUG> } filter . toXContent ( builder , params ) ; return builder ; } }
public class BouncyDesktop { public static void main ( String [ ] argv ) { <START_BUG> new com . badlogic . gdx . backends . lwjgl . LwjglApplication ( new Bouncy ( ) , "Bouncy" , 320 , 480 ) ; <END_BUG> } }
public class WaterRipples implements InputListener , GdxTest { static final short WIDTH = 50 ; static final short HEIGHT = 50 ; static final float INV_WIDTH = 1.0F / ( WaterRipples . WIDTH ) ; static final float INV_HEIGHT = 1.0F / ( WaterRipples . HEIGHT ) ; static final float DAMPING = 0.9F ; static final float DISPLACEMENT = - 10 ; static final float TICK = 0.033F ; static final int RADIUS = 3 ; float accum ; boolean initialized = false ; PerspectiveCamera camera ; SpriteBatch batch ; Font font ; Mesh mesh ; Texture texture ; Plane plane = new Plane ( new Vector3 ( ) , new Vector3 ( 1 , 0 , 0 ) , new Vector3 ( 0 , 1 , 0 ) ) ; Vector3 point = new Vector3 ( ) ; float [ ] [ ] last ; float [ ] [ ] curr ; float [ ] [ ] intp ; float [ ] vertices ; @ Override public void surfaceCreated ( ) { if ( ! ( initialized ) ) { camera = new PerspectiveCamera ( ) ; camera . getPosition ( ) . set ( ( ( WaterRipples . WIDTH ) / 2.0F ) , ( ( WaterRipples . HEIGHT ) / 2.0F ) , ( ( WaterRipples . WIDTH ) / 2.0F ) ) ; camera . setViewport ( graphics . getWidth ( ) , graphics . getWidth ( ) ) ; camera . setFov ( 90 ) ; camera . setNear ( 0.1F ) ; camera . setFar ( 1000 ) ; last = new float [ ( WaterRipples . WIDTH ) + 1 ] [ ( WaterRipples . HEIGHT ) + 1 ] ; curr = new float [ ( WaterRipples . WIDTH ) + 1 ] [ ( WaterRipples . HEIGHT ) + 1 ] ; intp = new float [ ( WaterRipples . WIDTH ) + 1 ] [ ( WaterRipples . HEIGHT ) + 1 ] ; vertices = new float [ ( ( ( WaterRipples . WIDTH ) + 1 ) * ( ( WaterRipples . HEIGHT ) + 1 ) ) * 5 ] ; <START_BUG> mesh = new Mesh ( false , false , ( ( ( WaterRipples . WIDTH ) + 1 ) * ( ( WaterRipples . HEIGHT ) + 1 ) ) , ( ( ( WaterRipples . WIDTH ) * ( WaterRipples . HEIGHT ) ) * 6 ) , new com . badlogic . gdx . graphics . VertexAttribute ( Usage . Position , 3 , "a_Position" ) , new com . badlogic . gdx . graphics . VertexAttribute ( Usage . TextureCoordinates , 2 , "a_texCoords" ) ) ; <END_BUG> texture = graphics . newTexture ( files . getFileHandle ( "data/stones.jpg" , Internal ) , Linear , Linear , ClampToEdge , ClampToEdge ) ; createIndices ( ) ; updateVertices ( curr ) ; initialized = true ; batch = new SpriteBatch ( ) ; font = graphics . newFont ( "Arial" , 12 , Plain ) ; input . addInputListener ( this ) ; } } private void createIndices ( ) { } private void updateVertices ( float [ ] [ ] curr ) { } private void updateWater ( ) { } private void interpolateWater ( float alpha ) { } private void touchWater ( Vector3 point ) { } long lastTick = System . nanoTime ( ) ; Random rand = new Random ( ) ; @ Override public void render ( ) { } @ Override public void dispose ( ) { } @ Override public boolean keyDown ( int keycode ) { } @ Override public boolean keyUp ( int keycode ) { } @ Override public boolean keyTyped ( char character ) { } @ Override public boolean touchDown ( int x , int y , int pointer ) { } @ Override public boolean touchUp ( int x , int y , int pointer ) { } @ Override public boolean touchDragged ( int x , int y , int pointer ) { } @ Override public void surfaceChanged ( int width , int height ) { } @ Override public boolean needsGL20 ( ) { } }
public class CommitPoints implements Iterable < CommitPoint > { private final ImmutableList < CommitPoint > commitPoints ; public CommitPoints ( List < CommitPoint > commitPoints ) { } public ImmutableList < CommitPoint > commits ( ) { } public boolean hasVersion ( long version ) { } public FileInfo findPhysicalIndexFile ( String physicalName ) { } public FileInfo findNameFile ( String name ) { } @ Override public Iterator < CommitPoint > iterator ( ) { } public static byte [ ] toXContent ( CommitPoint commitPoint ) throws Exception { } public static CommitPoint fromXContent ( byte [ ] data ) throws Exception { XContentParser parser = XContentFactory . xContent ( JSON ) . createParser ( data ) ; try { String currentFieldName = null ; XContentParser . Token token = parser . nextToken ( ) ; if ( token == null ) { throw new IOException ( "No<seq2seq4repair_space>commit<seq2seq4repair_space>point<seq2seq4repair_space>data" ) ; } long version = - 1 ; String name = null ; CommitPoint . Type type = null ; List < CommitPoint . FileInfo > indexFiles = Lists . newArrayList ( ) ; List < CommitPoint . FileInfo > translogFiles = Lists . newArrayList ( ) ; while ( ( token = parser . nextToken ( ) ) != ( Token . END_OBJECT ) ) { if ( token == ( Token . FIELD_NAME ) ) { currentFieldName = parser . currentName ( ) ; } else if ( token == ( Token . START_OBJECT ) ) { List < CommitPoint . FileInfo > files = null ; if ( ( "index_files" . equals ( currentFieldName ) ) || ( "indexFiles" . equals ( currentFieldName ) ) ) { files = indexFiles ; } else if ( ( "translog_files" . equals ( currentFieldName ) ) || ( "translogFiles" . equals ( currentFieldName ) ) ) { files = translogFiles ; } else { throw new IOException ( ( ( "Can't<seq2seq4repair_space>handle<seq2seq4repair_space>object<seq2seq4repair_space>with<seq2seq4repair_space>name<seq2seq4repair_space>[" + currentFieldName ) + "]" ) ) ; } while ( ( token = parser . nextToken ( ) ) != ( Token . END_OBJECT ) ) { if ( token == ( Token . FIELD_NAME ) ) { currentFieldName = parser . currentName ( ) ; } else if ( token == ( Token . START_OBJECT ) ) { String fileName = currentFieldName ; String physicalName = null ; long size = - 1 ; while ( ( token = parser . nextToken ( ) ) != ( Token . END_OBJECT ) ) { if ( token == ( Token . FIELD_NAME ) ) { currentFieldName = parser . currentName ( ) ; } else if ( token . isValue ( ) ) { if ( ( "physical_name" . equals ( currentFieldName ) ) || ( "physicalName" . equals ( currentFieldName ) ) ) { physicalName = parser . text ( ) ; } else if ( "length" . equals ( currentFieldName ) ) { size = parser . longValue ( ) ; } } } if ( physicalName == null ) { throw new IOException ( ( ( "Malformed<seq2seq4repair_space>commit,<seq2seq4repair_space>missing<seq2seq4repair_space>physical_name<seq2seq4repair_space>for<seq2seq4repair_space>[" + fileName ) + "]" ) ) ; } if ( size == ( - 1 ) ) { <START_BUG> throw new IOException ( ( ( "Malformed<seq2seq4repair_space>commit,<seq2seq4repair_space>missing<seq2seq4repair_space>size<seq2seq4repair_space>for<seq2seq4repair_space>[" + fileName ) + "]" ) ) ; <END_BUG> } files . add ( new CommitPoint . FileInfo ( fileName , physicalName , size ) ) ; } } } else if ( token . isValue ( ) ) { if ( "version" . equals ( currentFieldName ) ) { version = parser . longValue ( ) ; } else if ( "name" . equals ( currentFieldName ) ) { name = parser . text ( ) ; } else if ( "type" . equals ( currentFieldName ) ) { type = Type . valueOf ( parser . text ( ) ) ; } } } if ( version == ( - 1 ) ) { throw new IOException ( "Malformed<seq2seq4repair_space>commit,<seq2seq4repair_space>missing<seq2seq4repair_space>version" ) ; } if ( name == null ) { throw new IOException ( "Malformed<seq2seq4repair_space>commit,<seq2seq4repair_space>missing<seq2seq4repair_space>name" ) ; } if ( type == null ) { throw new IOException ( "Malformed<seq2seq4repair_space>commit,<seq2seq4repair_space>missing<seq2seq4repair_space>type" ) ; } return new CommitPoint ( version , name , type , indexFiles , translogFiles ) ; } finally { parser . close ( ) ; } } }
final class AndroidGraphics implements Renderer , Graphics { protected final GLSurfaceView view ; private AndroidInput input ; protected RenderListener listener ; protected int width ; protected int height ; protected AndroidApplication app ; protected GL10 gl10 ; protected GL11 gl11 ; protected GL20 gl20 ; private long lastFrameTime = System . nanoTime ( ) ; private float deltaTime = 0 ; private WindowedMean mean = new WindowedMean ( 5 ) ; private boolean dispose = false ; public AndroidGraphics ( AndroidApplication activity , boolean useGL2IfAvailable ) { } protected void setInput ( AndroidInput input ) { } private boolean checkGL20 ( Activity context ) { } @ Override public GL10 getGL10 ( ) { } @ Override public GL11 getGL11 ( ) { } @ Override public GL20 getGL20 ( ) { } @ Override public int getHeight ( ) { } @ Override public int getWidth ( ) { } @ Override public boolean isGL11Available ( ) { } @ Override public boolean isGL20Available ( ) { } @ Override public Font newFont ( String fontName , int size , FontStyle style , boolean managed ) { } @ Override public Font newFont ( FileHandle file , int size , FontStyle style , boolean managed ) { } @ Override public Pixmap newPixmap ( int width , int height , Format format ) { } @ Override public Pixmap newPixmap ( InputStream in ) { } @ Override public Pixmap newPixmap ( FileHandle file ) { } @ Override public Pixmap newPixmap ( Object nativePixmap ) { } @ Override public Texture newTexture ( int width , int height , Pixmap . Format format , TextureFilter minFilter , TextureFilter magFilter , TextureWrap uWrap , TextureWrap vWrap , boolean managed ) { } @ Override public Texture newTexture ( Pixmap pixmap , TextureFilter minFilter , TextureFilter magFilter , TextureWrap uWrap , TextureWrap vWrap , boolean managed ) { } @ Override public void setRenderListener ( RenderListener listener ) { synchronized ( this ) { if ( ( this . listener ) != null ) <START_BUG> listener . dispose ( app ) ; <END_BUG> this . listener = listener ; } } @ Override public void onDrawFrame ( javax . microedition . khronos . opengles . GL10 gl ) { } private void setupGL ( javax . microedition . khronos . opengles . GL10 gl ) { } @ Override public void onSurfaceChanged ( javax . microedition . khronos . opengles . GL10 gl , int width , int height ) { } @ Override public void onSurfaceCreated ( javax . microedition . khronos . opengles . GL10 gl , EGLConfig config ) { } @ Override public float getDeltaTime ( ) { } public void disposeRenderListener ( ) { } @ Override public GraphicsType getType ( ) { } }
public class FilterBrowseActivity extends RoboFragmentActivity { @ Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ) ; <START_BUG> setTitle ( getString ( saved_filters_title ) ) ; <END_BUG> setContentView ( issue_filter_list ) ; } }
public class Vector3 implements Vector < Vector3 > , Serializable { private static final long serialVersionUID = 3840054589595372522L ; public float x ; public float y ; public float z ; public static final Vector3 tmp = new Vector3 ( ) ; public static final Vector3 tmp2 = new Vector3 ( ) ; public static final Vector3 tmp3 = new Vector3 ( ) ; public static final Vector3 X = new Vector3 ( 1 , 0 , 0 ) ; public static final Vector3 Y = new Vector3 ( 0 , 1 , 0 ) ; public static final Vector3 Z = new Vector3 ( 0 , 0 , 1 ) ; public static final Vector3 Zero = new Vector3 ( 0 , 0 , 0 ) ; private static final Matrix4 tmpMat = new Matrix4 ( ) ; public Vector3 ( ) { } public Vector3 ( float x , float y , float z ) { } public Vector3 ( final Vector3 vector ) { } public Vector3 ( final float [ ] values ) { } public Vector3 set ( float x , float y , float z ) { } public Vector3 set ( final Vector3 vector ) { } public Vector3 set ( final float [ ] values ) { } public Vector3 cpy ( ) { } public Vector3 tmp ( ) { } public Vector3 tmp2 ( ) { } Vector3 tmp3 ( ) { } public Vector3 add ( final Vector3 vector ) { } public Vector3 add ( float x , float y , float z ) { } public Vector3 add ( float values ) { } public Vector3 sub ( final Vector3 a_vec ) { } public Vector3 sub ( float x , float y , float z ) { } public Vector3 sub ( float value ) { } public Vector3 mul ( float value ) { } public Vector3 mul ( final Vector3 other ) { } public Vector3 mul ( float vx , float vy , float vz ) { } public Vector3 div ( float value ) { } public Vector3 div ( float vx , float vy , float vz ) { } public Vector3 div ( final Vector3 other ) { } public float len ( ) { } public float len2 ( ) { } public boolean idt ( final Vector3 vector ) { } public float dst ( final Vector3 vector ) { } public Vector3 nor ( ) { } public static float dot ( float x1 , float y1 , float z1 , float x2 , float y2 , float z2 ) { } public float dot ( final Vector3 vector ) { } public Vector3 crs ( final Vector3 vector ) { } public Vector3 crs ( float x , float y , float z ) { } public Vector3 mul ( final Matrix4 matrix ) { } public Vector3 prj ( final Matrix4 matrix ) { } public Vector3 rot ( final Matrix4 matrix ) { } public Vector3 rotate ( float angle , float axisX , float axisY , float axisZ ) { } public Vector3 rotate ( final Vector3 axis , float angle ) { } public boolean isUnit ( ) { } public boolean isZero ( ) { } public Vector3 lerp ( final Vector3 target , float alpha ) { } public Vector3 slerp ( final Vector3 target , float alpha ) { } public String toString ( ) { } public float dot ( float x , float y , float z ) { } public float dst2 ( Vector3 point ) { } public Vector3 limit ( float limit ) { } public Vector3 clamp ( float min , float max ) { } public float dst2 ( float x , float y , float z ) { final float a = x - ( this . x ) ; final float b = y - ( this . y ) ; final float c = z - ( this . z ) ; <START_BUG> return ( ( a * a ) + ( b * b ) ) + ( c * b ) ; <END_BUG> } public float dst ( float x , float y , float z ) { } @ Override public int hashCode ( ) { } @ Override public boolean equals ( Object obj ) { } public boolean epsilonEquals ( final Vector3 obj , float epsilon ) { } public boolean epsilonEquals ( float x , float y , float z , float epsilon ) { } public Vector3 scale ( float scalarX , float scalarY , float scalarZ ) { } }
public class ViewIssueActivity extends DialogFragmentActivity { private static final String ARG_COMMENTS = "comments" ; private static final int REQUEST_CODE_COMMENT = 1 ; private static final int REQUEST_CODE_LABELS = 2 ; private static final int REQUEST_CODE_MILESTONE = 3 ; private static final int REQUEST_CODE_ASSIGNEE = 4 ; private static final int REQUEST_CODE_CLOSE = 5 ; private static final int REQUEST_CODE_REOPEN = 6 ; private static final int REQUEST_CODE_EDIT = 7 ; public static Intent viewIssueIntentFor ( Issue issue ) { } @ Inject private ContextScopedProvider < IssueService > service ; @ Inject private IssueStore store ; @ Inject private LabelService labelService ; @ Inject private MilestoneService milestoneService ; @ Inject private CollaboratorService collaboratorService ; @ Inject private AvatarHelper avatarHelper ; private RepositoryId repositoryId ; private RefreshAnimation refreshAnimation = new RefreshAnimation ( ) ; @ InjectExtra ( EXTRA_REPOSITORY_NAME ) private String repository ; @ InjectExtra ( EXTRA_REPOSITORY_OWNER ) private String repositoryOwner ; @ InjectExtra ( EXTRA_ISSUE_NUMBER ) private int issueNumber ; private Issue issue ; @ InjectExtra ( value = ViewIssueActivity . ARG_COMMENTS , optional = true ) private List < Comment > comments ; private LabelsDialog labelsDialog ; private MilestoneDialog milestoneDialog ; private AssigneeDialog assigneeDialog ; @ InjectView ( id . list ) private ListView list ; private IssueHeaderViewHolder header ; private View loadingView ; protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ) ; setContentView ( issue_view ) ; setTitle ( ( ( getString ( issue_title ) ) + ( issueNumber ) ) ) ; repositoryId = RepositoryId . create ( repositoryOwner , repository ) ; issue = store . getIssue ( repositoryId , issueNumber ) ; labelsDialog = new LabelsDialog ( this , ViewIssueActivity . REQUEST_CODE_LABELS , repositoryId , labelService ) ; milestoneDialog = new MilestoneDialog ( this , ViewIssueActivity . REQUEST_CODE_MILESTONE , repositoryId , milestoneService ) ; assigneeDialog = new AssigneeDialog ( this , ViewIssueActivity . REQUEST_CODE_ASSIGNEE , repositoryId , collaboratorService ) ; View headerView = getLayoutInflater ( ) . inflate ( issue_header , null ) ; headerView . findViewById ( ll_milestone ) . setOnClickListener ( new OnClickListener ( ) { public void onClick ( View v ) { if ( ( issue ) != null ) milestoneDialog . show ( issue . getMilestone ( ) ) ; } } ) ; headerView . findViewById ( ll_assignee ) . setOnClickListener ( new OnClickListener ( ) { public void onClick ( View v ) { if ( ( issue ) != null ) { User assignee = issue . getAssignee ( ) ; assigneeDialog . show ( ( assignee != null ? assignee . getLogin ( ) : null ) ) ; } } } ) ; headerView . findViewById ( ll_state ) . setOnClickListener ( new OnClickListener ( ) { public void onClick ( View v ) { if ( ( issue ) != null ) confirmEditState ( STATE_OPEN . equals ( issue . getState ( ) ) ) ; } } ) ; headerView . findViewById ( ll_labels ) . setOnClickListener ( new OnClickListener ( ) { public void onClick ( View v ) { if ( ( issue ) != null ) labelsDialog . show ( issue . getLabels ( ) ) ; } } ) ; header = new IssueHeaderViewHolder ( headerView , avatarHelper , getResources ( ) ) ; list . setFastScrollEnabled ( true ) ; list . addHeaderView ( headerView ) ; <START_BUG> loadingView = getLayoutInflater ( ) . inflate ( issue_load_item , null ) ; <END_BUG> if ( ( ( issue ) != null ) && ( ( comments ) != null ) ) updateList ( issue , comments ) ; else { if ( ( issue ) != null ) header . updateViewFor ( issue ) ; refreshIssue ( ) ; } } private void refreshIssue ( ) { } private void updateList ( Issue issue , List < Comment > comments ) { } @ SuppressWarnings ( "unchecked" ) private ViewHoldingListAdapter < Comment > getRootAdapter ( ) { } @ Override public boolean onCreateOptionsMenu ( Menu options ) { } @ Override public boolean onPrepareOptionsMenu ( Menu menu ) { } @ Override public boolean onOptionsItemSelected ( MenuItem item ) { } protected void onActivityResult ( int requestCode , int resultCode , Intent data ) { } private void createComment ( final String comment ) { } @ Override public void onDialogResult ( int requestCode , int resultCode , Bundle arguments ) { } private void confirmEditState ( boolean close ) { } private void editState ( final boolean close ) { } private void editLabels ( final String [ ] labels ) { } private void editMilestone ( final String title ) { } private void editAssignee ( final String user ) { } private void editBody ( final String title , final String body ) { } }
@ Test public class SimpleIndexQueryParserTests { private Injector injector ; private IndexQueryParserService queryParser ; @ BeforeClass public void setupQueryParser ( ) throws IOException { Settings settings = ImmutableSettings . settingsBuilder ( ) . put ( "index.cache.filter.type" , "none" ) . build ( ) ; Index index = new Index ( "test" ) ; injector = new ModulesBuilder ( ) . add ( new org . elasticsearch . index . codec . CodecModule ( settings ) , new org . elasticsearch . common . settings . SettingsModule ( settings ) , new org . elasticsearch . threadpool . ThreadPoolModule ( settings ) , new IndicesQueriesModule ( ) , new org . elasticsearch . script . ScriptModule ( settings ) , new MapperServiceModule ( ) , new org . elasticsearch . index . settings . IndexSettingsModule ( index , settings ) , new org . elasticsearch . index . cache . IndexCacheModule ( settings ) , new org . elasticsearch . index . analysis . AnalysisModule ( settings ) , new org . elasticsearch . index . engine . IndexEngineModule ( settings ) , new org . elasticsearch . index . similarity . SimilarityModule ( settings ) , new org . elasticsearch . index . query . IndexQueryParserModule ( settings ) , new org . elasticsearch . index . IndexNameModule ( index ) , new AbstractModule ( ) { @ Override protected void configure ( ) { bind ( ClusterService . class ) . toProvider ( Providers . of ( ( ( ClusterService ) ( null ) ) ) ) ; } } ) . createInjector ( ) ; String mapping = copyToStringFromClasspath ( "/org/elasticsearch/test/unit/index/query/mapping.json" ) ; <START_BUG> injector . getInstance ( MapperService . class ) . add ( "person" , mapping , true ) ; <END_BUG> injector . getInstance ( MapperService . class ) . documentMapper ( "person" ) . parse ( new org . elasticsearch . common . bytes . BytesArray ( copyToBytesFromClasspath ( "/org/elasticsearch/test/unit/index/query/data.json" ) ) ) ; this . queryParser = injector . getInstance ( IndexQueryParserService . class ) ; } @ AfterClass public void close ( ) { } private IndexQueryParserService queryParser ( ) throws IOException { } private BytesRef longToPrefixCoded ( long val ) { } @ Test public void testQueryStringBuilder ( ) throws Exception { } @ Test public void testQueryString ( ) throws Exception { } @ Test public void testQueryStringFields1Builder ( ) throws Exception { } @ Test public void testQueryStringFields1 ( ) throws Exception { } @ Test public void testQueryStringFieldsMatch ( ) throws Exception { } @ Test public void testQueryStringFields2Builder ( ) throws Exception { } @ Test public void testQueryStringFields2 ( ) throws Exception { } @ Test public void testQueryStringFields3Builder ( ) throws Exception { } @ Test public void testQueryStringFields3 ( ) throws Exception { } @ Test public void testMatchAllBuilder ( ) throws Exception { } @ Test public void testMatchAll ( ) throws Exception { } @ Test public void testDisMaxBuilder ( ) throws Exception { } @ Test public void testDisMax ( ) throws Exception { } @ Test public void testDisMax2 ( ) throws Exception { } @ Test public void testTermQueryBuilder ( ) throws IOException { } @ Test public void testTermQuery ( ) throws IOException { } @ Test public void testFuzzyQueryBuilder ( ) throws IOException { } @ Test public void testFuzzyQuery ( ) throws IOException { } @ Test public void testFuzzyQueryWithFieldsBuilder ( ) throws IOException { } @ Test public void testFuzzyQueryWithFields ( ) throws IOException { } @ Test public void testFuzzyQueryWithFields2 ( ) throws IOException { } @ Test public void testFieldQueryBuilder1 ( ) throws IOException { } @ Test public void testFieldQuery1 ( ) throws IOException { } @ Test public void testFieldQuery2 ( ) throws IOException { } @ Test public void testFieldQuery3 ( ) throws IOException { } @ Test public void testTextQuery1 ( ) throws IOException { } @ Test public void testTextQuery2 ( ) throws IOException { } @ Test public void testTextQuery3 ( ) throws IOException { } @ Test public void testTextQuery4 ( ) throws IOException { } @ Test public void testTextQuery4_2 ( ) throws IOException { } @ Test public void testTermWithBoostQueryBuilder ( ) throws IOException { } @ Test public void testTermWithBoostQuery ( ) throws IOException { } @ Test public void testPrefixQueryBuilder ( ) throws IOException { } @ Test public void testPrefixQuery ( ) throws IOException { } @ Test public void testPrefixBoostQuery ( ) throws IOException { } @ Test public void testPrefixFilteredQueryBuilder ( ) throws IOException { } @ Test public void testPrefixFilteredQuery ( ) throws IOException { } @ Test public void testPrefixNamedFilteredQuery ( ) throws IOException { } @ Test public void testPrefixQueryBoostQueryBuilder ( ) throws IOException { } @ Test public void testPrefixQueryBoostQuery ( ) throws IOException { } @ Test public void testPrefixQueryWithUnknownField ( ) throws IOException { } @ Test public void testWildcardQueryBuilder ( ) throws IOException { } @ Test public void testWildcardQuery ( ) throws IOException { } @ Test public void testWildcardBoostQuery ( ) throws IOException { } @ Test public void testRangeQueryBuilder ( ) throws IOException { } @ Test public void testRangeQuery ( ) throws IOException { } @ Test public void testRange2Query ( ) throws IOException { } @ Test public void testRangeFilteredQueryBuilder ( ) throws IOException { } @ Test public void testRangeFilteredQuery ( ) throws IOException { } @ Test public void testRangeNamedFilteredQuery ( ) throws IOException { } @ Test public void testNumericRangeFilteredQueryBuilder ( ) throws IOException { } @ Test public void testNumericRangeFilteredQuery ( ) throws IOException { } @ Test public void testBoolFilteredQuery ( ) throws IOException { } @ Test public void testAndFilteredQueryBuilder ( ) throws IOException { } @ Test public void testAndFilteredQuery ( ) throws IOException { } @ Test public void testAndNamedFilteredQuery ( ) throws IOException { } @ Test public void testAndFilteredQuery2 ( ) throws IOException { }
public class InternalBoundedCountHistogramFacet extends InternalHistogramFacet { private static final String STREAM_TYPE = "cBdHistogram" ; public static void registerStreams ( ) { } static Stream STREAM = new Stream ( ) { @ Override public Facet readFacet ( String type , StreamInput in ) throws IOException { } } ; @ Override public String streamType ( ) { } public static class CountEntry implements Entry { private final long key ; private final long count ; public CountEntry ( long key , long count ) { } @ Override public long key ( ) { } @ Override public long getKey ( ) { } @ Override public long count ( ) { } @ Override public long getCount ( ) { } @ Override public double total ( ) { } @ Override public double getTotal ( ) { } @ Override public long totalCount ( ) { } @ Override public long getTotalCount ( ) { } @ Override public double mean ( ) { } @ Override public double getMean ( ) { } @ Override public double min ( ) { } @ Override public double getMin ( ) { } @ Override public double max ( ) { } @ Override public double getMax ( ) { } } private String name ; ComparatorType comparatorType ; boolean cachedCounts ; int [ ] counts ; int size ; long interval ; long offset ; InternalBoundedCountHistogramFacet . CountEntry [ ] entries = null ; private InternalBoundedCountHistogramFacet ( ) { } public InternalBoundedCountHistogramFacet ( String name , ComparatorType comparatorType , long interval , long offset , int size , int [ ] counts , boolean cachedCounts ) { } @ Override public String name ( ) { } @ Override public String getName ( ) { } @ Override public String type ( ) { } @ Override public String getType ( ) { } @ Override public List < InternalBoundedCountHistogramFacet . CountEntry > entries ( ) { } @ Override public List < InternalBoundedCountHistogramFacet . CountEntry > getEntries ( ) { } @ Override public Iterator < Entry > iterator ( ) { } private InternalBoundedCountHistogramFacet . CountEntry [ ] computeEntries ( ) { } void releaseCache ( ) { } @ Override <START_BUG> public Facet reduce ( String name , List < Facet > facets ) { <END_BUG> if ( ( facets . size ( ) ) == 1 ) { InternalBoundedCountHistogramFacet firstHistoFacet = ( ( InternalBoundedCountHistogramFacet ) ( facets . get ( 0 ) ) ) ; if ( ( comparatorType ) != ( ComparatorType . KEY ) ) { Arrays . sort ( firstHistoFacet . entries , comparatorType . comparator ( ) ) ; } return facets . get ( 0 ) ; } InternalBoundedCountHistogramFacet firstHistoFacet = ( ( InternalBoundedCountHistogramFacet ) ( facets . get ( 0 ) ) ) ; for ( int i = 1 ; i < ( facets . size ( ) ) ; i ++ ) { InternalBoundedCountHistogramFacet histoFacet = ( ( InternalBoundedCountHistogramFacet ) ( facets . get ( i ) ) ) ; for ( int j = 0 ; j < ( firstHistoFacet . size ) ; j ++ ) { firstHistoFacet . counts [ j ] += histoFacet . counts [ j ] ; } histoFacet . releaseCache ( ) ; } if ( ( comparatorType ) != ( ComparatorType . KEY ) ) { Arrays . sort ( firstHistoFacet . entries , comparatorType . comparator ( ) ) ; } return firstHistoFacet ; } static final class Fields { static final XContentBuilderString _TYPE = new XContentBuilderString ( "_type" ) ; static final XContentBuilderString ENTRIES = new XContentBuilderString ( "entries" ) ; static final XContentBuilderString KEY = new XContentBuilderString ( "key" ) ; static final XContentBuilderString COUNT = new XContentBuilderString ( "count" ) ; } @ Override public XContentBuilder toXContent ( XContentBuilder builder , Params params ) throws IOException { } public static InternalBoundedCountHistogramFacet readHistogramFacet ( StreamInput in ) throws IOException { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } }
public class IOSFiles implements Files { static final String appDir = System . getenv ( "HOME" ) ; static final String externalPath = ( IOSFiles . appDir ) + "/Documents" ; static final String localPath = ( IOSFiles . appDir ) + "/Library/local" ; static final String internalPath = NSBundle . getMainBundle ( ) . getBundlePath ( ) ; public IOSFiles ( ) { } @ Override public FileHandle getFileHandle ( String fileName , FileType type ) { } @ Override public FileHandle classpath ( String path ) { } @ Override public FileHandle internal ( String path ) { <START_BUG> return new IOSFileHandle ( path , FileType . Internal ) ; <END_BUG> } @ Override public FileHandle external ( String path ) { } @ Override public FileHandle absolute ( String path ) { } @ Override public FileHandle local ( String path ) { } @ Override public String getExternalStoragePath ( ) { } @ Override public boolean isExternalStorageAvailable ( ) { } @ Override public String getLocalStoragePath ( ) { } @ Override public boolean isLocalStorageAvailable ( ) { } }
public class MetaDataService extends AbstractComponent { private final ClusterService clusterService ; private final ShardsRoutingStrategy shardsRoutingStrategy ; private final IndicesService indicesService ; private final NodeIndexCreatedAction nodeIndexCreatedAction ; private final NodeIndexDeletedAction nodeIndexDeletedAction ; private final NodeMappingCreatedAction nodeMappingCreatedAction ; @ Inject public MetaDataService ( Settings settings , ClusterService clusterService , IndicesService indicesService , ShardsRoutingStrategy shardsRoutingStrategy , NodeIndexCreatedAction nodeIndexCreatedAction , NodeIndexDeletedAction nodeIndexDeletedAction , NodeMappingCreatedAction nodeMappingCreatedAction ) { } public synchronized MetaDataService . CreateIndexResult createIndex ( final String index , final Settings indexSettings , TimeValue timeout ) throws IndexAlreadyExistsException { } public synchronized MetaDataService . DeleteIndexResult deleteIndex ( final String index , TimeValue timeout ) throws IndexMissingException { } public synchronized void updateMapping ( final String index , final String type , final String mappingSource ) { } public synchronized MetaDataService . PutMappingResult putMapping ( final String [ ] indices , String mappingType , final String mappingSource , boolean ignoreConflicts , TimeValue timeout ) throws ElasticSearchException { ClusterState clusterState = clusterService . state ( ) ; for ( String index : indices ) { IndexRoutingTable indexTable = clusterState . routingTable ( ) . indicesRouting ( ) . get ( index ) ; if ( indexTable == null ) { throw new IndexMissingException ( new Index ( index ) ) ; } } Map < String , DocumentMapper > newMappers = Maps . newHashMap ( ) ; Map < String , DocumentMapper > existingMappers = Maps . newHashMap ( ) ; for ( String index : indices ) { IndexService indexService = indicesService . indexService ( index ) ; if ( indexService != null ) { DocumentMapper newMapper = indexService . mapperService ( ) . parse ( mappingType , mappingSource ) ; newMappers . put ( index , newMapper ) ; DocumentMapper existingMapper = indexService . mapperService ( ) . documentMapper ( mappingType ) ; if ( existingMapper != null ) { DocumentMapper . MergeResult mergeResult = existingMapper . merge ( newMapper , mergeFlags ( ) . simulate ( true ) ) ; if ( ( ! ignoreConflicts ) && ( mergeResult . hasConflicts ( ) ) ) { throw new org . elasticsearch . index . mapper . MergeMappingException ( mergeResult . conflicts ( ) ) ; } <START_BUG> existingMappers . put ( index , newMapper ) ; <END_BUG> } } else { throw new IndexMissingException ( new Index ( index ) ) ; } } if ( mappingType == null ) { mappingType = newMappers . values ( ) . iterator ( ) . next ( ) . type ( ) ; } else if ( ! ( mappingType . equals ( newMappers . values ( ) . iterator ( ) . next ( ) . type ( ) ) ) ) { throw new InvalidTypeNameException ( "Type<seq2seq4repair_space>name<seq2seq4repair_space>provided<seq2seq4repair_space>does<seq2seq4repair_space>not<seq2seq4repair_space>match<seq2seq4repair_space>type<seq2seq4repair_space>name<seq2seq4repair_space>within<seq2seq4repair_space>mapping<seq2seq4repair_space>definition" ) ; } if ( ( mappingType . charAt ( 0 ) ) == '_' ) { throw new InvalidTypeNameException ( "Document<seq2seq4repair_space>mapping<seq2seq4repair_space>type<seq2seq4repair_space>name<seq2seq4repair_space>can't<seq2seq4repair_space>start<seq2seq4repair_space>with<seq2seq4repair_space>'_'" ) ; } final Map < String , Tuple < String , String > > mappings = Maps . newHashMap ( ) ; for ( Map . Entry < String , DocumentMapper > entry : newMappers . entrySet ( ) ) { Tuple < String , String > mapping ; String index = entry . getKey ( ) ; DocumentMapper newMapper = entry . getValue ( ) ; if ( existingMappers . containsKey ( entry . getKey ( ) ) ) { DocumentMapper existingMapper = existingMappers . get ( entry . getKey ( ) ) ; existingMapper . merge ( newMapper , mergeFlags ( ) . simulate ( false ) ) ; mapping = new Tuple < String , String > ( existingMapper . type ( ) , existingMapper . buildSource ( ) ) ; } else { mapping = new Tuple < String , String > ( newMapper . type ( ) , newMapper . buildSource ( ) ) ; } mappings . put ( index , mapping ) ; logger . info ( ( ( ( ( ( ( "Index<seq2seq4repair_space>[" + index ) + "]:<seq2seq4repair_space>Put<seq2seq4repair_space>mapping<seq2seq4repair_space>[" ) + ( mapping . v1 ( ) ) ) + "]<seq2seq4repair_space>with<seq2seq4repair_space>source<seq2seq4repair_space>[" ) + ( mapping . v2 ( ) ) ) + "]" ) ) ; } final CountDownLatch latch = new CountDownLatch ( ( ( clusterService . state ( ) . nodes ( ) . size ( ) ) * ( indices . length ) ) ) ; final Set < String > indicesSet = Sets . newHashSet ( indices ) ; final String fMappingType = mappingType ; NodeMappingCreatedAction . Listener listener = new NodeMappingCreatedAction . Listener ( ) { @ Override public void onNodeMappingCreated ( NodeMappingCreatedAction . NodeMappingCreatedResponse response ) { if ( ( indicesSet . contains ( response . index ( ) ) ) && ( response . type ( ) . equals ( fMappingType ) ) ) { latch . countDown ( ) ; } } } ; nodeMappingCreatedAction . add ( listener ) ; clusterService . submitStateUpdateTask ( ( ( "put-mapping<seq2seq4repair_space>[" + mappingType ) + "]" ) , new ClusterStateUpdateTask ( ) { @ Override public ClusterState execute ( ClusterState currentState ) { MetaData . MetaData . Builder builder = newMetaDataBuilder ( ) . metaData ( currentState . metaData ( ) ) ; for ( String indexName : indices ) { IndexMetaData . IndexMetaData indexMetaData = currentState . metaData ( ) . index ( indexName ) ; if ( indexMetaData == null ) { throw new IndexMissingException ( new Index ( indexName ) ) ; } Tuple < String , String > mapping = mappings . get ( indexName ) ; builder . put ( newIndexMetaDataBuilder ( indexMetaData ) . putMapping ( mapping . v1 ( ) , mapping . v2 ( ) ) ) ; } return newClusterStateBuilder ( ) . state ( currentState ) . metaData ( builder ) . build ( ) ; } } ) ; boolean acknowledged ; try { acknowledged = latch . await ( timeout . millis ( ) , TimeUnit . MILLISECONDS ) ; }
public class RestDeleteByQueryAction extends BaseRestHandler { @ Inject public RestDeleteByQueryAction ( Settings settings , Client client , RestController controller ) { } @ Override public void handleRequest ( final RestRequest request , final RestChannel channel ) { DeleteByQueryRequest deleteByQueryRequest = new DeleteByQueryRequest ( splitIndices ( request . param ( "index" ) ) ) ; deleteByQueryRequest . listenerThreaded ( false ) ; try { <START_BUG> deleteByQueryRequest . querySource ( RestActions . parseQuerySource ( request ) ) ; <END_BUG> deleteByQueryRequest . queryParserName ( request . param ( "query_parser_name" ) ) ; String typesParam = request . param ( "type" ) ; if ( typesParam != null ) { deleteByQueryRequest . types ( RestActions . splitTypes ( typesParam ) ) ; } deleteByQueryRequest . timeout ( request . paramAsTime ( "timeout" , DEFAULT_TIMEOUT ) ) ; } catch ( Exception e ) { try { JsonBuilder builder = RestJsonBuilder . restJsonBuilder ( request ) ; channel . sendResponse ( new JsonRestResponse ( request , PRECONDITION_FAILED , builder . startObject ( ) . field ( "error" , e . getMessage ( ) ) . endObject ( ) ) ) ; } catch ( IOException e1 ) { logger . error ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>failure<seq2seq4repair_space>response" , e1 ) ; } return ; } client . deleteByQuery ( deleteByQueryRequest , new org . elasticsearch . action . ActionListener < DeleteByQueryResponse > ( ) { @ Override public void onResponse ( DeleteByQueryResponse result ) { try { JsonBuilder builder = RestJsonBuilder . restJsonBuilder ( request ) ; builder . startObject ( ) . field ( "ok" , true ) ; builder . startObject ( "_indices" ) ; for ( IndexDeleteByQueryResponse indexDeleteByQueryResponse : result . indices ( ) . values ( ) ) { builder . startObject ( indexDeleteByQueryResponse . index ( ) ) ; builder . startObject ( "_shards" ) ; builder . field ( "total" , indexDeleteByQueryResponse . totalShards ( ) ) ; builder . field ( "successful" , indexDeleteByQueryResponse . successfulShards ( ) ) ; builder . field ( "failed" , indexDeleteByQueryResponse . failedShards ( ) ) ; builder . endObject ( ) ; builder . endObject ( ) ; } builder . endObject ( ) ; builder . endObject ( ) ; channel . sendResponse ( new JsonRestResponse ( request , OK , builder ) ) ; } catch ( Exception e ) { onFailure ( e ) ; } } @ Override public void onFailure ( Throwable e ) { try { channel . sendResponse ( new JsonThrowableRestResponse ( request , e ) ) ; } catch ( IOException e1 ) { logger . error ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>failure<seq2seq4repair_space>response" , e1 ) ; } } } ) ; } }
public class SearchSourceBuilder implements ToXContent { public static SearchSourceBuilder searchSource ( ) { } public static HighlightBuilder highlight ( ) { } private QueryBuilder queryBuilder ; private byte [ ] queryBinary ; private FilterBuilder filterBuilder ; private byte [ ] filterBinary ; private int from = - 1 ; private int size = - 1 ; private Boolean explain ; private Boolean version ; private List < SortBuilder > sorts ; private boolean trackScores = false ; private Float minScore ; private List < String > fieldNames ; private List < SearchSourceBuilder . ScriptField > scriptFields ; private List < AbstractFacetBuilder > facets ; private byte [ ] facetsBinary ; private HighlightBuilder highlightBuilder ; private TObjectFloatHashMap < String > indexBoost = null ; public SearchSourceBuilder ( ) { } public SearchSourceBuilder query ( QueryBuilder query ) { } public SearchSourceBuilder query ( byte [ ] queryBinary ) { } public SearchSourceBuilder query ( String queryString ) { } public SearchSourceBuilder filter ( FilterBuilder filter ) { } public SearchSourceBuilder filter ( String filterString ) { } public SearchSourceBuilder filter ( byte [ ] filter ) { } public SearchSourceBuilder from ( int from ) { } public SearchSourceBuilder size ( int size ) { } public SearchSourceBuilder minScore ( float minScore ) { } public SearchSourceBuilder explain ( Boolean explain ) { } public SearchSourceBuilder version ( Boolean version ) { } public SearchSourceBuilder sort ( String name , SortOrder order ) { } public SearchSourceBuilder sort ( String name ) { } public SearchSourceBuilder sort ( SortBuilder sort ) { } public SearchSourceBuilder trackScores ( boolean trackScores ) { } public SearchSourceBuilder facet ( AbstractFacetBuilder facet ) { } public SearchSourceBuilder facets ( byte [ ] facetsBinary ) { } public HighlightBuilder highlighter ( ) { } public SearchSourceBuilder highlight ( HighlightBuilder highlightBuilder ) { } public SearchSourceBuilder noFields ( ) { } public SearchSourceBuilder fields ( List < String > fields ) { } public SearchSourceBuilder fields ( String ... fields ) { } public SearchSourceBuilder field ( String name ) { } public SearchSourceBuilder scriptField ( String name , String script ) { } public SearchSourceBuilder scriptField ( String name , String script , Map < String , Object > params ) { } public SearchSourceBuilder scriptField ( String name , String lang , String script , Map < String , Object > params ) { } public SearchSourceBuilder indexBoost ( String index , float indexBoost ) { } @ Override public String toString ( ) { } public BytesStream buildAsUnsafeBytes ( XContentType contentType ) throws SearchSourceBuilderException { try { XContentBuilder builder = XContentFactory . contentBuilder ( contentType ) ; toXContent ( builder , EMPTY_PARAMS ) ; <START_BUG> return builder . unsafeStream ( ) ; <END_BUG> } catch ( Exception e ) { throw new SearchSourceBuilderException ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>build<seq2seq4repair_space>search<seq2seq4repair_space>source" , e ) ; } } public byte [ ] buildAsBytes ( ) throws SearchSourceBuilderException { } public byte [ ] buildAsBytes ( XContentType contentType ) throws SearchSourceBuilderException { } @ Override public XContentBuilder toXContent ( XContentBuilder builder , Params params ) throws IOException { } private static class ScriptField { private final String fieldName ; private final String script ; private final String lang ; private final Map < String , Object > params ; private ScriptField ( String fieldName , String lang , String script , Map < String , Object > params ) { } public String fieldName ( ) { } public String script ( ) { } public String lang ( ) { } public Map < String , Object > params ( ) { } } }
client ( ) . admin ( ) . cluster ( ) . prepareHealth ( ) . setWaitForEvents ( LANGUID ) . setWaitForGreenStatus ( ) . execute ( ) . actionGet ( ) ; client ( ) . admin ( ) . indices ( ) . preparePutMapping ( "test" ) . setType ( "type1" ) . setSource ( XContentFactory . jsonBuilder ( ) . startObject ( ) . startObject ( "type1" ) . startObject ( "properties" ) . startObject ( "foo" ) . field ( "type" , "string" ) . endObject ( ) . startObject ( "bar" ) . field ( "type" , "integer" ) . endObject ( ) . startObject ( "baz" ) . field ( "type" , "string" ) . field ( "analyzer" , "snowball" ) . endObject ( ) . startObject ( "pin" ) . startObject ( "properties" ) . startObject ( "location" ) . field ( "type" , "geo_point" ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; client ( ) . admin ( ) . indices ( ) . preparePutMapping ( "test" ) . setType ( "child-type" ) . setSource ( XContentFactory . jsonBuilder ( ) . startObject ( ) . startObject ( "child-type" ) . startObject ( "_parent" ) . field ( "type" , "type1" ) . endObject ( ) . startObject ( "properties" ) . startObject ( "foo" ) . field ( "type" , "string" ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; client ( ) . admin ( ) . indices ( ) . prepareRefresh ( ) . execute ( ) . actionGet ( ) ; ValidateQueryResponse response ; response = client ( ) . admin ( ) . indices ( ) . prepareValidateQuery ( "test" ) . setQuery ( "foo" . getBytes ( Charsets . UTF_8 ) ) . setExplain ( true ) . execute ( ) . actionGet ( ) ; assertThat ( response . isValid ( ) , equalTo ( false ) ) ; assertThat ( response . getQueryExplanation ( ) . size ( ) , equalTo ( 1 ) ) ; assertThat ( response . getQueryExplanation ( ) . get ( 0 ) . getError ( ) , containsString ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>parse" ) ) ; assertThat ( response . getQueryExplanation ( ) . get ( 0 ) . getExplanation ( ) , nullValue ( ) ) ; assertExplanation ( QueryBuilders . queryString ( "_id:1" ) , equalTo ( "ConstantScore(_uid:type1#1)" ) ) ; assertExplanation ( QueryBuilders . idsQuery ( "type1" ) . addIds ( "1" ) . addIds ( "2" ) , equalTo ( "ConstantScore(_uid:type1#1<seq2seq4repair_space>_uid:type1#2)" ) ) ; assertExplanation ( QueryBuilders . queryString ( "foo" ) , equalTo ( "_all:foo" ) ) ; assertExplanation ( QueryBuilders . filteredQuery ( QueryBuilders . termQuery ( "foo" , "1" ) , FilterBuilders . orFilter ( FilterBuilders . termFilter ( "bar" , "2" ) , FilterBuilders . termFilter ( "baz" , "3" ) ) ) , equalTo ( "filtered(foo:1)->cache(bar:[2<seq2seq4repair_space>TO<seq2seq4repair_space>2])<seq2seq4repair_space>cache(baz:3)" ) ) ; assertExplanation ( QueryBuilders . filteredQuery ( QueryBuilders . termQuery ( "foo" , "1" ) , FilterBuilders . orFilter ( FilterBuilders . termFilter ( "bar" , "2" ) ) ) , equalTo ( "filtered(foo:1)->cache(bar:[2<seq2seq4repair_space>TO<seq2seq4repair_space>2])" ) ) ; assertExplanation ( QueryBuilders . filteredQuery ( QueryBuilders . matchAllQuery ( ) , FilterBuilders . geoPolygonFilter ( "pin.location" ) . addPoint ( 40 , ( - 70 ) ) . addPoint ( 30 , ( - 80 ) ) . addPoint ( 20 , ( - 90 ) ) ) , equalTo ( "ConstantScore(GeoPolygonFilter(pin.location,<seq2seq4repair_space>[[40.0,<seq2seq4repair_space>-70.0],<seq2seq4repair_space>[30.0,<seq2seq4repair_space>-80.0],<seq2seq4repair_space>[20.0,<seq2seq4repair_space>-90.0]]))" ) ) ; assertExplanation ( QueryBuilders . constantScoreQuery ( FilterBuilders . geoBoundingBoxFilter ( "pin.location" ) . topLeft ( 40 , ( - 80 ) ) . bottomRight ( 20 , ( - 70 ) ) ) , equalTo ( "ConstantScore(GeoBoundingBoxFilter(pin.location,<seq2seq4repair_space>[40.0,<seq2seq4repair_space>-80.0],<seq2seq4repair_space>[20.0,<seq2seq4repair_space>-70.0]))" ) ) ; assertExplanation ( QueryBuilders . constantScoreQuery ( FilterBuilders . geoDistanceFilter ( "pin.location" ) . lat ( 10 ) . lon ( 20 ) . distance ( 15 , MILES ) . geoDistance ( PLANE ) ) , equalTo ( "ConstantScore(GeoDistanceFilter(pin.location,<seq2seq4repair_space>PLANE,<seq2seq4repair_space>15.0,<seq2seq4repair_space>10.0,<seq2seq4repair_space>20.0))" ) ) ; assertExplanation ( QueryBuilders . constantScoreQuery ( FilterBuilders . geoDistanceFilter ( "pin.location" ) . lat ( 10 ) . lon ( 20 ) . distance ( 15 , MILES ) . geoDistance ( PLANE ) ) , equalTo ( "ConstantScore(GeoDistanceFilter(pin.location,<seq2seq4repair_space>PLANE,<seq2seq4repair_space>15.0,<seq2seq4repair_space>10.0,<seq2seq4repair_space>20.0))" ) ) ; assertExplanation ( QueryBuilders . constantScoreQuery ( FilterBuilders . geoDistanceRangeFilter ( "pin.location" ) . lat ( 10 ) . lon ( 20 ) . from ( "15miles" ) . to ( "25miles" ) . geoDistance ( PLANE ) ) , equalTo ( "ConstantScore(GeoDistanceRangeFilter(pin.location,<seq2seq4repair_space>PLANE,<seq2seq4repair_space>[15.0<seq2seq4repair_space>-<seq2seq4repair_space>25.0],<seq2seq4repair_space>10.0,<seq2seq4repair_space>20.0))" ) ) ; assertExplanation ( QueryBuilders . filteredQuery ( QueryBuilders . termQuery ( "foo" , "1" ) , FilterBuilders . andFilter ( FilterBuilders . termFilter ( "bar" , "2" ) , FilterBuilders . termFilter ( "baz" , "3" ) ) ) , equalTo ( "filtered(foo:1)->+cache(bar:[2<seq2seq4repair_space>TO<seq2seq4repair_space>2])<seq2seq4repair_space>+cache(baz:3)" ) ) ; assertExplanation ( QueryBuilders . constantScoreQuery ( FilterBuilders . termsFilter ( "foo" , "1" , "2" , "3" ) ) , equalTo ( "ConstantScore(cache(foo:1<seq2seq4repair_space>foo:2<seq2seq4repair_space>foo:3))" ) ) ; assertExplanation ( QueryBuilders . constantScoreQuery ( FilterBuilders . notFilter ( FilterBuilders . termFilter ( "foo" , "bar" ) ) ) , equalTo ( "ConstantScore(NotFilter(cache(foo:bar)))" ) ) ; <START_BUG> assertExplanation ( QueryBuilders . filteredQuery ( QueryBuilders . termQuery ( "foo" , "1" ) , FilterBuilders . hasChildFilter ( "child-type" , QueryBuilders . fieldQuery ( "foo" , "1" ) ) ) , equalTo ( "filtered(foo:1)->child_filter[child-type/type1](filtered(foo:1)->cache(_type:child-type))" ) ) ; <END_BUG> assertExplanation ( QueryBuilders . filteredQuery ( QueryBuilders . termQuery ( "foo" , "1" ) , FilterBuilders . scriptFilter ( "true" ) ) , equalTo ( "filtered(foo:1)->ScriptFilter(true)" ) ) ; } @ Test public void explainValidateQueryTwoNodes ( ) throws IOException { } @ Test public void explainDateRangeInQueryString ( ) { } private void assertExplanation ( QueryBuilder queryBuilder , Matcher < String > matcher ) { } }
public class MoreLikeThisFieldQueryParser extends AbstractIndexComponent implements XContentQueryParser { public static final String NAME = "mlt_field" ; public MoreLikeThisFieldQueryParser ( Index index , @ IndexSettings Settings indexSettings ) { } @ Override public String [ ] names ( ) { } @ Override public Query parse ( QueryParseContext parseContext ) throws IOException , QueryParsingException { XContentParser parser = parseContext . parser ( ) ; XContentParser . Token token = parser . nextToken ( ) ; assert token == ( Token . FIELD_NAME ) ; String fieldName = parser . currentName ( ) ; token = parser . nextToken ( ) ; assert token == ( Token . START_OBJECT ) ; MoreLikeThisQuery mltQuery = new MoreLikeThisQuery ( ) ; mltQuery . setSimilarity ( parseContext . searchSimilarity ( ) ) ; String currentFieldName = null ; while ( ( token = parser . nextToken ( ) ) != ( Token . END_OBJECT ) ) { if ( token == ( Token . FIELD_NAME ) ) { currentFieldName = parser . currentName ( ) ; } else if ( token . isValue ( ) ) { if ( "like_text" . equals ( currentFieldName ) ) { mltQuery . setLikeText ( parser . text ( ) ) ; } else if ( ( "min_term_freq" . equals ( currentFieldName ) ) || ( "minTermFreq" . equals ( currentFieldName ) ) ) { mltQuery . setMinTermFrequency ( parser . intValue ( ) ) ; } else if ( ( "max_query_terms" . equals ( currentFieldName ) ) || ( "maxQueryTerms" . equals ( currentFieldName ) ) ) { mltQuery . setMaxQueryTerms ( parser . intValue ( ) ) ; } else if ( ( "min_doc_freq" . equals ( currentFieldName ) ) || ( "minDocFreq" . equals ( currentFieldName ) ) ) { mltQuery . setMinDocFreq ( parser . intValue ( ) ) ; } else if ( ( "max_doc_freq" . equals ( currentFieldName ) ) || ( "maxDocFreq" . equals ( currentFieldName ) ) ) { mltQuery . setMaxDocFreq ( parser . intValue ( ) ) ; } else if ( ( "min_word_len" . equals ( currentFieldName ) ) || ( "minWordLen" . equals ( currentFieldName ) ) ) { mltQuery . setMinWordLen ( parser . intValue ( ) ) ; } else if ( ( "max_word_len" . equals ( currentFieldName ) ) || ( "maxWordLen" . equals ( currentFieldName ) ) ) { mltQuery . setMaxWordLen ( parser . intValue ( ) ) ; } else if ( ( "boost_terms" . equals ( currentFieldName ) ) || ( "boostTerms" . equals ( currentFieldName ) ) ) { mltQuery . setBoostTerms ( true ) ; mltQuery . setBoostTermsFactor ( parser . floatValue ( ) ) ; } else if ( ( "percent_terms_to_match" . equals ( currentFieldName ) ) || ( "percentTermsToMatch" . equals ( currentFieldName ) ) ) { mltQuery . setPercentTermsToMatch ( parser . floatValue ( ) ) ; } } else if ( token == ( Token . START_ARRAY ) ) { if ( ( "stop_words" . equals ( currentFieldName ) ) || ( "stopWords" . equals ( currentFieldName ) ) ) { Set < String > stopWords = Sets . newHashSet ( ) ; while ( ( token = parser . nextToken ( ) ) != ( Token . END_ARRAY ) ) { stopWords . add ( parser . text ( ) ) ; } mltQuery . setStopWords ( stopWords ) ; } } } if ( ( mltQuery . getLikeText ( ) ) == null ) { throw new QueryParsingException ( index , "more_like_this_field<seq2seq4repair_space>requires<seq2seq4repair_space>'like_text'<seq2seq4repair_space>to<seq2seq4repair_space>be<seq2seq4repair_space>specified" ) ; } token = parser . nextToken ( ) ; assert token == ( Token . END_OBJECT ) ; MapperService . SmartNameFieldMappers smartNameFieldMappers = parseContext . smartFieldMappers ( fieldName ) ; if ( smartNameFieldMappers != null ) { if ( smartNameFieldMappers . hasMapper ( ) ) { fieldName = smartNameFieldMappers . mapper ( ) . names ( ) . indexName ( ) ; mltQuery . setAnalyzer ( smartNameFieldMappers . mapper ( ) . searchAnalyzer ( ) ) ; } } if ( ( mltQuery . getAnalyzer ( ) ) == null ) { mltQuery . setAnalyzer ( parseContext . mapperService ( ) . searchAnalyzer ( ) ) ; } mltQuery . setMoreLikeFields ( new String [ ] { fieldName } ) ; <START_BUG> return wrapSmartNameQuery ( mltQuery , smartNameFieldMappers , parseContext . indexCache ( ) ) ; <END_BUG> } }
public class Fixture { private final World world ; private final Body body ; protected final long addr ; protected Shape shape ; protected Fixture ( World world , Body body , long addr ) { } public Type getType ( ) { } private native int jniGetType ( long addr ) { } public Shape getShape ( ) { if ( ( shape ) == null ) { long shapeAddr = jniGetShape ( addr ) ; <START_BUG> int type = Shape . jniGetType ( addr ) ; <END_BUG> if ( type == 0 ) shape = new CircleShape ( shapeAddr ) ; else shape = new PolygonShape ( shapeAddr ) ; } return shape ; } private native long jniGetShape ( long addr ) { } public void setSensor ( boolean sensor ) { } private native void jniSetSensor ( long addr , boolean sensor ) { } public boolean isSensor ( ) { } private native boolean jniIsSensor ( long addr ) { } public void setFilterData ( Filter filter ) { } private native void jniSetFilterData ( long addr , short categoryBits , short maskBits , short groupIndex ) { } private final short [ ] tmp = new short [ 3 ] ; private final Filter filter = new Filter ( ) ; public Filter getFilterData ( ) { } private native void jniGetFilterData ( long addr , short [ ] filter ) { } public Body getBody ( ) { } public boolean testPoint ( Vector2 p ) { } private native boolean jniTestPoint ( long addr , float x , float y ) { } public void setDensity ( float density ) { } private native void jniSetDensity ( long addr , float density ) { } public float getDensity ( ) { } private native float jniGetDensity ( long addr ) { } public float getFriction ( ) { } private native float jniGetFriction ( long addr ) { } public void setFriction ( float friction ) { } private native void jniSetFriction ( long addr , float friction ) { } public float getRestitution ( ) { } private native float jniGetRestitution ( long addr ) { } public void setRestitution ( float restitution ) { } private native void jniSetRestitution ( long addr , float restitution ) { } }
public class JoglDebugStarter { public static void main ( String [ ] argv ) { JoglApplicationConfiguration config = new JoglApplicationConfiguration ( ) ; config . useGL20 = true ; <START_BUG> new com . badlogic . gdx . backends . jogl . JoglApplication ( new com . badlogic . gdx . tests . ProjectiveTextureTest ( ) , config ) ; <END_BUG> } }
public class SpritePerformanceTest extends GdxTest { StringBuilder log = new StringBuilder ( ) ; static final int SPRITES = 500 ; Sprite [ ] sprites ; Texture texture ; SpriteBatch vaBatch ; SpriteBatch vboBatch ; SpriteCache cache ; int spritesHandle ; float rotation = 0 ; long startTime ; int frames ; String [ ] modes = new String [ ] { "SpriteBatch<seq2seq4repair_space>blended" , "SpriteBatch<seq2seq4repair_space>not<seq2seq4repair_space>blended" , "SpriteBatch<seq2seq4repair_space>animated<seq2seq4repair_space>blended" , "SpriteBatch<seq2seq4repair_space>animated<seq2seq4repair_space>not<seq2seq4repair_space>blended" , "SpriteBatch<seq2seq4repair_space>VBO<seq2seq4repair_space>blended" , "SpriteBatch<seq2seq4repair_space>VBO<seq2seq4repair_space>not<seq2seq4repair_space>blended" , "SpriteBatch<seq2seq4repair_space>VBO<seq2seq4repair_space>animated<seq2seq4repair_space>blended" , "SpriteBatch<seq2seq4repair_space>VBO<seq2seq4repair_space>animated<seq2seq4repair_space>not<seq2seq4repair_space>blended" , "SpriteCache<seq2seq4repair_space>blended" , "SpriteCache<seq2seq4repair_space>not<seq2seq4repair_space>blended" } ; int mode = 0 ; public void create ( ) { texture = graphics . newTexture ( files . internal ( "data/badlogicsmall.jpg" ) , Linear , Linear , ClampToEdge , ClampToEdge ) ; vaBatch = new SpriteBatch ( 1000 ) ; <START_BUG> vboBatch = new SpriteBatch ( 1000 ) ; <END_BUG> cache = new SpriteCache ( ) ; sprites = new Sprite [ SpritePerformanceTest . SPRITES ] ; for ( int i = 0 ; i < ( SpritePerformanceTest . SPRITES ) ; i ++ ) { int x = ( ( int ) ( ( Math . random ( ) ) * ( ( graphics . getWidth ( ) ) - 32 ) ) ) ; int y = ( ( int ) ( ( Math . random ( ) ) * ( ( graphics . getHeight ( ) ) - 32 ) ) ) ; sprites [ i ] = new Sprite ( texture ) ; sprites [ i ] . setPosition ( x , y ) ; } cache . beginCache ( ) ; for ( int i = 0 ; i < ( SpritePerformanceTest . SPRITES ) ; i ++ ) { cache . add ( sprites [ i ] ) ; } int spritesHandle = cache . endCache ( ) ; startTime = System . nanoTime ( ) ; frames = 0 ; } public void render ( ) { } void renderSpriteBatch ( ) { } void renderSpriteBatchBlendDisabled ( ) { } void renderSpriteBatchAnimated ( ) { } void renderSpriteBatchAnimatedBlendDisabled ( ) { } void renderSpriteBatchVBO ( ) { } void renderSpriteBatchBlendDisabledVBO ( ) { } void renderSpriteBatchAnimatedVBO ( ) { } void renderSpriteBatchAnimatedBlendDisabledVBO ( ) { } void renderSpriteCache ( ) { } void renderSpriteCacheBlendDisabled ( ) { } @ Override public boolean needsGL20 ( ) { } }
public class TransportMultiGetAction extends TransportAction < MultiGetRequest , MultiGetResponse > { private final ClusterService clusterService ; private final TransportShardMultiGetAction shardAction ; @ Inject public TransportMultiGetAction ( Settings settings , ThreadPool threadPool , TransportService transportService , ClusterService clusterService , TransportShardMultiGetAction shardAction ) { } @ Override protected void doExecute ( final MultiGetRequest request , final ActionListener < MultiGetResponse > listener ) { } class TransportHandler extends BaseTransportRequestHandler < MultiGetRequest > { @ Override public MultiGetRequest newInstance ( ) { } @ Override public void messageReceived ( final MultiGetRequest request , final TransportChannel channel ) throws Exception { request . listenerThreaded ( false ) ; execute ( request , new ActionListener < MultiGetResponse > ( ) { @ Override public void onResponse ( MultiGetResponse response ) { try { channel . sendResponse ( response ) ; <START_BUG> } catch ( Exception e ) { <END_BUG> onFailure ( e ) ; } } @ Override public void onFailure ( Throwable e ) { try { channel . sendResponse ( e ) ; } catch ( Exception e1 ) { logger . warn ( ( ( ( ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>error<seq2seq4repair_space>response<seq2seq4repair_space>for<seq2seq4repair_space>action<seq2seq4repair_space>[" + ( MultiGetAction . NAME ) ) + "]<seq2seq4repair_space>and<seq2seq4repair_space>request<seq2seq4repair_space>[" ) + request ) + "]" ) , e1 ) ; } } } ) ; } @ Override public String executor ( ) { } } }
@ Test public void queryStringAnalyzedWildcard ( ) throws Exception { } @ Test public void testLowercaseExpandedTerms ( ) { } @ Test public void testDateRangeInQueryString ( ) { } @ Test public void typeFilterTypeIndexedTests ( ) throws Exception { } @ Test public void typeFilterTypeNotIndexedTests ( ) throws Exception { } private void typeFilterTests ( String index ) throws Exception { } @ Test public void idsFilterTestsIdIndexed ( ) throws Exception { } @ Test public void idsFilterTestsIdNotIndexed ( ) throws Exception { } private void idsFilterTests ( String index ) throws Exception { } @ Test public void testLimitFilter ( ) throws Exception { } @ Test public void filterExistsMissingTests ( ) throws Exception { } @ Test public void passQueryOrFilterAsJSONStringTest ( ) throws Exception { } @ Test public void testFiltersWithCustomCacheKey ( ) throws Exception { } @ Test public void testMatchQueryNumeric ( ) throws Exception { } @ Test public void testMultiMatchQuery ( ) throws Exception { } @ Test public void testMatchQueryZeroTermsQuery ( ) { } public void testMultiMatchQueryZeroTermsQuery ( ) { } @ Test public void testMultiMatchQueryMinShouldMatch ( ) { } @ Test public void testFuzzyQueryString ( ) { } @ Test public void testQuotedQueryStringWithBoost ( ) throws InterruptedException , ExecutionException { } @ Test public void testSpecialRangeSyntaxInQueryString ( ) { } @ Test public void testEmptyTermsFilter ( ) throws Exception { } @ Test public void testFieldDataTermsFilter ( ) throws Exception { } @ Test public void testTermsLookupFilter ( ) throws Exception { } @ Test public void testBasicFilterById ( ) throws Exception { } @ Test public void testBasicQueryById ( ) throws Exception { } @ Test public void testNumericTermsAndRanges ( ) throws Exception { } @ Test public void testNumericRangeFilter_2826 ( ) throws Exception { } @ Test public void testEmptyTopLevelFilter ( ) { } @ Test public void testMustNot ( ) throws IOException , InterruptedException , ExecutionException , ElasticsearchException { } @ Test public void testSimpleSpan ( ) throws IOException , InterruptedException , ExecutionException , ElasticsearchException { } @ Test public void testSpanMultiTermQuery ( ) throws IOException , ElasticsearchException { } @ Test public void testSimpleDFSQuery ( ) throws IOException , ElasticsearchException { } @ Test public void testMultiFieldQueryString ( ) { } @ Test public void testMatchQueryWithSynonyms ( ) throws IOException { } @ Test public void testMatchQueryWithStackedStems ( ) throws IOException { } @ Test public void testQueryStringWithSynonyms ( ) throws IOException { } @ Test public void testCustomWordDelimiterQueryString ( ) { } @ Test public void testMultiMatchLenientIssue3797 ( ) { } @ Test public void testIndicesQuery ( ) throws Exception { } @ Test public void testIndicesFilter ( ) throws Exception { } @ Test public void testIndicesQuerySkipParsing ( ) throws Exception { } @ Test public void testIndicesFilterSkipParsing ( ) throws Exception { } @ Test public void testIndicesQueryMissingIndices ( ) throws IOException , InterruptedException , ExecutionException { } @ Test public void testIndicesFilterMissingIndices ( ) throws IOException , InterruptedException , ExecutionException { } @ Test public void testMinScore ( ) throws InterruptedException , ExecutionException { } @ Test public void testQueryStringWithSlopAndFields ( ) { } private static FilterBuilder rangeFilter ( String field , Object from , Object to ) { } @ Test public void testSimpleQueryString ( ) throws InterruptedException , ExecutionException { createIndex ( "test" ) ; <START_BUG> indexRandom ( true , client ( ) . prepareIndex ( "test" , "type1" , "1" ) . setSource ( "body" , "foo" ) , client ( ) . prepareIndex ( "test" , "type1" , "2" ) . setSource ( "body" , "bar" ) , client ( ) . prepareIndex ( "test" , "type1" , "3" ) . setSource ( "body" , "foo<seq2seq4repair_space>bar" ) , client ( ) . prepareIndex ( "test" , "type1" , "4" ) . setSource ( "body" , "quux<seq2seq4repair_space>baz<seq2seq4repair_space>eggplant" ) , client ( ) . prepareIndex ( "test" , "type1" , "5" ) . setSource ( "body" , "quux<seq2seq4repair_space>baz<seq2seq4repair_space>spaghetti" ) , client ( ) . prepareIndex ( "test" , "type1" , "6" ) . setSource ( "otherbody" , "spaghetti" ) ) ; <END_BUG> SearchResponse searchResponse = client ( ) . prepareSearch ( ) . setQuery ( simpleQueryString ( "foo<seq2seq4repair_space>bar" ) ) . get ( ) ; assertHitCount ( searchResponse , 3L ) ; assertSearchHits ( searchResponse , "1" , "2" , "3" ) ; searchResponse = client ( ) . prepareSearch ( ) . setQuery ( simpleQueryString ( "foo<seq2seq4repair_space>bar" ) . defaultOperator ( AND ) ) . get ( ) ; assertHitCount ( searchResponse , 1L ) ; assertFirstHit ( searchResponse , hasId ( "3" ) ) ; searchResponse = client ( ) . prepareSearch ( ) . setQuery ( simpleQueryString ( "\"quux<seq2seq4repair_space>baz\"<seq2seq4repair_space>+(eggplant<seq2seq4repair_space>|<seq2seq4repair_space>spaghetti)" ) ) . get ( ) ; assertHitCount ( searchResponse , 2L ) ; assertSearchHits ( searchResponse , "4" , "5" ) ; searchResponse = client ( ) . prepareSearch ( ) . setQuery ( simpleQueryString ( "eggplants" ) . analyzer ( "snowball" ) ) . get ( ) ; assertHitCount ( searchResponse , 1L ) ; assertFirstHit ( searchResponse , hasId ( "4" ) ) ; searchResponse = client ( ) . prepareSearch ( ) . setQuery ( simpleQueryString ( "spaghetti" ) . field ( "body" , 10.0F ) . field ( "otherbody" , 2.0F ) ) . get ( ) ; assertHitCount ( searchResponse , 2L ) ; assertFirstHit ( searchResponse , hasId ( "5" ) ) ; assertSearchHits ( searchResponse , "5" , "6" ) ; } @ Test public void testSimpleQueryStringLowercasing ( ) { } @ Test public void testQueryStringLocale ( ) { } @ Test public void testNestedFieldSimpleQueryString ( ) throws IOException { } @ Test public void testSimpleQueryStringFlags ( ) throws InterruptedException , ExecutionException { } @ Test public void testSimpleQueryStringLenient ( ) throws InterruptedException , ExecutionException { } @ Test public void testDateProvidedAsNumber ( ) throws InterruptedException , ExecutionException { } @ Test public void testRangeFilterNoCacheWithNow ( ) throws Exception { } @ Test public void testSearchEmptyDoc ( ) { }
public class TopChildrenQueryParser implements QueryParser { public static final String NAME = "top_children" ; @ Inject public TopChildrenQueryParser ( ) { } @ Override public String [ ] names ( ) { } @ Override public Query parse ( QueryParseContext parseContext ) throws IOException , QueryParsingException { XContentParser parser = parseContext . parser ( ) ; Query query = null ; boolean queryFound = false ; float boost = 1.0F ; String childType = null ; ScoreType scoreType = ScoreType . MAX ; int factor = 5 ; int incrementalFactor = 2 ; String queryName = null ; String currentFieldName = null ; XContentParser . Token token ; while ( ( token = parser . nextToken ( ) ) != ( Token . END_OBJECT ) ) { if ( token == ( Token . FIELD_NAME ) ) { currentFieldName = parser . currentName ( ) ; } else if ( token == ( Token . START_OBJECT ) ) { if ( "query" . equals ( currentFieldName ) ) { queryFound = true ; String [ ] origTypes = QueryParseContext . setTypesWithPrevious ( ( childType == null ? null : new String [ ] { childType } ) ) ; try { query = parseContext . parseInnerQuery ( ) ; } finally { QueryParseContext . setTypes ( origTypes ) ; } } else { throw new QueryParsingException ( parseContext . index ( ) , ( ( "[top_children]<seq2seq4repair_space>query<seq2seq4repair_space>does<seq2seq4repair_space>not<seq2seq4repair_space>support<seq2seq4repair_space>[" + currentFieldName ) + "]" ) ) ; } } else if ( token . isValue ( ) ) { if ( "type" . equals ( currentFieldName ) ) { childType = parser . text ( ) ; } else if ( "_scope" . equals ( currentFieldName ) ) { throw new QueryParsingException ( parseContext . index ( ) , "the<seq2seq4repair_space>[_scope]<seq2seq4repair_space>support<seq2seq4repair_space>in<seq2seq4repair_space>[top_children]<seq2seq4repair_space>query<seq2seq4repair_space>has<seq2seq4repair_space>been<seq2seq4repair_space>removed,<seq2seq4repair_space>use<seq2seq4repair_space>a<seq2seq4repair_space>filter<seq2seq4repair_space>as<seq2seq4repair_space>a<seq2seq4repair_space>facet_filter<seq2seq4repair_space>in<seq2seq4repair_space>the<seq2seq4repair_space>relevant<seq2seq4repair_space>global<seq2seq4repair_space>facet" ) ; } else if ( "score" . equals ( currentFieldName ) ) { scoreType = ScoreType . fromString ( parser . text ( ) ) ; } else if ( ( "score_mode" . equals ( currentFieldName ) ) || ( "scoreMode" . equals ( currentFieldName ) ) ) { scoreType = ScoreType . fromString ( parser . text ( ) ) ; } else if ( "boost" . equals ( currentFieldName ) ) { boost = parser . floatValue ( ) ; } else if ( "factor" . equals ( currentFieldName ) ) { factor = parser . intValue ( ) ; } else if ( ( "incremental_factor" . equals ( currentFieldName ) ) || ( "incrementalFactor" . equals ( currentFieldName ) ) ) { incrementalFactor = parser . intValue ( ) ; } else if ( "_name" . equals ( currentFieldName ) ) { queryName = parser . text ( ) ; } else { throw new QueryParsingException ( parseContext . index ( ) , ( ( "[top_children]<seq2seq4repair_space>query<seq2seq4repair_space>does<seq2seq4repair_space>not<seq2seq4repair_space>support<seq2seq4repair_space>[" + currentFieldName ) + "]" ) ) ; } } } if ( ! queryFound ) { throw new QueryParsingException ( parseContext . index ( ) , "[top_children]<seq2seq4repair_space>requires<seq2seq4repair_space>'query'<seq2seq4repair_space>field" ) ; } if ( childType == null ) { throw new QueryParsingException ( parseContext . index ( ) , "[top_children]<seq2seq4repair_space>requires<seq2seq4repair_space>'type'<seq2seq4repair_space>field" ) ; } if ( query == null ) { return null ; } if ( "delete_by_query" . equals ( SearchContext . current ( ) . source ( ) ) ) { throw new QueryParsingException ( parseContext . index ( ) , "[top_children]<seq2seq4repair_space>unsupported<seq2seq4repair_space>in<seq2seq4repair_space>delete_by_query<seq2seq4repair_space>api" ) ; } DocumentMapper childDocMapper = parseContext . mapperService ( ) . documentMapper ( childType ) ; if ( childDocMapper == null ) { throw new QueryParsingException ( parseContext . index ( ) , ( ( "No<seq2seq4repair_space>mapping<seq2seq4repair_space>for<seq2seq4repair_space>for<seq2seq4repair_space>type<seq2seq4repair_space>[" + childType ) + "]" ) ) ; } <START_BUG> if ( ( childDocMapper . parentFieldMapper ( ) ) == null ) { <END_BUG> throw new QueryParsingException ( parseContext . index ( ) , ( ( "Type<seq2seq4repair_space>[" + childType ) + "]<seq2seq4repair_space>does<seq2seq4repair_space>not<seq2seq4repair_space>have<seq2seq4repair_space>parent<seq2seq4repair_space>mapping" ) ) ; } String parentType = childDocMapper . parentFieldMapper ( ) . type ( ) ; query . setBoost ( boost ) ; query = new org . elasticsearch . common . lucene . search . XFilteredQuery ( query , parseContext . cacheFilter ( childDocMapper . typeFilter ( ) , null ) ) ; TopChildrenQuery childQuery = new TopChildrenQuery ( query , childType , parentType , scoreType , factor , incrementalFactor , parseContext . cacheRecycler ( ) ) ; if ( queryName != null ) { parseContext . addNamedQuery ( queryName , childQuery ) ; } return childQuery ; } }
public class FacetsPhase implements SearchPhase { @ Override public Map < String , ? extends SearchParseElement > parseElements ( ) { } @ Override public void preProcess ( SearchContext context ) { } @ Override public void execute ( SearchContext context ) throws ElasticSearchException { if ( ( context . facets ( ) ) == null ) { return ; } if ( ( context . queryResult ( ) . facets ( ) ) != null ) { return ; } if ( ( context . searcher ( ) . globalCollectors ( ) ) != null ) { <START_BUG> Query query = new DeletionAwareConstantScoreQuery ( Queries . MATCH_ALL_FILTER ) ; <END_BUG> if ( ( context . types ( ) . length ) > 0 ) { if ( ( context . types ( ) . length ) == 1 ) { String type = context . types ( ) [ 0 ] ; DocumentMapper docMapper = context . mapperService ( ) . documentMapper ( type ) ; query = new FilteredQuery ( query , context . filterCache ( ) . cache ( docMapper . typeFilter ( ) ) ) ; } else { BooleanFilter booleanFilter = new BooleanFilter ( ) ; for ( String type : context . types ( ) ) { DocumentMapper docMapper = context . mapperService ( ) . documentMapper ( type ) ; booleanFilter . add ( new FilterClause ( context . filterCache ( ) . cache ( docMapper . typeFilter ( ) ) , Occur . SHOULD ) ) ; } query = new FilteredQuery ( query , booleanFilter ) ; } } context . searcher ( ) . useGlobalCollectors ( true ) ; try { context . searcher ( ) . search ( query , NOOP_COLLECTOR ) ; } catch ( IOException e ) { throw new org . elasticsearch . search . query . QueryPhaseExecutionException ( context , "Failed<seq2seq4repair_space>to<seq2seq4repair_space>execute<seq2seq4repair_space>global<seq2seq4repair_space>facets" , e ) ; } finally { context . searcher ( ) . useGlobalCollectors ( false ) ; } } SearchContextFacets contextFacets = context . facets ( ) ; List < Facet > facets = Lists . newArrayListWithCapacity ( 2 ) ; if ( ( contextFacets . facetCollectors ( ) ) != null ) { for ( FacetCollector facetCollector : contextFacets . facetCollectors ( ) ) { facets . add ( facetCollector . facet ( ) ) ; } } context . queryResult ( ) . facets ( new org . elasticsearch . search . facets . internal . InternalFacets ( facets ) ) ; } }
public final class BytesRefOrdValComparator extends FieldComparator < BytesRef > { final IndexFieldData . WithOrdinals < ? > indexFieldData ; final int [ ] ords ; final SortMode sortMode ; final BytesRef [ ] values ; final int [ ] readerGen ; int currentReaderGen = - 1 ; WithOrdinals termsIndex ; int bottomSlot = - 1 ; int bottomOrd ; boolean bottomSameReader ; BytesRef bottomValue ; final BytesRef tempBR = new BytesRef ( ) ; public BytesRefOrdValComparator ( IndexFieldData . WithOrdinals < ? > indexFieldData , int numHits , SortMode sortMode ) { } @ Override public int compare ( int slot1 , int slot2 ) { } @ Override public int compareBottom ( int doc ) { } @ Override public void copy ( int slot , int doc ) { } @ Override public int compareDocToValue ( int doc , BytesRef value ) { } abstract class PerSegmentComparator extends FieldComparator < BytesRef > { @ Override public FieldComparator < BytesRef > setNextReader ( AtomicReaderContext context ) throws IOException { } @ Override public int compare ( int slot1 , int slot2 ) { } @ Override public void setBottom ( final int bottom ) { } @ Override public BytesRef value ( int slot ) { } @ Override public int compareValues ( BytesRef val1 , BytesRef val2 ) { } @ Override public int compareDocToValue ( int doc , BytesRef value ) { } } private final class ByteOrdComparator extends BytesRefOrdValComparator . PerSegmentComparator { private final byte [ ] readerOrds ; private final WithOrdinals termsIndex ; private final int docBase ; public ByteOrdComparator ( byte [ ] readerOrds , BytesValues . WithOrdinals termsIndex , int docBase ) { } @ Override public int compareBottom ( int doc ) { } @ Override public void copy ( int slot , int doc ) { } } private final class ShortOrdComparator extends BytesRefOrdValComparator . PerSegmentComparator { private final short [ ] readerOrds ; private final WithOrdinals termsIndex ; private final int docBase ; public ShortOrdComparator ( short [ ] readerOrds , BytesValues . WithOrdinals termsIndex , int docBase ) { } @ Override public int compareBottom ( int doc ) { } @ Override public void copy ( int slot , int doc ) { } } private final class IntOrdComparator extends BytesRefOrdValComparator . PerSegmentComparator { private final int [ ] readerOrds ; private final WithOrdinals termsIndex ; private final int docBase ; public IntOrdComparator ( int [ ] readerOrds , BytesValues . WithOrdinals termsIndex , int docBase ) { } @ Override public int compareBottom ( int doc ) { } @ Override public void copy ( int slot , int doc ) { } } final class AnyOrdComparator extends BytesRefOrdValComparator . PerSegmentComparator { private final IndexFieldData fieldData ; private final Docs readerOrds ; private final WithOrdinals termsIndex ; private final int docBase ; public AnyOrdComparator ( IndexFieldData fieldData , BytesValues . WithOrdinals termsIndex , int docBase ) { } @ Override public int compareBottom ( int doc ) { } @ Override public void copy ( int slot , int doc ) { } } @ Override public FieldComparator < BytesRef > setNextReader ( AtomicReaderContext context ) throws IOException { } @ Override public void setBottom ( final int bottom ) { } @ Override public BytesRef value ( int slot ) { } protected static final int binarySearch ( BytesValues . WithOrdinals a , BytesRef key ) { <START_BUG> return BytesRefOrdValComparator . binarySearch ( a , key , 1 , ( ( a . ordinals ( ) . getNumOrds ( ) ) - 1 ) ) ; <END_BUG> } protected static final int binarySearch ( BytesValues . WithOrdinals a , BytesRef key , int low , int high ) { } class MultiAnyOrdComparator extends BytesRefOrdValComparator . PerSegmentComparator { private final WithOrdinals termsIndex ; private final Docs readerOrds ; private MultiAnyOrdComparator ( BytesValues . WithOrdinals termsIndex ) { } @ Override public int compareBottom ( int doc ) throws IOException { } @ Override public void copy ( int slot , int doc ) throws IOException { } @ Override public int compareDocToValue ( int doc , BytesRef value ) { } } static BytesRef getRelevantValue ( BytesValues . WithOrdinals readerValues , int docId , SortMode sortMode ) { } static int getRelevantOrd ( Ordinals . Docs readerOrds , int docId , SortMode sortMode ) { } }
public class AuthenticatorActivity extends RoboSherlockAccountAuthenticatorActivity { public static final String PARAM_AUTHTOKEN_TYPE = "authtokenType" ; private static final String PARAM_CONFIRMCREDENTIALS = "confirmCredentials" ; private static final String PARAM_USERNAME = "username" ; private static final String TAG = "GHAuthenticatorActivity" ; private static void configureSyncFor ( Account account ) { Log . d ( AuthenticatorActivity . TAG , "Trying<seq2seq4repair_space>to<seq2seq4repair_space>configure<seq2seq4repair_space>account<seq2seq4repair_space>for<seq2seq4repair_space>sync..." ) ; ContentResolver . setIsSyncable ( account , Constants . GITHUB_PROVIDER_AUTHORITY , 1 ) ; ContentResolver . setSyncAutomatically ( account , Constants . GITHUB_PROVIDER_AUTHORITY , true ) ; <START_BUG> ContentResolver . addPeriodicSync ( account , Constants . GITHUB_PROVIDER_AUTHORITY , new Bundle ( ) , ( ( long ) ( 15 * 60 ) ) ) ; <END_BUG> } private AccountManager accountManager ; @ InjectView ( id . et_login ) private EditText usernameEdit ; @ InjectView ( id . et_password ) private EditText passwordEdit ; @ InjectView ( id . b_login ) private Button okButton ; @ Inject private LeavingBlankTextFieldWarner leavingBlankTextFieldWarner ; private TextWatcher watcher = validationTextWatcher ( ) ; private RoboAsyncTask < User > authenticationTask ; private String authToken ; private String authTokenType ; private Boolean confirmCredentials = false ; private String password ; protected boolean requestNewAccount = false ; private String username ; @ Override public void onCreate ( Bundle icicle ) { } private void setNonBlankValidationFor ( EditText editText ) { } private TextWatcher validationTextWatcher ( ) { } @ Override protected void onResume ( ) { } private void updateUIWithValidation ( ) { } private boolean populated ( EditText editText ) { } @ Override protected Dialog onCreateDialog ( int id ) { } public void handleLogin ( View view ) { } protected void finishConfirmCredentials ( boolean result ) { } protected void finishLogin ( ) { } private void hideProgress ( ) { } private void showProgress ( ) { } public void onAuthenticationResult ( boolean result ) { } }
public class TransportSearchScrollScanAction extends AbstractComponent { private final ClusterService clusterService ; private final SearchServiceTransportAction searchService ; private final SearchPhaseController searchPhaseController ; @ Inject public TransportSearchScrollScanAction ( Settings settings , ClusterService clusterService , SearchServiceTransportAction searchService , SearchPhaseController searchPhaseController ) { } public void execute ( SearchScrollRequest request , ParsedScrollId scrollId , ActionListener < SearchResponse > listener ) { } private class AsyncAction { private final SearchScrollRequest request ; private final ActionListener < SearchResponse > listener ; private final ParsedScrollId scrollId ; private final DiscoveryNodes nodes ; private volatile AtomicArray < ShardSearchFailure > shardFailures ; private final AtomicArray < QueryFetchSearchResult > queryFetchResults ; private final AtomicInteger successfulOps ; private final AtomicInteger counter ; private final long startTime = System . currentTimeMillis ( ) ; private AsyncAction ( SearchScrollRequest request , ParsedScrollId scrollId , ActionListener < SearchResponse > listener ) { } protected final ShardSearchFailure [ ] buildShardFailures ( ) { } protected final void addShardFailure ( final int shardIndex , ShardSearchFailure failure ) { } public void start ( ) { if ( ( scrollId . getContext ( ) . length ) == 0 ) { <START_BUG> final InternalSearchResponse internalResponse = new InternalSearchResponse ( new InternalSearchHits ( InternalSearchHits . EMPTY , Long . parseLong ( this . scrollId . getAttributes ( ) . get ( "total_hits" ) ) , 0.0F ) , null , null , null , false , null ) ; <END_BUG> listener . onResponse ( new SearchResponse ( internalResponse , request . scrollId ( ) , 0 , 0 , 0L , buildShardFailures ( ) ) ) ; return ; } Tuple < String , Long > [ ] context = scrollId . getContext ( ) ; for ( int i = 0 ; i < ( context . length ) ; i ++ ) { Tuple < String , Long > target = context [ i ] ; DiscoveryNode node = nodes . get ( target . v1 ( ) ) ; if ( node != null ) { executePhase ( i , node , target . v2 ( ) ) ; } else { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ( ( ( ( "Node<seq2seq4repair_space>[" + ( target . v1 ( ) ) ) + "]<seq2seq4repair_space>not<seq2seq4repair_space>available<seq2seq4repair_space>for<seq2seq4repair_space>scroll<seq2seq4repair_space>request<seq2seq4repair_space>[" ) + ( scrollId . getSource ( ) ) ) + "]" ) ) ; } successfulOps . decrementAndGet ( ) ; if ( ( counter . decrementAndGet ( ) ) == 0 ) { finishHim ( ) ; } } } for ( Tuple < String , Long > target : scrollId . getContext ( ) ) { DiscoveryNode node = nodes . get ( target . v1 ( ) ) ; if ( node == null ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ( ( ( ( "Node<seq2seq4repair_space>[" + ( target . v1 ( ) ) ) + "]<seq2seq4repair_space>not<seq2seq4repair_space>available<seq2seq4repair_space>for<seq2seq4repair_space>scroll<seq2seq4repair_space>request<seq2seq4repair_space>[" ) + ( scrollId . getSource ( ) ) ) + "]" ) ) ; } successfulOps . decrementAndGet ( ) ; if ( ( counter . decrementAndGet ( ) ) == 0 ) { finishHim ( ) ; } } else { } } } void executePhase ( final int shardIndex , DiscoveryNode node , final long searchId ) { } void onPhaseFailure ( Throwable t , long searchId , int shardIndex ) { } private void finishHim ( ) { } private void innerFinishHim ( ) throws IOException { } } }
public class Pools { private static final ObjectMap < Class , ReflectionPool > typePools = new ObjectMap ( ) ; public static < T > Pool < T > get ( Class < T > type ) { } public static < T > T obtain ( Class < T > type ) { } public static void free ( Object object ) { } public static void freeAll ( Array objects ) { if ( objects == null ) throw new IllegalArgumentException ( "objects<seq2seq4repair_space>cannot<seq2seq4repair_space>be<seq2seq4repair_space>null." ) ; for ( int i = 0 , n = objects . size ; i < n ; i ++ ) { Object object = objects . get ( i ) ; <START_BUG> if ( objects == null ) <END_BUG> continue ; ReflectionPool pool = Pools . typePools . get ( object . getClass ( ) ) ; if ( pool == null ) return ; pool . free ( object ) ; } } private Pools ( ) { } }
public class ProjectiveTextureTest extends GdxTest { PerspectiveCamera cam ; PerspectiveCamera projector ; Texture texture ; Mesh plane ; Matrix4 planeTrans = new Matrix4 ( ) ; Matrix4 cubeTrans = new Matrix4 ( ) ; Matrix4 modelNormal = new Matrix4 ( ) ; ShaderProgram projTexShader ; Stage ui ; Skin skin ; InputMultiplexer multiplexer = new InputMultiplexer ( ) ; PerspectiveCamController controller ; ImmediateModeRenderer20 renderer ; float angle = 0 ; private SelectBox camera ; private Label fps ; @ Override public void create ( ) { } public void setupScene ( ) { } public void setupUI ( ) { <START_BUG> ui = new Stage ( 480 , 320 , true ) ; <END_BUG> skin = new Skin ( files . internal ( "data/uiskin.json" ) ) ; TextButton reload = new TextButton ( "Reload<seq2seq4repair_space>Shaders" , skin . get ( TextButtonStyle . class ) ) ; camera = new SelectBox ( skin . get ( SelectBoxStyle . class ) ) ; camera . setItems ( "Camera" , "Light" ) ; fps = new Label ( "fps:<seq2seq4repair_space>" , skin . get ( LabelStyle . class ) ) ; Table table = new Table ( ) ; table . setFillParent ( true ) ; table . top ( ) . padTop ( 15 ) ; table . add ( reload ) . spaceRight ( 5 ) ; table . add ( camera ) . spaceRight ( 5 ) ; table . add ( fps ) ; ui . addActor ( table ) ; reload . addListener ( new ClickListener ( ) { public void clicked ( InputEvent event , float x , float y ) { ShaderProgram prog = new ShaderProgram ( files . internal ( "data/shaders/projtex-vert.glsl" ) . readString ( ) , files . internal ( "data/shaders/projtex-frag.glsl" ) . readString ( ) ) ; if ( ( prog . isCompiled ( ) ) == false ) { app . log ( "GLSL<seq2seq4repair_space>ERROR" , ( "Couldn\'t<seq2seq4repair_space>reload<seq2seq4repair_space>shaders:\n" + ( prog . getLog ( ) ) ) ) ; } else { projTexShader . dispose ( ) ; projTexShader = prog ; } } } ) ; } public void setupShaders ( ) { } @ Override public void render ( ) { } Vector3 position = new Vector3 ( ) ; private void renderMesh ( ShaderProgram shader , Matrix4 cam , Matrix4 projector , Matrix4 model , Mesh mesh , Color color ) { } @ Override public void dispose ( ) { } }
public class HistogramAggregator extends BucketsAggregator { private final Numeric valuesSource ; @ Nullable private final ValueFormatter formatter ; private final Rounding rounding ; private final InternalOrder order ; private final boolean keyed ; private final long minDocCount ; private final ExtendedBounds extendedBounds ; private final InternalHistogram . Factory histogramFactory ; private final LongHash bucketOrds ; private SortedNumericDocValues values ; public HistogramAggregator ( String name , AggregatorFactories factories , Rounding rounding , InternalOrder order , boolean keyed , long minDocCount , @ Nullable ExtendedBounds extendedBounds , @ Nullable ValuesSource . Numeric valuesSource , @ Nullable ValueFormatter formatter , long initialCapacity , InternalHistogram . Factory < ? > histogramFactory , AggregationContext aggregationContext , Aggregator parent ) { } @ Override public boolean shouldCollect ( ) { } @ Override public void setNextReader ( AtomicReaderContext reader ) { } @ Override public void collect ( int doc , long owningBucketOrdinal ) throws IOException { } @ Override public InternalAggregation buildAggregation ( long owningBucketOrdinal ) { assert owningBucketOrdinal == 0 ; List < InternalHistogram . Bucket > buckets = new ArrayList < > ( ( ( int ) ( bucketOrds . size ( ) ) ) ) ; for ( long i = 0 ; i < ( bucketOrds . size ( ) ) ; i ++ ) { <START_BUG> buckets . add ( histogramFactory . createBucket ( rounding . valueForKey ( bucketOrds . get ( i ) ) , bucketDocCount ( i ) , bucketAggregations ( i ) , formatter ) ) ; <END_BUG> } CollectionUtil . introSort ( buckets , order . comparator ( ) ) ; InternalHistogram . EmptyBucketInfo emptyBucketInfo = ( ( minDocCount ) == 0 ) ? new InternalHistogram . EmptyBucketInfo ( rounding , buildEmptySubAggregations ( ) , extendedBounds ) : null ; return histogramFactory . create ( name , buckets , order , minDocCount , emptyBucketInfo , formatter , keyed ) ; } @ Override public InternalAggregation buildEmptyAggregation ( ) { } @ Override public void doClose ( ) { } public static class Factory extends ValuesSourceAggregatorFactory < ValuesSource . Numeric > { private final Rounding rounding ; private final InternalOrder order ; private final boolean keyed ; private final long minDocCount ; private final ExtendedBounds extendedBounds ; private final InternalHistogram . Factory < ? > histogramFactory ; public Factory ( String name , ValuesSourceConfig < ValuesSource . Numeric > config , Rounding rounding , InternalOrder order , boolean keyed , long minDocCount , ExtendedBounds extendedBounds , InternalHistogram . Factory < ? > histogramFactory ) { } @ Override protected Aggregator createUnmapped ( AggregationContext aggregationContext , Aggregator parent ) { } @ Override protected Aggregator create ( ValuesSource . Numeric valuesSource , long expectedBucketsCount , AggregationContext aggregationContext , Aggregator parent ) { } } }
public class TextAreaTest extends GdxTest { private Stage stage ; private Skin skin ; @ Override public void create ( ) { } @ Override public void render ( ) { gl . glClearColor ( 0.2F , 0.2F , 0.2F , 1 ) ; gl . glClear ( GL_COLOR_BUFFER_BIT ) ; stage . draw ( ) ; app . log ( "X" , ( "FPS:<seq2seq4repair_space>" + ( graphics . getFramesPerSecond ( ) ) ) ) ; <START_BUG> SpriteBatch spriteBatch = ( ( SpriteBatch ) ( stage . getBatch ( ) ) ) ; <END_BUG> app . log ( "X" , ( "render<seq2seq4repair_space>calls:<seq2seq4repair_space>" + ( spriteBatch . totalRenderCalls ) ) ) ; spriteBatch . totalRenderCalls = 0 ; } @ Override public void resize ( int width , int height ) { } @ Override public void dispose ( ) { } }
public class MockRamIndexStore extends AbstractIndexStore { @ Inject public MockRamIndexStore ( Index index , Settings indexSettings , IndexService indexService , IndicesStore indicesStore ) { } @ Override public boolean persistent ( ) { } @ Override public Class < ? extends DirectoryService > shardDirectory ( ) { <START_BUG> return MockRamDirecorySerivce . class ; <END_BUG> } @ Override public ByteSizeValue backingStoreTotalSpace ( ) { } @ Override public ByteSizeValue backingStoreFreeSpace ( ) { } }
public class ShortValuesComparatorSource extends IndexFieldData . XFieldComparatorSource { private final IndexNumericFieldData indexFieldData ; private final Object missingValue ; public ShortValuesComparatorSource ( IndexNumericFieldData indexFieldData , @ Nullable Object missingValue ) { } @ Override public Type reducedType ( ) { } @ Override public FieldComparator < ? > newComparator ( String fieldname , int numHits , int sortPos , boolean reversed ) throws IOException { assert fieldname . equals ( indexFieldData . getFieldNames ( ) . indexName ( ) ) ; short dMissingValue ; if ( ( ( missingValue ) == null ) || ( "_last" . equals ( missingValue ) ) ) { dMissingValue = ( reversed ) ? Short . MIN_VALUE : Short . MAX_VALUE ; } else if ( "_first" . equals ( missingValue ) ) { dMissingValue = ( reversed ) ? Short . MAX_VALUE : Short . MIN_VALUE ; } else { dMissingValue = ( ( missingValue ) instanceof Number ) ? ( ( Number ) ( missingValue ) ) . shortValue ( ) : Short . parseShort ( missingValue . toString ( ) ) ; } <START_BUG> return new ShortValuesComparator ( indexFieldData , dMissingValue , numHits ) ; <END_BUG> } }
public class ThreadPool extends AbstractComponent { public static class Names { public static final String SAME = "same" ; public static final String CACHED = "cached" ; public static final String INDEX = "index" ; public static final String SEARCH = "search" ; public static final String PERCOLATE = "percolate" ; public static final String MANAGEMENT = "management" ; public static final String MERGE = "merge" ; public static final String SNAPSHOT = "snapshot" ; } private final ImmutableMap < String , Executor > executors ; private final ScheduledExecutorService scheduler ; private final ThreadPool . EstimatedTimeThread estimatedTimeThread ; public ThreadPool ( ) { } @ Inject public ThreadPool ( Settings settings ) { } public long estimatedTimeInMillis ( ) { } public Executor cached ( ) { } public Executor executor ( String name ) { } public ScheduledExecutorService scheduler ( ) { } public ScheduledFuture < ? > scheduleWithFixedDelay ( Runnable command , TimeValue interval ) { } public ScheduledFuture < ? > schedule ( TimeValue delay , String name , Runnable command ) { } public void shutdown ( ) { } public void shutdownNow ( ) { } public boolean awaitTermination ( long timeout , TimeUnit unit ) throws InterruptedException { } private Executor build ( String name , String defaultType , @ Nullable Settings settings , Settings defaultSettings ) { if ( settings == null ) { settings = Builder . EMPTY_SETTINGS ; } String type = settings . get ( "type" , defaultType ) ; ThreadFactory threadFactory = EsExecutors . daemonThreadFactory ( settings , ( ( "[" + name ) + "]" ) ) ; if ( "same" . equals ( type ) ) { logger . debug ( "creating<seq2seq4repair_space>thread_pool<seq2seq4repair_space>[{}],<seq2seq4repair_space>type<seq2seq4repair_space>[{}]" , name , type ) ; return MoreExecutors . sameThreadExecutor ( ) ; } else if ( "cached" . equals ( type ) ) { TimeValue keepAlive = settings . getAsTime ( "keep_alive" , defaultSettings . getAsTime ( "keep_alive" , timeValueMinutes ( 5 ) ) ) ; logger . debug ( "creating<seq2seq4repair_space>thread_pool<seq2seq4repair_space>[{}],<seq2seq4repair_space>type<seq2seq4repair_space>[{}],<seq2seq4repair_space>keep_alive<seq2seq4repair_space>[{}]" , name , type , keepAlive ) ; return new ThreadPoolExecutor ( 0 , Integer . MAX_VALUE , keepAlive . millis ( ) , TimeUnit . MILLISECONDS , new SynchronousQueue < Runnable > ( ) , threadFactory ) ; } else if ( "fixed" . equals ( type ) ) { int size = settings . getAsInt ( "size" , defaultSettings . getAsInt ( "size" , ( ( Runtime . getRuntime ( ) . availableProcessors ( ) ) * 5 ) ) ) ; int queueSize = settings . getAsInt ( "queue_size" , defaultSettings . getAsInt ( "queue_size" , ( - 1 ) ) ) ; RejectedExecutionHandler rejectedExecutionHandler ; String rejectSetting = settings . get ( "reject_policy" , defaultSettings . get ( "reject_policy" , "abort" ) ) ; if ( "abort" . equals ( rejectSetting ) ) { rejectedExecutionHandler = new ThreadPoolExecutor . AbortPolicy ( ) ; } else if ( "caller" . equals ( rejectSetting ) ) { rejectedExecutionHandler = new ThreadPoolExecutor . CallerRunsPolicy ( ) ; } else { throw new ElasticSearchIllegalArgumentException ( ( ( ( ( "reject_policy<seq2seq4repair_space>[" + rejectSetting ) + "]<seq2seq4repair_space>not<seq2seq4repair_space>valid<seq2seq4repair_space>for<seq2seq4repair_space>[" ) + name ) + "]<seq2seq4repair_space>thread<seq2seq4repair_space>pool" ) ) ; } logger . debug ( "creating<seq2seq4repair_space>thread_pool<seq2seq4repair_space>[{}],<seq2seq4repair_space>type<seq2seq4repair_space>[{}],<seq2seq4repair_space>size<seq2seq4repair_space>[{}],<seq2seq4repair_space>queue_size<seq2seq4repair_space>[{}],<seq2seq4repair_space>reject_policy<seq2seq4repair_space>[{}]" , name , type , size , queueSize , rejectSetting ) ; <START_BUG> return new ThreadPoolExecutor ( size , size , 0L , TimeUnit . MILLISECONDS , ( queueSize <= 0 ? new LinkedTransferQueue < Runnable > ( ) : new LinkedBlockingQueue < Runnable > ( queueSize ) ) , threadFactory , rejectedExecutionHandler ) ; <END_BUG> } else if ( "scaling" . equals ( type ) ) { TimeValue keepAlive = settings . getAsTime ( "keep_alive" , defaultSettings . getAsTime ( "keep_alive" , timeValueMinutes ( 5 ) ) ) ; int min = settings . getAsInt ( "min" , defaultSettings . getAsInt ( "min" , 1 ) ) ; int size = settings . getAsInt ( "size" , defaultSettings . getAsInt ( "size" , ( ( Runtime . getRuntime ( ) . availableProcessors ( ) ) * 5 ) ) ) ; logger . debug ( "creating<seq2seq4repair_space>thread_pool<seq2seq4repair_space>[{}],<seq2seq4repair_space>type<seq2seq4repair_space>[{}],<seq2seq4repair_space>min<seq2seq4repair_space>[{}],<seq2seq4repair_space>size<seq2seq4repair_space>[{}],<seq2seq4repair_space>keep_alive<seq2seq4repair_space>[{}]" , name , type , min , size , keepAlive ) ; return DynamicExecutors . newScalingThreadPool ( min , size , keepAlive . millis ( ) , threadFactory ) ; } else if ( "blocking" . equals ( type ) ) { TimeValue keepAlive = settings . getAsTime ( "keep_alive" , defaultSettings . getAsTime ( "keep_alive" , timeValueMinutes ( 5 ) ) ) ; int min = settings . getAsInt ( "min" , defaultSettings . getAsInt ( "min" , 1 ) ) ; int size = settings . getAsInt ( "size" , defaultSettings . getAsInt ( "size" , ( ( Runtime . getRuntime ( ) . availableProcessors ( ) ) * 5 ) ) ) ; SizeValue capacity = settings . getAsSize ( "capacity" , defaultSettings . getAsSize ( "capacity" , new SizeValue ( 0 ) ) ) ; TimeValue waitTime = settings . getAsTime ( "wait_time" , defaultSettings . getAsTime ( "wait_time" , timeValueSeconds ( 60 ) ) ) ; logger . debug ( "creating<seq2seq4repair_space>thread_pool<seq2seq4repair_space>[{}],<seq2seq4repair_space>type<seq2seq4repair_space>[{}],<seq2seq4repair_space>min<seq2seq4repair_space>[{}],<seq2seq4repair_space>size<seq2seq4repair_space>[{}],<seq2seq4repair_space>keep_alive<seq2seq4repair_space>[{}],<seq2seq4repair_space>wait_time<seq2seq4repair_space>[{}]" , name , type , min , size , keepAlive , waitTime ) ; return DynamicExecutors . newBlockingThreadPool ( min , size , keepAlive . millis ( ) , ( ( int ) ( capacity . singles ( ) ) ) , waitTime . millis ( ) , threadFactory ) ; } throw new ElasticSearchIllegalArgumentException ( ( ( ( ( "No<seq2seq4repair_space>type<seq2seq4repair_space>found<seq2seq4repair_space>[" + type ) + "],<seq2seq4repair_space>for<seq2seq4repair_space>[" ) + name ) + "]" ) ) ; } class LoggingRunnable implements Runnable { private final Runnable runnable ; LoggingRunnable ( Runnable runnable ) { } @ Override public void run ( ) { } @ Override public int hashCode ( ) { } @ Override public boolean equals ( Object obj ) { } @ Override public String toString ( ) { } } class ThreadedRunnable implements Runnable { private final Runnable runnable ; private final Executor executor ;
public class TransportNodesListShardStoreMetaData extends TransportNodesOperationAction < TransportNodesListShardStoreMetaData . Request , TransportNodesListShardStoreMetaData . NodesStoreFilesMetaData , TransportNodesListShardStoreMetaData . NodeRequest , TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData > { private final IndicesService indicesService ; private final NodeEnvironment nodeEnv ; @ Inject public TransportNodesListShardStoreMetaData ( Settings settings , ClusterName clusterName , ThreadPool threadPool , ClusterService clusterService , TransportService transportService , IndicesService indicesService , NodeEnvironment nodeEnv ) { } public ActionFuture < TransportNodesListShardStoreMetaData . NodesStoreFilesMetaData > list ( ShardId shardId , boolean onlyUnallocated , Set < String > nodesIds , @ Nullable TimeValue timeout ) { <START_BUG> return execute ( new TransportNodesListShardStoreMetaData . Request ( shardId , onlyUnallocated , nodesIds ) . timeout ( timeout ) ) ; <END_BUG> } @ Override protected String executor ( ) { } @ Override protected String transportAction ( ) { } @ Override protected TransportNodesListShardStoreMetaData . Request newRequest ( ) { } @ Override protected TransportNodesListShardStoreMetaData . NodeRequest newNodeRequest ( ) { } @ Override protected TransportNodesListShardStoreMetaData . NodeRequest newNodeRequest ( String nodeId , TransportNodesListShardStoreMetaData . Request request ) { } @ Override protected TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData newNodeResponse ( ) { } @ Override protected TransportNodesListShardStoreMetaData . NodesStoreFilesMetaData newResponse ( TransportNodesListShardStoreMetaData . Request request , AtomicReferenceArray responses ) { } @ Override protected TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData nodeOperation ( TransportNodesListShardStoreMetaData . NodeRequest request ) throws ElasticSearchException { } private TransportNodesListShardStoreMetaData . StoreFilesMetaData listStoreMetaData ( ShardId shardId ) throws IOException { } @ Override protected boolean accumulateExceptions ( ) { } public static class StoreFilesMetaData implements Iterable < StoreFileMetaData > , Streamable { private boolean allocated ; private ShardId shardId ; private Map < String , StoreFileMetaData > files ; StoreFilesMetaData ( ) { } public StoreFilesMetaData ( boolean allocated , ShardId shardId , Map < String , StoreFileMetaData > files ) { } public boolean allocated ( ) { } public ShardId shardId ( ) { } public long totalSizeInBytes ( ) { } @ Override public Iterator < StoreFileMetaData > iterator ( ) { } public boolean fileExists ( String name ) { } public StoreFileMetaData file ( String name ) { } public static TransportNodesListShardStoreMetaData . StoreFilesMetaData readStoreFilesMetaData ( StreamInput in ) throws IOException { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } static class Request extends NodesOperationRequest < TransportNodesListShardStoreMetaData . Request > { private ShardId shardId ; private boolean unallocated ; public Request ( ) { } public Request ( ShardId shardId , boolean unallocated , Set < String > nodesIds ) { } public Request ( ShardId shardId , boolean unallocated , String ... nodesIds ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } public static class NodesStoreFilesMetaData extends NodesOperationResponse < TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData > { private FailedNodeException [ ] failures ; NodesStoreFilesMetaData ( ) { } public NodesStoreFilesMetaData ( ClusterName clusterName , TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData [ ] nodes , FailedNodeException [ ] failures ) { } public FailedNodeException [ ] failures ( ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } static class NodeRequest extends NodeOperationRequest { private ShardId shardId ; private boolean unallocated ; NodeRequest ( ) { } NodeRequest ( String nodeId , TransportNodesListShardStoreMetaData . Request request ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } public static class NodeStoreFilesMetaData extends NodeOperationResponse { private TransportNodesListShardStoreMetaData . StoreFilesMetaData storeFilesMetaData ; NodeStoreFilesMetaData ( ) { } public NodeStoreFilesMetaData ( DiscoveryNode node , TransportNodesListShardStoreMetaData . StoreFilesMetaData storeFilesMetaData ) { } public TransportNodesListShardStoreMetaData . StoreFilesMetaData storeFilesMetaData ( ) { } public static TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData readListShardStoreNodeOperationResponse ( StreamInput in ) throws IOException { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } }
public class IndexShardGatewayService extends AbstractIndexShardComponent implements CloseableIndexComponent { private final boolean snapshotOnClose ; private final ThreadPool threadPool ; private final InternalIndexShard indexShard ; private final IndexShardGateway shardGateway ; private volatile long lastIndexVersion ; private volatile long lastTranslogId = - 1 ; private volatile int lastTotalTranslogOperations ; private volatile long lastTranslogLength ; private final TimeValue snapshotInterval ; private volatile ScheduledFuture snapshotScheduleFuture ; private RecoveryStatus recoveryStatus ; private SnapshotLock snapshotLock ; @ Inject public IndexShardGatewayService ( ShardId shardId , @ IndexSettings Settings indexSettings , ThreadPool threadPool , IndexShard indexShard , IndexShardGateway shardGateway ) { } public void routingStateChanged ( ) { } public static interface RecoveryListener { void onRecoveryDone ( ) { } void onIgnoreRecovery ( String reason ) { } void onRecoveryFailed ( IndexShardGatewayRecoveryException e ) { } } public RecoveryStatus recoveryStatus ( ) { } public SnapshotStatus snapshotStatus ( ) { } public void recover ( final IndexShardGatewayService . RecoveryListener listener ) throws IgnoreGatewayRecoveryException , IndexShardGatewayRecoveryException { } public synchronized void snapshot ( final String reason ) throws IndexShardGatewaySnapshotFailedException { if ( ! ( indexShard . routingEntry ( ) . primary ( ) ) ) { return ; } if ( indexShard . routingEntry ( ) . relocating ( ) ) { return ; } if ( ( indexShard . state ( ) ) == ( IndexShardState . CREATED ) ) { return ; } if ( ( indexShard . state ( ) ) == ( IndexShardState . RECOVERING ) ) { return ; } if ( ( snapshotLock ) == null ) { try { snapshotLock = shardGateway . obtainSnapshotLock ( ) ; } catch ( Exception e ) { logger . warn ( "failed<seq2seq4repair_space>to<seq2seq4repair_space>obtain<seq2seq4repair_space>snapshot<seq2seq4repair_space>lock,<seq2seq4repair_space>ignoring<seq2seq4repair_space>snapshot" , e ) ; return ; } } try { SnapshotStatus snapshotStatus = indexShard . snapshot ( new Engine . SnapshotHandler < SnapshotStatus > ( ) { @ Override public SnapshotStatus snapshot ( SnapshotIndexCommit snapshotIndexCommit , Translog . Snapshot translogSnapshot ) throws EngineException { if ( ( ( ( lastIndexVersion ) != ( snapshotIndexCommit . getVersion ( ) ) ) || ( ( lastTranslogId ) != ( translogSnapshot . translogId ( ) ) ) ) || ( ( lastTranslogLength ) < ( translogSnapshot . length ( ) ) ) ) { logger . debug ( "snapshot<seq2seq4repair_space>({})<seq2seq4repair_space>to<seq2seq4repair_space>{}<seq2seq4repair_space>..." , reason , shardGateway ) ; SnapshotStatus snapshotStatus = shardGateway . snapshot ( new IndexShardGateway . Snapshot ( snapshotIndexCommit , translogSnapshot , lastIndexVersion , lastTranslogId , lastTranslogLength , lastTotalTranslogOperations ) ) ; lastIndexVersion = snapshotIndexCommit . getVersion ( ) ; lastTranslogId = translogSnapshot . translogId ( ) ; lastTranslogLength = translogSnapshot . length ( ) ; <START_BUG> lastTotalTranslogOperations = translogSnapshot . totalOperations ( ) ; <END_BUG> return snapshotStatus ; } return null ; } } ) ; if ( snapshotStatus != null ) { if ( logger . isDebugEnabled ( ) ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( "snapshot<seq2seq4repair_space>(" ) . append ( reason ) . append ( ")<seq2seq4repair_space>completed<seq2seq4repair_space>to<seq2seq4repair_space>" ) . append ( shardGateway ) . append ( ",<seq2seq4repair_space>took<seq2seq4repair_space>[" ) . append ( TimeValue . timeValueMillis ( snapshotStatus . time ( ) ) ) . append ( "]\n" ) ; sb . append ( "<seq2seq4repair_space>index<seq2seq4repair_space>:<seq2seq4repair_space>version<seq2seq4repair_space>[" ) . append ( lastIndexVersion ) . append ( "],<seq2seq4repair_space>number_of_files<seq2seq4repair_space>[" ) . append ( snapshotStatus . index ( ) . numberOfFiles ( ) ) . append ( "]<seq2seq4repair_space>with<seq2seq4repair_space>total_size<seq2seq4repair_space>[" ) . append ( new org . elasticsearch . common . unit . ByteSizeValue ( snapshotStatus . index ( ) . totalSize ( ) ) ) . append ( "],<seq2seq4repair_space>took<seq2seq4repair_space>[" ) . append ( TimeValue . timeValueMillis ( snapshotStatus . index ( ) . time ( ) ) ) . append ( "]\n" ) ; sb . append ( "<seq2seq4repair_space>translog<seq2seq4repair_space>:<seq2seq4repair_space>id<seq2seq4repair_space>[" ) . append ( lastTranslogId ) . append ( "],<seq2seq4repair_space>number_of_operations<seq2seq4repair_space>[" ) . append ( snapshotStatus . translog ( ) . expectedNumberOfOperations ( ) ) . append ( "],<seq2seq4repair_space>took<seq2seq4repair_space>[" ) . append ( TimeValue . timeValueMillis ( snapshotStatus . translog ( ) . time ( ) ) ) . append ( "]" ) ; logger . debug ( sb . toString ( ) ) ; } } } catch ( SnapshotFailedEngineException e ) { if ( ( e . getCause ( ) ) instanceof IllegalStateException ) { } else { throw new IndexShardGatewaySnapshotFailedException ( shardId , "Failed<seq2seq4repair_space>to<seq2seq4repair_space>snapshot" , e ) ; } } catch ( IllegalIndexShardStateException e ) { } catch ( IndexShardGatewaySnapshotFailedException e ) { throw e ; } catch ( Exception e ) { throw new IndexShardGatewaySnapshotFailedException ( shardId , "Failed<seq2seq4repair_space>to<seq2seq4repair_space>snapshot" , e ) ; } } public void snapshotOnClose ( ) { } public synchronized void close ( boolean delete ) { } private synchronized void scheduleSnapshotIfNeeded ( ) { } private class SnapshotRunnable implements Runnable { @ Override public void run ( ) { } } }
public class HasParentFilterParser implements FilterParser { public static final String NAME = "has_parent" ; @ Inject public HasParentFilterParser ( ) { } @ Override public String [ ] names ( ) { } @ Override public Filter parse ( QueryParseContext parseContext ) throws IOException , QueryParsingException { XContentParser parser = parseContext . parser ( ) ; Query query = null ; boolean queryFound = false ; String parentType = null ; boolean cache = false ; CacheKeyFilter . Key cacheKey = null ; String filterName = null ; String currentFieldName = null ; XContentParser . Token token ; while ( ( token = parser . nextToken ( ) ) != ( Token . END_OBJECT ) ) { if ( token == ( Token . FIELD_NAME ) ) { currentFieldName = parser . currentName ( ) ; } else if ( token == ( Token . START_OBJECT ) ) { if ( "query" . equals ( currentFieldName ) ) { String [ ] origTypes = QueryParseContext . setTypesWithPrevious ( ( parentType == null ? null : new String [ ] { parentType } ) ) ; try { query = parseContext . parseInnerQuery ( ) ; queryFound = true ; } finally { QueryParseContext . setTypes ( origTypes ) ; } } else if ( "filter" . equals ( currentFieldName ) ) { String [ ] origTypes = QueryParseContext . setTypesWithPrevious ( ( parentType == null ? null : new String [ ] { parentType } ) ) ; try { Filter innerFilter = parseContext . parseInnerFilter ( ) ; query = new org . elasticsearch . common . lucene . search . XConstantScoreQuery ( innerFilter ) ; queryFound = true ; } finally { QueryParseContext . setTypes ( origTypes ) ; } } else { throw new QueryParsingException ( parseContext . index ( ) , ( ( "[has_parent]<seq2seq4repair_space>filter<seq2seq4repair_space>does<seq2seq4repair_space>not<seq2seq4repair_space>support<seq2seq4repair_space>[" + currentFieldName ) + "]" ) ) ; } } else if ( token . isValue ( ) ) { if ( ( ( "type" . equals ( currentFieldName ) ) || ( "parent_type" . equals ( currentFieldName ) ) ) || ( "parentType" . equals ( currentFieldName ) ) ) { parentType = parser . text ( ) ; } else if ( "_scope" . equals ( currentFieldName ) ) { throw new QueryParsingException ( parseContext . index ( ) , "the<seq2seq4repair_space>[_scope]<seq2seq4repair_space>support<seq2seq4repair_space>in<seq2seq4repair_space>[has_parent]<seq2seq4repair_space>filter<seq2seq4repair_space>has<seq2seq4repair_space>been<seq2seq4repair_space>removed,<seq2seq4repair_space>use<seq2seq4repair_space>a<seq2seq4repair_space>filter<seq2seq4repair_space>as<seq2seq4repair_space>a<seq2seq4repair_space>facet_filter<seq2seq4repair_space>in<seq2seq4repair_space>the<seq2seq4repair_space>relevant<seq2seq4repair_space>global<seq2seq4repair_space>facet" ) ; } else if ( "_name" . equals ( currentFieldName ) ) { filterName = parser . text ( ) ; } else if ( "_cache" . equals ( currentFieldName ) ) { cache = parser . booleanValue ( ) ; } else if ( ( "_cache_key" . equals ( currentFieldName ) ) || ( "_cacheKey" . equals ( currentFieldName ) ) ) { cacheKey = new CacheKeyFilter . Key ( parser . text ( ) ) ; } else { throw new QueryParsingException ( parseContext . index ( ) , ( ( "[has_parent]<seq2seq4repair_space>filter<seq2seq4repair_space>does<seq2seq4repair_space>not<seq2seq4repair_space>support<seq2seq4repair_space>[" + currentFieldName ) + "]" ) ) ; } } } if ( ! queryFound ) { throw new QueryParsingException ( parseContext . index ( ) , "[has_parent]<seq2seq4repair_space>filter<seq2seq4repair_space>requires<seq2seq4repair_space>'query'<seq2seq4repair_space>field" ) ; } if ( query == null ) { return null ; } if ( parentType == null ) { throw new QueryParsingException ( parseContext . index ( ) , "[has_parent]<seq2seq4repair_space>filter<seq2seq4repair_space>requires<seq2seq4repair_space>'parent_type'<seq2seq4repair_space>field" ) ; } DocumentMapper parentDocMapper = parseContext . mapperService ( ) . documentMapper ( parentType ) ; if ( parentDocMapper == null ) { throw new QueryParsingException ( parseContext . index ( ) , ( ( "[has_parent]<seq2seq4repair_space>filter<seq2seq4repair_space>configured<seq2seq4repair_space>'parent_type'<seq2seq4repair_space>[" + parentType ) + "]<seq2seq4repair_space>is<seq2seq4repair_space>not<seq2seq4repair_space>a<seq2seq4repair_space>valid<seq2seq4repair_space>type" ) ) ; } query = new org . elasticsearch . common . lucene . search . XFilteredQuery ( query , parseContext . cacheFilter ( parentDocMapper . typeFilter ( ) , null ) ) ; Set < String > parentTypes = new HashSet < String > ( 5 ) ; parentTypes . add ( parentType ) ; for ( DocumentMapper documentMapper : parseContext . mapperService ( ) ) { ParentFieldMapper parentFieldMapper = documentMapper . parentFieldMapper ( ) ; <START_BUG> if ( parentFieldMapper != null ) { <END_BUG> DocumentMapper parentTypeDocumentMapper = parseContext . mapperService ( ) . documentMapper ( parentFieldMapper . type ( ) ) ; if ( parentTypeDocumentMapper == null ) { parentTypes . add ( parentFieldMapper . type ( ) ) ; } } } Filter parentFilter ; if ( ( parentTypes . size ( ) ) == 1 ) { DocumentMapper documentMapper = parseContext . mapperService ( ) . documentMapper ( parentTypes . iterator ( ) . next ( ) ) ; parentFilter = parseContext . cacheFilter ( documentMapper . typeFilter ( ) , null ) ; } else { XBooleanFilter parentsFilter = new XBooleanFilter ( ) ; for ( String parentTypeStr : parentTypes ) { DocumentMapper documentMapper = parseContext . mapperService ( ) . documentMapper ( parentTypeStr ) ; Filter filter = parseContext . cacheFilter ( documentMapper . typeFilter ( ) , null ) ; parentsFilter . add ( filter , SHOULD ) ; } parentFilter = parentsFilter ; } Filter childrenFilter = parseContext . cacheFilter ( new org . elasticsearch . common . lucene . search . NotFilter ( parentFilter ) , null ) ; Query parentConstantScoreQuery = new org . elasticsearch . index . search . child . ParentConstantScoreQuery ( query , parentType , childrenFilter , false ) ; if ( filterName != null ) { parseContext . addNamedQuery ( filterName , parentConstantScoreQuery ) ; } boolean deleteByQuery = "delete_by_query" . equals ( SearchContext . current ( ) . source ( ) ) ; if ( deleteByQuery ) { return new org . elasticsearch . index . search . child . DeleteByQueryWrappingFilter ( parentConstantScoreQuery ) ; } else { return new org . elasticsearch . index . search . child . CustomQueryWrappingFilter ( parentConstantScoreQuery ) ; } } }
public class HtmlUtilsTest { @ Test public void toggleRemoved ( ) { } @ Test public void togglesRemoved ( ) { } @ Test public void emailQuoted ( ) { } @ Test public void emailFragment ( ) { } @ Test public void emailFragments ( ) { } @ Test public void trailingEmailFragment ( ) { } @ Test public void leadingBreak ( ) { } @ Test public void trailingBreak ( ) { } @ Test public void wrappedBreaks ( ) { } @ Test public void wrappedParagraphs ( ) { } @ Test public void leadingWhitespace ( ) { } @ Test public void trailingWhitespace ( ) { } @ Test public void wrappedWhitetspace ( ) { } @ Test public void preWithNoWhitespace ( ) { } @ Test public void preWithSpaces ( ) { } @ Test public void preWithTabs ( ) { } @ Test public void preWithNewline ( ) { } @ Test public void preWithAllWhitepsace ( ) { } @ Test public void multiplePresEscaped ( ) { } @ Test public void formatPreCodeOnly ( ) { String html = "<pre><code>a\nb\nc\n</code></pre>" ; CharSequence formatted = HtmlUtils . format ( html ) ; assertNotNull ( formatted ) ; <START_BUG> assertEquals ( "<pre><code>a<br>b<br>c<br></code></pre>" , formatted . toString ( ) ) ; <END_BUG> } }
public class FsTranslog extends AbstractIndexShardComponent implements Translog { class ApplySettings implements IndexSettingsService . Listener { @ Override public void onRefreshSettings ( Settings settings ) { } } private final IndexSettingsService indexSettingsService ; private final ReadWriteLock rwl = new ReentrantReadWriteLock ( ) ; private final File [ ] locations ; private volatile FsTranslogFile current ; private volatile FsTranslogFile trans ; private Type type ; private boolean syncOnEachOperation = false ; private int bufferSize ; private int transientBufferSize ; private final FsTranslog . ApplySettings applySettings = new FsTranslog . ApplySettings ( ) ; @ Inject public FsTranslog ( ShardId shardId , @ IndexSettings Settings indexSettings , IndexSettingsService indexSettingsService , NodeEnvironment nodeEnv ) { } public FsTranslog ( ShardId shardId , @ IndexSettings Settings indexSettings , File location ) { } @ Override public void close ( boolean delete ) { } public File [ ] locations ( ) { } @ Override public long currentId ( ) { } @ Override public int estimatedNumberOfOperations ( ) { } @ Override public long memorySizeInBytes ( ) { } @ Override public long translogSizeInBytes ( ) { } @ Override public void clearUnreferenced ( ) { } @ Override public void newTranslog ( long id ) throws TranslogException { } @ Override public void newTransientTranslog ( long id ) throws TranslogException { } @ Override public void makeTransientCurrent ( ) { } @ Override public void revertTransient ( ) { } public byte [ ] read ( Location location ) { } @ Override public Location add ( Operation operation ) throws TranslogException { CachedStreamOutput . Entry cachedEntry = CachedStreamOutput . popEntry ( ) ; rwl . readLock ( ) . lock ( ) ; try { <START_BUG> BytesStreamOutput out = cachedEntry . cachedBytes ( ) ; <END_BUG> out . writeInt ( 0 ) ; TranslogStreams . writeTranslogOperation ( out , operation ) ; out . flush ( ) ; int size = out . size ( ) ; out . seek ( 0 ) ; out . writeInt ( ( size - 4 ) ) ; Location location = current . add ( out . underlyingBytes ( ) , 0 , size ) ; if ( syncOnEachOperation ) { current . sync ( ) ; } FsTranslogFile trans = this . trans ; if ( trans != null ) { try { location = trans . add ( out . underlyingBytes ( ) , 0 , size ) ; } catch ( ClosedChannelException e ) { } } return location ; } catch ( Exception e ) { throw new TranslogException ( shardId , ( ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>write<seq2seq4repair_space>operation<seq2seq4repair_space>[" + operation ) + "]" ) , e ) ; } finally { rwl . readLock ( ) . unlock ( ) ; CachedStreamOutput . pushEntry ( cachedEntry ) ; } } @ Override public FsChannelSnapshot snapshot ( ) throws TranslogException { } @ Override public Snapshot snapshot ( Snapshot snapshot ) { } @ Override public void sync ( ) { } @ Override public boolean syncNeeded ( ) { } @ Override public void syncOnEachOperation ( boolean syncOnEachOperation ) { } }
public class SearchContext implements Releasable { private static ThreadLocal < SearchContext > current = new ThreadLocal < SearchContext > ( ) ; public static void setCurrent ( SearchContext value ) { } public static void removeCurrent ( ) { } public static SearchContext current ( ) { } private final long id ; private final InternalSearchRequest request ; private final SearchShardTarget shardTarget ; private SearchType searchType ; private final Searcher engineSearcher ; private final ScriptService scriptService ; private final IndexShard indexShard ; private final IndexService indexService ; private final ContextIndexSearcher searcher ; private final DfsSearchResult dfsResult ; private final QuerySearchResult queryResult ; private final FetchSearchResult fetchResult ; private ScanContext scanContext ; private float queryBoost = 1.0F ; private long timeoutInMillis = - 1 ; private List < String > groupStats ; private Scroll scroll ; private boolean explain ; private boolean version = false ; private List < String > fieldNames ; private ScriptFieldsContext scriptFields ; private PartialFieldsContext partialFields ; private int from = - 1 ; private int size = - 1 ; private Sort sort ; private Float minimumScore ; private boolean trackScores = false ; private ParsedQuery originalQuery ; private Query query ; private Filter filter ; private Filter aliasFilter ; private int [ ] docIdsToLoad ; private int docsIdsToLoadFrom ; private int docsIdsToLoadSize ; private SearchContextFacets facets ; private SearchContextHighlight highlight ; private SearchLookup searchLookup ; private boolean queryRewritten ; private volatile long keepAlive ; private volatile long lastAccessTime ; private List < ScopePhase > scopePhases = null ; private Map < String , BlockJoinQuery > nestedQueries ; public SearchContext ( long id , InternalSearchRequest request , SearchShardTarget shardTarget , Engine . Searcher engineSearcher , IndexService indexService , IndexShard indexShard , ScriptService scriptService ) { } @ Override public boolean release ( ) throws ElasticSearchException { } public void preProcess ( ) { if ( ( query ( ) ) == null ) { parsedQuery ( MATCH_ALL_PARSED_QUERY ) ; } if ( ( queryBoost ( ) ) != 1.0F ) { parsedQuery ( new ParsedQuery ( new org . elasticsearch . common . lucene . search . function . FunctionScoreQuery ( query ( ) , new BoostScoreFunction ( queryBoost ) ) , parsedQuery ( ) ) ) ; } Filter searchFilter = mapperService ( ) . searchFilter ( types ( ) ) ; if ( searchFilter != null ) { <START_BUG> if ( Queries . isMatchAllQuery ( query ( ) ) ) { <END_BUG> Query q = new DeletionAwareConstantScoreQuery ( filterCache ( ) . cache ( searchFilter ) ) ; q . setBoost ( query ( ) . getBoost ( ) ) ; parsedQuery ( new ParsedQuery ( q , parsedQuery ( ) ) ) ; } else { parsedQuery ( new ParsedQuery ( new FilteredQuery ( query ( ) , filterCache ( ) . cache ( searchFilter ) ) , parsedQuery ( ) ) ) ; } } } public long id ( ) { } public InternalSearchRequest request ( ) { } public SearchType searchType ( ) { } public SearchContext searchType ( SearchType searchType ) { } public SearchShardTarget shardTarget ( ) { } public int numberOfShards ( ) { } public boolean hasTypes ( ) { } public String [ ] types ( ) { } public float queryBoost ( ) { } public SearchContext queryBoost ( float queryBoost ) { } public long nowInMillis ( ) { } public Scroll scroll ( ) { } public SearchContext scroll ( Scroll scroll ) { } public SearchContextFacets facets ( ) { } public SearchContext facets ( SearchContextFacets facets ) { } public SearchContextHighlight highlight ( ) { } public void highlight ( SearchContextHighlight highlight ) { } public boolean hasScriptFields ( ) { } public ScriptFieldsContext scriptFields ( ) { } public boolean hasPartialFields ( ) { } public PartialFieldsContext partialFields ( ) { } public ContextIndexSearcher searcher ( ) { } public IndexShard indexShard ( ) { } public MapperService mapperService ( ) { } public AnalysisService analysisService ( ) { } public IndexQueryParserService queryParserService ( ) { } public SimilarityService similarityService ( ) { } public ScriptService scriptService ( ) { } public FilterCache filterCache ( ) { } public FieldDataCache fieldDataCache ( ) { } public IdCache idCache ( ) { } public long timeoutInMillis ( ) { } public void timeoutInMillis ( long timeoutInMillis ) { } public SearchContext minimumScore ( float minimumScore ) { } public Float minimumScore ( ) { } public SearchContext sort ( Sort sort ) { } public Sort sort ( ) { } public SearchContext trackScores ( boolean trackScores ) { } public boolean trackScores ( ) { } public SearchContext parsedFilter ( Filter filter ) { } public Filter parsedFilter ( ) { } public Filter aliasFilter ( ) { } public SearchContext parsedQuery ( ParsedQuery query ) { } public ParsedQuery parsedQuery ( ) { } public Query query ( ) { } public boolean queryRewritten ( ) { } public SearchContext updateRewriteQuery ( Query rewriteQuery ) { } public int from ( ) { } public SearchContext from ( int from ) { } public int size ( ) { } public SearchContext size ( int size ) { } public boolean hasFieldNames ( ) { } public List < String > fieldNames ( ) { } public void emptyFieldNames ( ) { } public boolean explain ( ) { } public void explain ( boolean explain ) { } @ Nullable public List < String > groupStats ( ) { } public void groupStats ( List < String > groupStats ) { } public boolean version ( ) { } public void version ( boolean version ) { } public int [ ] docIdsToLoad ( ) { } public int docIdsToLoadFrom ( ) { } public int docIdsToLoadSize ( ) { } public SearchContext docIdsToLoad ( int [ ] docIdsToLoad , int docsIdsToLoadFrom , int docsIdsToLoadSize ) { } public void accessed ( long accessTime ) { }
public class NestedTests extends ElasticsearchIntegrationTest { @ Override public Settings indexSettings ( ) { } int numParents ; int [ ] numChildren ; @ Before public void init ( ) throws Exception { } @ Test public void simple ( ) throws Exception { } @ Test public void onNonNestedField ( ) throws Exception { } @ Test public void nestedWithSubTermsAgg ( ) throws Exception { } @ Test public void nestedAsSubAggregation ( ) throws Exception { } @ Test public void emptyAggregation ( ) throws Exception { prepareCreate ( "empty_bucket_idx" ) . addMapping ( "type" , "value" , "type=integer" , "nested" , "type=nested" ) . execute ( ) . actionGet ( ) ; List < IndexRequestBuilder > builders = new ArrayList < IndexRequestBuilder > ( ) ; for ( int i = 0 ; i < 2 ; i ++ ) { builders . add ( client ( ) . prepareIndex ( "empty_bucket_idx" , "type" , ( "" + i ) ) . setSource ( jsonBuilder ( ) . startObject ( ) . field ( "value" , ( i * 2 ) ) . startArray ( "nested" ) . startObject ( ) . field ( "value" , ( i + 1 ) ) . endObject ( ) . startObject ( ) . field ( "value" , ( i + 2 ) ) . endObject ( ) . startObject ( ) . field ( "value" , ( i + 3 ) ) . endObject ( ) . startObject ( ) . field ( "value" , ( i + 4 ) ) . endObject ( ) . startObject ( ) . field ( "value" , ( i + 5 ) ) . endObject ( ) . endArray ( ) . endObject ( ) ) ) ; } indexRandom ( true , builders . toArray ( new IndexRequestBuilder [ builders . size ( ) ] ) ) ; <START_BUG> SearchResponse searchResponse = client ( ) . prepareSearch ( "empty_bucket_idx" ) . setQuery ( matchAllQuery ( ) ) . addAggregation ( histogram ( "histo" ) . field ( "value" ) . interval ( 1L ) . emptyBuckets ( true ) . subAggregation ( nested ( "nested" ) . path ( "nested" ) ) ) . execute ( ) . actionGet ( ) ; <END_BUG> assertThat ( searchResponse . getHits ( ) . getTotalHits ( ) , equalTo ( 2L ) ) ; Histogram histo = searchResponse . getAggregations ( ) . get ( "histo" ) ; assertThat ( histo , Matchers . notNullValue ( ) ) ; Histogram . Bucket bucket = histo . getByKey ( 1L ) ; assertThat ( bucket , Matchers . notNullValue ( ) ) ; Nested nested = bucket . getAggregations ( ) . get ( "nested" ) ; assertThat ( nested , Matchers . notNullValue ( ) ) ; assertThat ( nested . getName ( ) , equalTo ( "nested" ) ) ; assertThat ( nested . getDocCount ( ) , is ( 0L ) ) ; } }
public class SimpleNodesInfoTests extends AbstractNodesTests { @ AfterMethod public void closeNodes ( ) { } @ Test public void testNodesInfos ( ) { startNode ( "server1" ) ; startNode ( "server2" ) ; String server1NodeId = ( ( org . elasticsearch . node . internal . InternalNode ) ( node ( "server1" ) ) ) . injector ( ) . getInstance ( ClusterService . class ) . state ( ) . nodes ( ) . localNodeId ( ) ; String server2NodeId = ( ( org . elasticsearch . node . internal . InternalNode ) ( node ( "server2" ) ) ) . injector ( ) . getInstance ( ClusterService . class ) . state ( ) . nodes ( ) . localNodeId ( ) ; <START_BUG> NodesInfoResponse response = client ( "server1" ) . admin ( ) . cluster ( ) . nodesInfo ( nodesInfo ( ) ) . actionGet ( ) ; <END_BUG> assertThat ( response . nodes ( ) . length , equalTo ( 2 ) ) ; assertThat ( response . nodesMap ( ) . get ( server1NodeId ) , notNullValue ( ) ) ; assertThat ( response . nodesMap ( ) . get ( server2NodeId ) , notNullValue ( ) ) ; response = client ( "server2" ) . admin ( ) . cluster ( ) . nodesInfo ( nodesInfo ( ) ) . actionGet ( ) ; assertThat ( response . nodes ( ) . length , equalTo ( 2 ) ) ; assertThat ( response . nodesMap ( ) . get ( server1NodeId ) , notNullValue ( ) ) ; assertThat ( response . nodesMap ( ) . get ( server2NodeId ) , notNullValue ( ) ) ; response = client ( "server1" ) . admin ( ) . cluster ( ) . nodesInfo ( nodesInfo ( server1NodeId ) ) . actionGet ( ) ; assertThat ( response . nodes ( ) . length , equalTo ( 1 ) ) ; assertThat ( response . nodesMap ( ) . get ( server1NodeId ) , notNullValue ( ) ) ; response = client ( "server2" ) . admin ( ) . cluster ( ) . nodesInfo ( nodesInfo ( server1NodeId ) ) . actionGet ( ) ; assertThat ( response . nodes ( ) . length , equalTo ( 1 ) ) ; assertThat ( response . nodesMap ( ) . get ( server1NodeId ) , notNullValue ( ) ) ; response = client ( "server1" ) . admin ( ) . cluster ( ) . nodesInfo ( nodesInfo ( server2NodeId ) ) . actionGet ( ) ; assertThat ( response . nodes ( ) . length , equalTo ( 1 ) ) ; assertThat ( response . nodesMap ( ) . get ( server2NodeId ) , notNullValue ( ) ) ; response = client ( "server2" ) . admin ( ) . cluster ( ) . nodesInfo ( nodesInfo ( server2NodeId ) ) . actionGet ( ) ; assertThat ( response . nodes ( ) . length , equalTo ( 1 ) ) ; assertThat ( response . nodesMap ( ) . get ( server2NodeId ) , notNullValue ( ) ) ; } }
public class RestValidateQueryAction extends BaseRestHandler { @ Inject public RestValidateQueryAction ( Settings settings , Client client , RestController controller ) { } @ Override public void handleRequest ( final RestRequest request , final RestChannel channel ) { ValidateQueryRequest validateQueryRequest = new ValidateQueryRequest ( RestActions . splitIndices ( request . param ( "index" ) ) ) ; validateQueryRequest . listenerThreaded ( false ) ; try { BroadcastOperationThreading operationThreading = BroadcastOperationThreading . fromString ( request . param ( "operation_threading" ) , SINGLE_THREAD ) ; if ( operationThreading == ( BroadcastOperationThreading . NO_THREADS ) ) { operationThreading = BroadcastOperationThreading . SINGLE_THREAD ; } validateQueryRequest . operationThreading ( operationThreading ) ; if ( request . hasContent ( ) ) { <START_BUG> validateQueryRequest . query ( request . contentByteArray ( ) , request . contentByteArrayOffset ( ) , request . contentLength ( ) , true ) ; <END_BUG> } else { String source = request . param ( "source" ) ; if ( source != null ) { validateQueryRequest . query ( source ) ; } else { BytesReference querySource = RestActions . parseQuerySource ( request ) ; if ( querySource != null ) { validateQueryRequest . query ( querySource , false ) ; } } } validateQueryRequest . types ( splitTypes ( request . param ( "type" ) ) ) ; if ( request . paramAsBoolean ( "explain" , false ) ) { validateQueryRequest . explain ( true ) ; } else { validateQueryRequest . explain ( false ) ; } } catch ( Exception e ) { try { XContentBuilder builder = RestXContentBuilder . restContentBuilder ( request ) ; channel . sendResponse ( new XContentRestResponse ( request , RestStatus . BAD_REQUEST , builder . startObject ( ) . field ( "error" , e . getMessage ( ) ) . endObject ( ) ) ) ; } catch ( IOException e1 ) { logger . error ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>failure<seq2seq4repair_space>response" , e1 ) ; } return ; } client . admin ( ) . indices ( ) . validateQuery ( validateQueryRequest , new org . elasticsearch . action . ActionListener < ValidateQueryResponse > ( ) { @ Override public void onResponse ( ValidateQueryResponse response ) { try { XContentBuilder builder = RestXContentBuilder . restContentBuilder ( request ) ; builder . startObject ( ) ; builder . field ( "valid" , response . valid ( ) ) ; buildBroadcastShardsHeader ( builder , response ) ; if ( ( ( response . queryExplanations ( ) ) != null ) && ( ! ( response . queryExplanations ( ) . isEmpty ( ) ) ) ) { builder . startArray ( "explanations" ) ; for ( QueryExplanation explanation : response . queryExplanations ( ) ) { builder . startObject ( ) ; if ( ( explanation . index ( ) ) != null ) { builder . field ( "index" , explanation . index ( ) , NONE ) ; } builder . field ( "valid" , explanation . valid ( ) ) ; if ( ( explanation . error ( ) ) != null ) { builder . field ( "error" , explanation . error ( ) ) ; } if ( ( explanation . explanation ( ) ) != null ) { builder . field ( "explanation" , explanation . explanation ( ) ) ; } builder . endObject ( ) ; } builder . endArray ( ) ; } builder . endObject ( ) ; channel . sendResponse ( new XContentRestResponse ( request , RestStatus . OK , builder ) ) ; } catch ( Exception e ) { onFailure ( e ) ; } } @ Override public void onFailure ( Throwable e ) { try { channel . sendResponse ( new XContentThrowableRestResponse ( request , e ) ) ; } catch ( IOException e1 ) { logger . error ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>failure<seq2seq4repair_space>response" , e1 ) ; } } } ) ; } }
public class TransportSearchFailuresTests extends AbstractNodesTests { @ AfterMethod public void closeNodes ( ) { } @ Test public void testFailedSearchWithWrongQuery ( ) throws Exception { logger . info ( "Start<seq2seq4repair_space>Testing<seq2seq4repair_space>failed<seq2seq4repair_space>search<seq2seq4repair_space>with<seq2seq4repair_space>wrong<seq2seq4repair_space>query" ) ; startNode ( "server1" ) ; <START_BUG> client ( "server1" ) . admin ( ) . indices ( ) . create ( createIndexRequest ( "test" ) . settings ( settingsBuilder ( ) . put ( "number_of_shards" , 3 ) . put ( "number_of_replicas" , 2 ) . put ( "routing.hash.type" , "simple" ) ) ) . actionGet ( ) ; <END_BUG> client ( "server1" ) . admin ( ) . cluster ( ) . prepareHealth ( ) . setWaitForYellowStatus ( ) . execute ( ) . actionGet ( ) ; for ( int i = 0 ; i < 100 ; i ++ ) { index ( client ( "server1" ) , Integer . toString ( i ) , "test" , i ) ; } RefreshResponse refreshResponse = client ( "server1" ) . admin ( ) . indices ( ) . refresh ( refreshRequest ( "test" ) ) . actionGet ( ) ; assertThat ( refreshResponse . totalShards ( ) , equalTo ( 9 ) ) ; assertThat ( refreshResponse . successfulShards ( ) , equalTo ( 3 ) ) ; assertThat ( refreshResponse . failedShards ( ) , equalTo ( 0 ) ) ; for ( int i = 0 ; i < 5 ; i ++ ) { try { SearchResponse searchResponse = client ( "server1" ) . search ( searchRequest ( "test" ) . source ( Unicode . fromStringAsBytes ( "{<seq2seq4repair_space>xxx<seq2seq4repair_space>}" ) ) ) . actionGet ( ) ; assertThat ( searchResponse . totalShards ( ) , equalTo ( 3 ) ) ; assertThat ( searchResponse . successfulShards ( ) , equalTo ( 0 ) ) ; assertThat ( searchResponse . failedShards ( ) , equalTo ( 3 ) ) ; assert false : "search<seq2seq4repair_space>should<seq2seq4repair_space>fail" ; } catch ( ElasticSearchException e ) { assertThat ( e . unwrapCause ( ) , instanceOf ( SearchPhaseExecutionException . class ) ) ; } } startNode ( "server2" ) ; assertThat ( client ( "server1" ) . admin ( ) . cluster ( ) . prepareHealth ( ) . setWaitForNodes ( "2" ) . execute ( ) . actionGet ( ) . timedOut ( ) , equalTo ( false ) ) ; logger . info ( "Running<seq2seq4repair_space>Cluster<seq2seq4repair_space>Health" ) ; ClusterHealthResponse clusterHealth = client ( "server1" ) . admin ( ) . cluster ( ) . health ( clusterHealthRequest ( "test" ) . waitForYellowStatus ( ) . waitForRelocatingShards ( 0 ) . waitForActiveShards ( 6 ) ) . actionGet ( ) ; logger . info ( ( "Done<seq2seq4repair_space>Cluster<seq2seq4repair_space>Health,<seq2seq4repair_space>status<seq2seq4repair_space>" + ( clusterHealth . status ( ) ) ) ) ; assertThat ( clusterHealth . timedOut ( ) , equalTo ( false ) ) ; assertThat ( clusterHealth . status ( ) , equalTo ( YELLOW ) ) ; assertThat ( clusterHealth . activeShards ( ) , equalTo ( 6 ) ) ; refreshResponse = client ( "server1" ) . admin ( ) . indices ( ) . refresh ( refreshRequest ( "test" ) ) . actionGet ( ) ; assertThat ( refreshResponse . totalShards ( ) , equalTo ( 9 ) ) ; assertThat ( refreshResponse . successfulShards ( ) , equalTo ( 6 ) ) ; assertThat ( refreshResponse . failedShards ( ) , equalTo ( 0 ) ) ; for ( int i = 0 ; i < 5 ; i ++ ) { try { SearchResponse searchResponse = client ( "server1" ) . search ( searchRequest ( "test" ) . source ( Unicode . fromStringAsBytes ( "{<seq2seq4repair_space>xxx<seq2seq4repair_space>}" ) ) ) . actionGet ( ) ; assertThat ( searchResponse . totalShards ( ) , equalTo ( 3 ) ) ; assertThat ( searchResponse . successfulShards ( ) , equalTo ( 0 ) ) ; assertThat ( searchResponse . failedShards ( ) , equalTo ( 3 ) ) ; assert false : "search<seq2seq4repair_space>should<seq2seq4repair_space>fail" ; } catch ( ElasticSearchException e ) { assertThat ( e . unwrapCause ( ) , instanceOf ( SearchPhaseExecutionException . class ) ) ; } } logger . info ( "Done<seq2seq4repair_space>Testing<seq2seq4repair_space>failed<seq2seq4repair_space>search" ) ; } private void index ( Client client , String id , String nameValue , int age ) throws IOException { } private XContentBuilder source ( String id , String nameValue , int age ) throws IOException { } }
final class LwjglGL11 extends LwjglGL10 implements GL11 { private IntBuffer tempInt ; private FloatBuffer tempFloat ; public LwjglGL11 ( ) { } private IntBuffer toBuffer ( int n , int [ ] src , int offset ) { } private IntBuffer toBuffer ( int [ ] src , int offset ) { } private FloatBuffer toBuffer ( float [ ] src , int offset ) { } public void glBindBuffer ( int target , int buffer ) { } public void glBufferData ( int target , int size , Buffer data , int usage ) { } public void glBufferSubData ( int target , int offset , int size , Buffer data ) { } public void glClipPlanef ( int plane , float [ ] equation , int offset ) { } public void glClipPlanef ( int plane , FloatBuffer equation ) { } public void glColor4ub ( byte red , byte green , byte blue , byte alpha ) { } public void glDeleteBuffers ( int n , int [ ] buffers , int offset ) { } public void glDeleteBuffers ( int n , IntBuffer buffers ) { } public void glGenBuffers ( int n , int [ ] buffers , int offset ) { for ( int i = offset ; i < ( offset + n ) ; i ++ ) <START_BUG> buffers [ offset ] = GL15 . glGenBuffers ( ) ; <END_BUG> } public void glGenBuffers ( int n , IntBuffer buffers ) { } public void glGetBooleanv ( int pname , boolean [ ] params , int offset ) { } public void glGetBooleanv ( int pname , IntBuffer params ) { } public void glGetBufferParameteriv ( int target , int pname , int [ ] params , int offset ) { } public void glGetBufferParameteriv ( int target , int pname , IntBuffer params ) { } public void glGetClipPlanef ( int pname , float [ ] eqn , int offset ) { } public void glGetClipPlanef ( int pname , FloatBuffer eqn ) { } public void glGetFixedv ( int pname , int [ ] params , int offset ) { } public void glGetFixedv ( int pname , IntBuffer params ) { } public void glGetFloatv ( int pname , float [ ] params , int offset ) { } public void glGetFloatv ( int pname , FloatBuffer params ) { } public void glGetLightfv ( int light , int pname , float [ ] params , int offset ) { } public void glGetLightfv ( int light , int pname , FloatBuffer params ) { } public void glGetLightxv ( int light , int pname , int [ ] params , int offset ) { } public void glGetLightxv ( int light , int pname , IntBuffer params ) { } public void glGetMaterialfv ( int face , int pname , float [ ] params , int offset ) { } public void glGetMaterialfv ( int face , int pname , FloatBuffer params ) { } public void glGetMaterialxv ( int face , int pname , int [ ] params , int offset ) { } public void glGetMaterialxv ( int face , int pname , IntBuffer params ) { } public void glGetPointerv ( int pname , Buffer [ ] params ) { } public void glGetTexEnviv ( int env , int pname , int [ ] params , int offset ) { } public void glGetTexEnviv ( int env , int pname , IntBuffer params ) { } public void glGetTexEnvxv ( int env , int pname , int [ ] params , int offset ) { } public void glGetTexEnvxv ( int env , int pname , IntBuffer params ) { } public void glGetTexParameterfv ( int target , int pname , float [ ] params , int offset ) { } public void glGetTexParameterfv ( int target , int pname , FloatBuffer params ) { } public void glGetTexParameteriv ( int target , int pname , int [ ] params , int offset ) { } public void glGetTexParameteriv ( int target , int pname , IntBuffer params ) { } public void glGetTexParameterxv ( int target , int pname , int [ ] params , int offset ) { } public void glGetTexParameterxv ( int target , int pname , IntBuffer params ) { } public boolean glIsBuffer ( int buffer ) { } public boolean glIsEnabled ( int cap ) { } public boolean glIsTexture ( int texture ) { } public void glPointParameterf ( int pname , float param ) { } public void glPointParameterfv ( int pname , float [ ] params , int offset ) { } public void glPointParameterfv ( int pname , FloatBuffer params ) { } public void glPointSizePointerOES ( int type , int stride , Buffer pointer ) { } public void glTexEnvi ( int target , int pname , int param ) { } public void glTexEnviv ( int target , int pname , int [ ] params , int offset ) { } public void glTexEnviv ( int target , int pname , IntBuffer params ) { } public void glTexParameterfv ( int target , int pname , float [ ] params , int offset ) { } public void glTexParameterfv ( int target , int pname , FloatBuffer params ) { } public void glTexParameteri ( int target , int pname , int param ) { } public void glTexParameteriv ( int target , int pname , int [ ] params , int offset ) { } public void glTexParameteriv ( int target , int pname , IntBuffer params ) { } public void glColorPointer ( int size , int type , int stride , int pointer ) { } public void glNormalPointer ( int type , int stride , int pointer ) { } public void glTexCoordPointer ( int size , int type , int stride , int pointer ) { }
private final ShardIndexingService indexingService ; private final ShardSearchService searchService ; private final ShardGetService getService ; private final ShardIndexWarmerService shardWarmerService ; private final ShardFilterCache shardFilterCache ; private final ShardIdCache shardIdCache ; private final ShardFieldData shardFieldData ; private final PercolatorQueriesRegistry percolatorQueriesRegistry ; private final ShardPercolateService shardPercolateService ; private final CodecService codecService ; private final ShardTermVectorService termVectorService ; private final Object mutex = new Object ( ) ; private final String checkIndexOnStartup ; private long checkIndexTook = 0 ; private volatile IndexShardState state ; private TimeValue refreshInterval ; private final TimeValue mergeInterval ; private volatile ScheduledFuture refreshScheduledFuture ; private volatile ScheduledFuture mergeScheduleFuture ; private volatile ShardRouting shardRouting ; private RecoveryStatus peerRecoveryStatus ; private InternalIndexShard . ApplyRefreshSettings applyRefreshSettings = new InternalIndexShard . ApplyRefreshSettings ( ) ; private final MeanMetric refreshMetric = new MeanMetric ( ) ; private final MeanMetric flushMetric = new MeanMetric ( ) ; @ Inject public InternalIndexShard ( ShardId shardId , @ IndexSettings Settings indexSettings , IndexSettingsService indexSettingsService , IndicesLifecycle indicesLifecycle , Store store , Engine engine , MergeSchedulerProvider mergeScheduler , Translog translog , ThreadPool threadPool , MapperService mapperService , IndexQueryParserService queryParserService , IndexCache indexCache , IndexAliasesService indexAliasesService , ShardIndexingService indexingService , ShardGetService getService , ShardSearchService searchService , ShardIndexWarmerService shardWarmerService , ShardFilterCache shardFilterCache , ShardIdCache shardIdCache , ShardFieldData shardFieldData , PercolatorQueriesRegistry percolatorQueriesRegistry , ShardPercolateService shardPercolateService , CodecService codecService , ShardTermVectorService termVectorService ) { } public MergeSchedulerProvider mergeScheduler ( ) { } public Store store ( ) { } public Engine engine ( ) { } public Translog translog ( ) { } public ShardIndexingService indexingService ( ) { } @ Override public ShardGetService getService ( ) { } @ Override public ShardTermVectorService termVectorService ( ) { } @ Override public ShardSearchService searchService ( ) { } @ Override public ShardIndexWarmerService warmerService ( ) { } @ Override public ShardFilterCache filterCache ( ) { } @ Override public ShardIdCache idCache ( ) { } @ Override public ShardFieldData fieldData ( ) { } @ Override public ShardRouting routingEntry ( ) { } public InternalIndexShard routingEntry ( ShardRouting newRouting ) { } public IndexShardState recovering ( String reason ) throws IndexShardClosedException , IndexShardRecoveringException , IndexShardRelocatedException , IndexShardStartedException { } public InternalIndexShard relocated ( String reason ) throws IndexShardNotStartedException { } public InternalIndexShard start ( String reason ) throws IndexShardClosedException , IndexShardRelocatedException , IndexShardStartedException { } @ Override public IndexShardState state ( ) { } @ Override public Create prepareCreate ( SourceToParse source ) throws ElasticSearchException { } @ Override public ParsedDocument create ( Engine . Create create ) throws ElasticSearchException { } @ Override public Index prepareIndex ( SourceToParse source ) throws ElasticSearchException { } @ Override public ParsedDocument index ( Engine . Index index ) throws ElasticSearchException { } @ Override public Delete prepareDelete ( String type , String id , long version ) throws ElasticSearchException { } @ Override public void delete ( Engine . Delete delete ) throws ElasticSearchException { } @ Override public DeleteByQuery prepareDeleteByQuery ( BytesReference querySource , @ Nullable String [ ] filteringAliases , String ... types ) throws ElasticSearchException { } @ Override public void deleteByQuery ( Engine . DeleteByQuery deleteByQuery ) throws ElasticSearchException { } @ Override public GetResult get ( Engine . Get get ) throws ElasticSearchException { } @ Override public void refresh ( Engine . Refresh refresh ) throws ElasticSearchException { } @ Override public RefreshStats refreshStats ( ) { } @ Override public FlushStats flushStats ( ) { } @ Override public DocsStats docStats ( ) { } @ Override public IndexingStats indexingStats ( String ... types ) { } @ Override public SearchStats searchStats ( String ... groups ) { } @ Override public GetStats getStats ( ) { } @ Override public StoreStats storeStats ( ) { } @ Override public MergeStats mergeStats ( ) { } @ Override public WarmerStats warmerStats ( ) { } @ Override public FilterCacheStats filterCacheStats ( ) { } @ Override public FieldDataStats fieldDataStats ( String ... fields ) { } @ Override public PercolatorQueriesRegistry percolateRegistry ( ) { } @ Override public ShardPercolateService shardPercolateService ( ) { } @ Override public IdCacheStats idCacheStats ( ) { } @ Override public CompletionStats completionStats ( String ... fields ) { } @ Override public void flush ( Engine . Flush flush ) throws ElasticSearchException { } @ Override public void optimize ( Engine . Optimize optimize ) throws ElasticSearchException { } @ Override public < T > T snapshot ( Engine . SnapshotHandler < T > snapshotHandler ) throws EngineException { } @ Override public void recover ( Engine . RecoveryHandler recoveryHandler ) throws EngineException { } @ Override public Searcher acquireSearcher ( ) { readAllowed ( ) ; <START_BUG> return engine . searcher ( ) ; <END_BUG> } public void close ( String reason ) { } public long checkIndexTook ( ) { } public void performRecoveryPrepareForTranslog ( ) throws ElasticSearchException { } public RecoveryStatus peerRecoveryStatus ( ) { } public void performRecoveryFinalization ( boolean withFlush , RecoveryStatus peerRecoveryStatus ) throws ElasticSearchException { } public void performRecoveryFinalization ( boolean withFlush ) throws ElasticSearchException { } public void performRecoveryOperation ( Translog . Operation operation ) throws ElasticSearchException { } public boolean ignoreRecoveryAttempt ( ) { } public void readAllowed ( ) throws IllegalIndexShardStateException { } private void writeAllowed ( ) throws IllegalIndexShardStateException { } private void verifyStartedOrRecovering ( ) throws IllegalIndexShardStateException { } private void verifyNotClosed ( ) throws IllegalIndexShardStateException { } private void verifyStarted ( ) throws IllegalIndexShardStateException { } private void startScheduledTasksIfNeeded ( ) { } private Query filterQueryIfNeeded ( Query query , String [ ] types ) { } public static final String INDEX_REFRESH_INTERVAL = "index.refresh_interval" ; private class ApplyRefreshSettings implements IndexSettingsService . Listener { @ Override public void onRefreshSettings ( Settings settings ) { } } class EngineRefresher implements Runnable { @ Override public void run ( ) { } } class EngineMerger implements Runnable { @ Override public void run ( ) { } } private void checkIndex ( boolean throwException ) throws IndexShardException { } }
class AndroidLiveWallpaper implements Application { protected WallpaperService service ; private Engine engine ; protected AndroidGraphicsLiveWallpaper graphics ; protected AndroidInput input ; protected AndroidAudio audio ; protected AndroidFiles files ; protected AndroidNet net ; protected ApplicationListener listener ; protected boolean firstResume = true ; protected final Array < Runnable > runnables = new Array < Runnable > ( ) ; protected final Array < Runnable > executedRunnables = new Array < Runnable > ( ) ; protected int logLevel = LOG_INFO ; public AndroidLiveWallpaper ( WallpaperService service , Engine engine ) { } public void initialize ( ApplicationListener listener , AndroidApplicationConfiguration config ) { graphics = new AndroidGraphicsLiveWallpaper ( this , config . useGL20 , ( ( config . resolutionStrategy ) == null ? new FillResolutionStrategy ( ) : config . resolutionStrategy ) ) ; <START_BUG> input = new AndroidInput ( this , this . getService ( ) , null , config ) ; <END_BUG> audio = new AndroidAudio ( this . getService ( ) , config ) ; files = new AndroidFiles ( this . getService ( ) . getAssets ( ) ) ; this . listener = listener ; Gdx . app = this ; Gdx . input = this . getInput ( ) ; Gdx . audio = this . getAudio ( ) ; Gdx . files = this . getFiles ( ) ; Gdx . graphics = this . getGraphics ( ) ; } public void onPause ( ) { } public void onResume ( ) { } public void onDestroy ( ) { } public WallpaperService getService ( ) { } public Engine getEngine ( ) { } public ApplicationListener getListener ( ) { } @ Override public void postRunnable ( Runnable runnable ) { } @ Override public Audio getAudio ( ) { } @ Override public Files getFiles ( ) { } @ Override public Graphics getGraphics ( ) { } @ Override public Input getInput ( ) { } @ Override public Net getNet ( ) { } @ Override public ApplicationType getType ( ) { } @ Override public int getVersion ( ) { } @ Override public long getJavaHeap ( ) { } @ Override public long getNativeHeap ( ) { } @ Override public Preferences getPreferences ( String name ) { } AndroidClipboard clipboard ; @ Override public Clipboard getClipboard ( ) { } @ Override public void debug ( String tag , String message ) { } @ Override public void debug ( String tag , String message , Throwable exception ) { } @ Override public void log ( String tag , String message ) { } @ Override public void log ( String tag , String message , Exception exception ) { } @ Override public void error ( String tag , String message ) { } @ Override public void error ( String tag , String message , Throwable exception ) { } @ Override public void setLogLevel ( int logLevel ) { } @ Override public void exit ( ) { } }
public class FetchPhase implements SearchPhase { @ Override public Map < String , ? extends SearchParseElement > parseElements ( ) { } @ Override public void preProcess ( SearchContext context ) { } public void execute ( SearchContext context ) { } private void doExplanation ( SearchContext context , int docId , InternalSearchHit searchHit ) { } private byte [ ] extractSource ( Document doc , DocumentMapper documentMapper ) { } private Uid extractUid ( SearchContext context , Document doc ) { } private Document loadDocument ( SearchContext context , FieldSelector fieldSelector , int docId ) { } private FieldSelector buildFieldSelectors ( SearchContext context ) { <START_BUG> if ( ( ( context . fieldNames ( ) ) == null ) || ( ( context . fieldNames ( ) . length ) == 0 ) ) { <END_BUG> return new UidAndSourceFieldSelector ( ) ; } FieldMappersFieldSelector fieldSelector = new FieldMappersFieldSelector ( ) ; for ( String fieldName : context . fieldNames ( ) ) { FieldMappers x = context . mapperService ( ) . smartNameFieldMappers ( fieldName ) ; if ( x == null ) { throw new FetchPhaseExecutionException ( context , ( ( "No<seq2seq4repair_space>mapping<seq2seq4repair_space>for<seq2seq4repair_space>field<seq2seq4repair_space>[" + fieldName ) + "]" ) ) ; } fieldSelector . add ( x ) ; } fieldSelector . add ( context . mapperService ( ) . uidFieldMappers ( ) ) ; return fieldSelector ; } }
public class StringBuilder implements Appendable , CharSequence { static final int INITIAL_CAPACITY = 16 ; public char [ ] chars ; public int length ; private static final char [ ] digits = new char [ ] { '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' } ; final char [ ] getValue ( ) { } public StringBuilder ( ) { } public StringBuilder ( int capacity ) { } public StringBuilder ( CharSequence seq ) { } public StringBuilder ( StringBuilder builder ) { } public StringBuilder ( String string ) { } private void enlargeBuffer ( int min ) { } final void appendNull ( ) { } final void append0 ( char [ ] value ) { int newSize = ( length ) + ( value . length ) ; if ( newSize > ( chars . length ) ) { enlargeBuffer ( newSize ) ; } <START_BUG> System . arraycopy ( value , 0 , value , length , value . length ) ; <END_BUG> length = newSize ; } final void append0 ( char [ ] value , int offset , int length ) { } final void append0 ( char ch ) { } final void append0 ( String string ) { } final void append0 ( CharSequence s , int start , int end ) { } public int capacity ( ) { } public char charAt ( int index ) { } final void delete0 ( int start , int end ) { } final void deleteCharAt0 ( int location ) { } public void ensureCapacity ( int min ) { } public void getChars ( int start , int end , char [ ] dest , int destStart ) { } final void insert0 ( int index , char [ ] value ) { } final void insert0 ( int index , char [ ] value , int start , int length ) { } final void insert0 ( int index , char ch ) { } final void insert0 ( int index , String string ) { } final void insert0 ( int index , CharSequence s , int start , int end ) { } public int length ( ) { } private void move ( int size , int index ) { } final void replace0 ( int start , int end , String string ) { } final void reverse0 ( ) { } public void setCharAt ( int index , char ch ) { } public void setLength ( int newLength ) { } public String substring ( int start ) { } public String substring ( int start , int end ) { } @ Override public String toString ( ) { } public CharSequence subSequence ( int start , int end ) { } public int indexOf ( String string ) { } public int indexOf ( String subString , int start ) { } public int lastIndexOf ( String string ) { } public int lastIndexOf ( String subString , int start ) { } public void trimToSize ( ) { } public int codePointAt ( int index ) { } public int codePointBefore ( int index ) { } public int codePointCount ( int beginIndex , int endIndex ) { } public int offsetByCodePoints ( int index , int codePointOffset ) { } public StringBuilder append ( boolean b ) { } public StringBuilder append ( char c ) { } public StringBuilder append ( int i ) { } public StringBuilder append ( long lng ) { } public StringBuilder append ( float f ) { } public StringBuilder append ( double d ) { } public StringBuilder append ( Object obj ) { } public StringBuilder append ( String str ) { } public StringBuilder append ( char [ ] ch ) { } public StringBuilder append ( char [ ] str , int offset , int len ) { } public StringBuilder append ( CharSequence csq ) { } public StringBuilder append ( StringBuilder builder ) { } public StringBuilder append ( CharSequence csq , int start , int end ) { } public StringBuilder append ( StringBuilder builder , int start , int end ) { } public StringBuilder appendCodePoint ( int codePoint ) { } public StringBuilder delete ( int start , int end ) { } public StringBuilder deleteCharAt ( int index ) { } public StringBuilder insert ( int offset , boolean b ) { } public StringBuilder insert ( int offset , char c ) { } public StringBuilder insert ( int offset , int i ) { } public StringBuilder insert ( int offset , long l ) { } public StringBuilder insert ( int offset , float f ) { } public StringBuilder insert ( int offset , double d ) { } public StringBuilder insert ( int offset , Object obj ) { } public StringBuilder insert ( int offset , String str ) { } public StringBuilder insert ( int offset , char [ ] ch ) { } public StringBuilder insert ( int offset , char [ ] str , int strOffset , int strLen ) { } public StringBuilder insert ( int offset , CharSequence s ) { } public StringBuilder insert ( int offset , CharSequence s , int start , int end ) { } public StringBuilder replace ( int start , int end , String str ) { } public StringBuilder reverse ( ) { } public int hashCode ( ) { } public boolean equals ( Object obj ) { } }
public class TranslogStreams { public static Operation readTranslogOperation ( StreamInput in ) throws IOException { } public static Source readSource ( byte [ ] data ) throws IOException { <START_BUG> BytesStreamInput in = new BytesStreamInput ( data ) ; <END_BUG> in . readInt ( ) ; Translog . Operation . Type type = Type . fromId ( in . readByte ( ) ) ; Translog . Operation operation ; switch ( type ) { case CREATE : operation = new Translog . Create ( ) ; break ; case DELETE : operation = new Translog . Delete ( ) ; break ; case DELETE_BY_QUERY : operation = new Translog . DeleteByQuery ( ) ; break ; case SAVE : operation = new Translog . Index ( ) ; break ; default : throw new IOException ( ( ( "No<seq2seq4repair_space>type<seq2seq4repair_space>for<seq2seq4repair_space>[" + type ) + "]" ) ) ; } return operation . readSource ( in ) ; } public static void writeTranslogOperation ( StreamOutput out , Translog . Operation op ) throws IOException { } }
public enum MemorySizeValue { ; public static ByteSizeValue parseBytesSizeValueOrHeapRatio ( String sValue ) { <START_BUG> if ( sValue . endsWith ( "%" ) ) { <END_BUG> final String percentAsString = sValue . substring ( 0 , ( ( sValue . length ( ) ) - 1 ) ) ; try { final double percent = Double . parseDouble ( percentAsString ) ; if ( ( percent < 0 ) || ( percent > 100 ) ) { throw new ElasticsearchParseException ( ( "Percentage<seq2seq4repair_space>should<seq2seq4repair_space>be<seq2seq4repair_space>in<seq2seq4repair_space>[0-100],<seq2seq4repair_space>got<seq2seq4repair_space>" + percentAsString ) ) ; } return new ByteSizeValue ( ( ( long ) ( ( percent / 100 ) * ( JvmInfo . jvmInfo ( ) . getMem ( ) . getHeapMax ( ) . bytes ( ) ) ) ) , ByteSizeUnit . BYTES ) ; } catch ( NumberFormatException e ) { throw new ElasticsearchParseException ( ( ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>parse<seq2seq4repair_space>[" + percentAsString ) + "]<seq2seq4repair_space>as<seq2seq4repair_space>a<seq2seq4repair_space>double" ) , e ) ; } } else { return ByteSizeValue . parseBytesSizeValue ( sValue ) ; } } }
updateRequest . parent ( request . param ( "parent" ) ) ; updateRequest . timeout ( request . paramAsTime ( "timeout" , updateRequest . timeout ( ) ) ) ; updateRequest . refresh ( request . paramAsBoolean ( "refresh" , updateRequest . refresh ( ) ) ) ; String replicationType = request . param ( "replication" ) ; if ( replicationType != null ) { updateRequest . replicationType ( ReplicationType . fromString ( replicationType ) ) ; } String consistencyLevel = request . param ( "consistency" ) ; if ( consistencyLevel != null ) { updateRequest . consistencyLevel ( WriteConsistencyLevel . fromString ( consistencyLevel ) ) ; } updateRequest . percolate ( request . param ( "percolate" , null ) ) ; updateRequest . script ( request . param ( "script" ) ) ; updateRequest . scriptLang ( request . param ( "lang" ) ) ; for ( Map . Entry < String , String > entry : request . params ( ) . entrySet ( ) ) { if ( entry . getKey ( ) . startsWith ( "sp_" ) ) { updateRequest . addScriptParam ( entry . getKey ( ) . substring ( 3 ) , entry . getValue ( ) ) ; } } String sField = request . param ( "fields" ) ; if ( sField != null ) { String [ ] sFields = Strings . splitStringByCommaToArray ( sField ) ; if ( sFields != null ) { updateRequest . fields ( sFields ) ; } } updateRequest . retryOnConflict ( request . paramAsInt ( "retry_on_conflict" , updateRequest . retryOnConflict ( ) ) ) ; if ( request . hasContent ( ) ) { try { updateRequest . source ( request . content ( ) ) ; IndexRequest upsertRequest = updateRequest . upsertRequest ( ) ; if ( upsertRequest != null ) { upsertRequest . routing ( request . param ( "routing" ) ) ; upsertRequest . parent ( request . param ( "parent" ) ) ; upsertRequest . timestamp ( request . param ( "timestamp" ) ) ; if ( request . hasParam ( "ttl" ) ) { upsertRequest . ttl ( request . paramAsTime ( "ttl" , null ) . millis ( ) ) ; } upsertRequest . version ( RestActions . parseVersion ( request ) ) ; upsertRequest . versionType ( VersionType . fromString ( request . param ( "version_type" ) , upsertRequest . versionType ( ) ) ) ; } IndexRequest doc = updateRequest . doc ( ) ; if ( doc != null ) { doc . routing ( request . param ( "routing" ) ) ; doc . parent ( request . param ( "parent" ) ) ; doc . timestamp ( request . param ( "timestamp" ) ) ; if ( request . hasParam ( "ttl" ) ) { doc . ttl ( request . paramAsTime ( "ttl" , null ) . millis ( ) ) ; } doc . version ( RestActions . parseVersion ( request ) ) ; doc . versionType ( VersionType . fromString ( request . param ( "version_type" ) , doc . versionType ( ) ) ) ; } } catch ( Exception e ) { try { channel . sendResponse ( new XContentThrowableRestResponse ( request , e ) ) ; } catch ( IOException e1 ) { logger . warn ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>response" , e1 ) ; } return ; } } client . update ( updateRequest , new org . elasticsearch . action . ActionListener < UpdateResponse > ( ) { @ Override public void onResponse ( UpdateResponse response ) { try { XContentBuilder builder = RestXContentBuilder . restContentBuilder ( request ) ; builder . startObject ( ) . field ( RestUpdateAction . Fields . OK , true ) . field ( RestUpdateAction . Fields . _INDEX , response . getIndex ( ) ) . field ( RestUpdateAction . Fields . _TYPE , response . getType ( ) ) . field ( RestUpdateAction . Fields . _ID , response . getId ( ) ) . field ( RestUpdateAction . Fields . _VERSION , response . getVersion ( ) ) ; if ( ( response . getGetResult ( ) ) != null ) { builder . startObject ( RestUpdateAction . Fields . GET ) ; response . getGetResult ( ) . toXContentEmbedded ( builder , request ) ; builder . endObject ( ) ; } if ( ( response . getMatches ( ) ) != null ) { builder . startArray ( RestUpdateAction . Fields . MATCHES ) ; for ( String match : response . getMatches ( ) ) { builder . value ( match ) ; } builder . endArray ( ) ; } builder . endObject ( ) ; RestStatus status = org . elasticsearch . rest . RestStatus . OK ; <START_BUG> if ( ( response . getVersion ( ) ) == 1 ) { <END_BUG> status = org . elasticsearch . rest . RestStatus . CREATED ; } channel . sendResponse ( new XContentRestResponse ( request , status , builder ) ) ; } catch ( Throwable e ) { onFailure ( e ) ; } } @ Override public void onFailure ( Throwable e ) { try { channel . sendResponse ( new XContentThrowableRestResponse ( request , e ) ) ; } catch ( IOException e1 ) { logger . error ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>failure<seq2seq4repair_space>response" , e1 ) ; } } } ) ; } static final class Fields { static final XContentBuilderString OK = new XContentBuilderString ( "ok" ) ; static final XContentBuilderString _INDEX = new XContentBuilderString ( "_index" ) ; static final XContentBuilderString _TYPE = new XContentBuilderString ( "_type" ) ; static final XContentBuilderString _ID = new XContentBuilderString ( "_id" ) ; static final XContentBuilderString _VERSION = new XContentBuilderString ( "_version" ) ; static final XContentBuilderString MATCHES = new XContentBuilderString ( "matches" ) ; static final XContentBuilderString GET = new XContentBuilderString ( "get" ) ; } }
public class AtomicQueue < T > { private final AtomicInteger writeIndex = new AtomicInteger ( ) ; private final AtomicInteger readIndex = new AtomicInteger ( ) ; private final AtomicReferenceArray < T > queue ; public AtomicQueue ( int capacity ) { } private int next ( int idx ) { <START_BUG> return ( idx + 1 ) & ( ( queue . length ( ) ) - 1 ) ; <END_BUG> } public boolean put ( T value ) { } public T poll ( ) { } }
@ Test public class SimpleAttachmentIntegrationTests { private final ESLogger logger = Loggers . getLogger ( getClass ( ) ) ; private Node node ; @ BeforeClass public void setupServer ( ) { <START_BUG> node = nodeBuilder ( ) . local ( true ) . node ( ) ; <END_BUG> } @ AfterClass public void closeServer ( ) { } @ BeforeMethod public void createIndex ( ) { } @ AfterMethod public void deleteIndex ( ) { } @ Test public void testSimpleAttachment ( ) throws Exception { } }
public class JoglDebugStarter { public static void main ( String [ ] argv ) { <START_BUG> new com . badlogic . gdx . backends . jogl . JoglApplication ( new com . badlogic . gdx . tests . MatrixJNITest ( ) , "Debug<seq2seq4repair_space>Test" , 480 , 320 , false ) ; <END_BUG> } }
for ( int j = 0 ; j < 3 ; j ++ ) { MD2Loader . VertexIndices vert = null ; boolean contains = false ; for ( int k = 0 ; k < ( vertCombos . size ( ) ) ; k ++ ) { MD2Loader . VertexIndices vIdx = vertCombos . get ( k ) ; if ( ( ( vIdx . vIdx ) == ( triangle . vertices [ j ] ) ) && ( ( vIdx . tIdx ) == ( triangle . texCoords [ j ] ) ) ) { vert = vIdx ; contains = true ; break ; } } if ( ! contains ) { vert = new MD2Loader . VertexIndices ( triangle . vertices [ j ] , triangle . texCoords [ j ] , vertIdx ) ; vertCombos . add ( vert ) ; vertIdx ++ ; } indices [ ( idx ++ ) ] = vert . nIdx ; } } idx = 0 ; float [ ] uvs = new float [ ( vertCombos . size ( ) ) * 2 ] ; for ( int i = 0 ; i < ( vertCombos . size ( ) ) ; i ++ ) { MD2Loader . VertexIndices vtI = vertCombos . get ( i ) ; uvs [ ( idx ++ ) ] = texCoords [ ( ( vtI . tIdx ) * 2 ) ] ; uvs [ ( idx ++ ) ] = texCoords [ ( ( ( vtI . tIdx ) * 2 ) + 1 ) ] ; } for ( int i = 0 ; i < ( frames . length ) ; i ++ ) { MD2Frame frame = frames [ i ] ; idx = 0 ; float [ ] newVerts = new float [ ( vertCombos . size ( ) ) * 3 ] ; for ( int j = 0 ; j < ( vertCombos . size ( ) ) ; j ++ ) { MD2Loader . VertexIndices vIdx = vertCombos . get ( j ) ; newVerts [ ( idx ++ ) ] = frame . vertices [ ( ( vIdx . vIdx ) * 3 ) ] ; newVerts [ ( idx ++ ) ] = frame . vertices [ ( ( ( vIdx . vIdx ) * 3 ) + 1 ) ] ; newVerts [ ( idx ++ ) ] = frame . vertices [ ( ( ( vIdx . vIdx ) * 3 ) + 2 ) ] ; } frame . vertices = newVerts ; } header . numVertices = vertCombos . size ( ) ; KeyframedAnimation animation = new KeyframedAnimation ( "all" , ( ( frames . length ) * 0.2F ) , new Keyframe [ frames . length ] ) ; for ( int frameNum = 0 ; frameNum < ( frames . length ) ; frameNum ++ ) { MD2Frame frame = frames [ frameNum ] ; float [ ] vertices = new float [ ( header . numVertices ) * 5 ] ; idx = 0 ; int idxV = 0 ; int idxT = 0 ; for ( int i = 0 ; i < ( header . numVertices ) ; i ++ ) { vertices [ ( idx ++ ) ] = frame . vertices [ ( idxV ++ ) ] ; vertices [ ( idx ++ ) ] = frame . vertices [ ( idxV ++ ) ] ; vertices [ ( idx ++ ) ] = frame . vertices [ ( idxV ++ ) ] ; vertices [ ( idx ++ ) ] = uvs [ ( idxT ++ ) ] ; vertices [ ( idx ++ ) ] = uvs [ ( idxT ++ ) ] ; } Keyframe keyFrame = new Keyframe ( 0 , 3 , vertices ) ; animation . keyframes [ frameNum ] = keyFrame ; } Mesh mesh = new Mesh ( false , ( ( header . numTriangles ) * 3 ) , indices . length , new com . badlogic . gdx . graphics . VertexAttribute ( Usage . Position , 3 , "a_pos" ) , new com . badlogic . gdx . graphics . VertexAttribute ( Usage . TextureCoordinates , 2 , "a_tex0" ) ) ; mesh . setIndices ( indices ) ; ObjectMap < String , KeyframedAnimation > animations = new ObjectMap < String , KeyframedAnimation > ( ) ; animations . put ( "all" , animation ) ; KeyframedSubMesh subMesh = new KeyframedSubMesh ( "md2-mesh" , mesh , null , animations , GL10 . GL_TRIANGLES ) ; KeyframedModel model = new KeyframedModel ( new KeyframedSubMesh [ ] { subMesh } ) ; <START_BUG> model . setAnimation ( "all" , 0 ) ; <END_BUG> return model ; } private float [ ] buildTexCoords ( MD2Header header , MD2Triangle [ ] triangles , float [ ] texCoords ) { } private short [ ] buildIndices ( MD2Triangle [ ] triangles ) { } private MD2Frame [ ] loadFrames ( MD2Header header , byte [ ] bytes ) throws IOException { } private final byte [ ] charBuffer = new byte [ 16 ] ; private MD2Frame loadFrame ( MD2Header header , LittleEndianInputStream in ) throws IOException { } private MD2Triangle [ ] loadTriangles ( MD2Header header , byte [ ] bytes ) throws IOException { } private float [ ] loadTexCoords ( MD2Header header , byte [ ] bytes ) throws IOException { } private MD2Header loadHeader ( byte [ ] bytes ) throws IOException { } private byte [ ] loadBytes ( InputStream in ) throws IOException { } public class VertexIndices { public VertexIndices ( short vIdx , short tIdx , short nIdx ) { } @ Override public int hashCode ( ) { } @ Override public boolean equals ( Object obj ) { } public short vIdx ; public short tIdx ; public short nIdx ; } }
public class TouchpadTest extends GdxTest { Stage stage ; Touchpad touchpad ; public void create ( ) { } public void render ( ) { } public void resize ( int width , int height ) { <START_BUG> stage . getViewport ( ) . update ( width , height , true ) ; <END_BUG> } public void dispose ( ) { } }
public class Cube { static final int FOLLOW = 0 ; static final int FIXED = 1 ; static final int CONTROLLED = 2 ; static final int DEAD = 3 ; static final float ACCELERATION = 20 ; static final float MAX_VELOCITY = 4 ; static final float DAMP = 0.8F ; Map map ; Vector2 pos = new Vector2 ( ) ; Vector2 accel = new Vector2 ( ) ; Vector2 vel = new Vector2 ( ) ; Rectangle bounds = new Rectangle ( ) ; int state = Cube . FOLLOW ; float stateTime = 0 ; Rectangle controllButtonRect = new Rectangle ( ( 480 - 64 ) , ( 320 - 64 ) , 64 , 64 ) ; Rectangle followButtonRect = new Rectangle ( ( 480 - 64 ) , ( 320 - 138 ) , 64 , 64 ) ; Rectangle dpadRect = new Rectangle ( 0 , 0 , 128 , 128 ) ; public Cube ( Map map , float x , float y ) { } Vector2 target = new Vector2 ( ) ; public void update ( float deltaTime ) { } private void processKeys ( ) { float x0 = ( ( input . getX ( 0 ) ) / ( ( float ) ( graphics . getWidth ( ) ) ) ) * 480 ; float x1 = ( ( input . getX ( 1 ) ) / ( ( float ) ( graphics . getWidth ( ) ) ) ) * 480 ; float y0 = 320 - ( ( ( input . getY ( 0 ) ) / ( ( float ) ( graphics . getHeight ( ) ) ) ) * 320 ) ; float y1 = 320 - ( ( ( input . getY ( 1 ) ) / ( ( float ) ( graphics . getHeight ( ) ) ) ) * 320 ) ; boolean controlButton = ( ( input . isTouched ( 0 ) ) && ( controllButtonRect . contains ( x0 , y0 ) ) ) || ( ( input . isTouched ( 1 ) ) && ( controllButtonRect . contains ( x1 , y1 ) ) ) ; boolean followButton = ( ( input . isTouched ( 0 ) ) && ( followButtonRect . contains ( x0 , y0 ) ) ) || ( ( input . isTouched ( 1 ) ) && ( followButtonRect . contains ( x1 , y1 ) ) ) ; if ( ( ( ( input . isKeyPressed ( SPACE ) ) || controlButton ) && ( ( state ) == ( Cube . FOLLOW ) ) ) && ( ( stateTime ) > 0.5F ) ) { stateTime = 0 ; state = Cube . CONTROLLED ; return ; } if ( ( ( ( input . isKeyPressed ( SPACE ) ) || controlButton ) && ( ( state ) == ( Cube . CONTROLLED ) ) ) && ( ( stateTime ) > 0.5F ) ) { stateTime = 0 ; state = Cube . FIXED ; return ; } if ( ( ( ( input . isKeyPressed ( SPACE ) ) || controlButton ) && ( ( state ) == ( Cube . FIXED ) ) ) && ( ( stateTime ) > 0.5F ) ) { stateTime = 0 ; state = Cube . CONTROLLED ; return ; } <START_BUG> if ( ( ( input . isKeyPressed ( SPACE ) ) || followButton ) && ( ( stateTime ) > 0.5F ) ) { <END_BUG> stateTime = 0 ; state = Cube . FOLLOW ; return ; } boolean touch0 = input . isTouched ( 0 ) ; boolean touch1 = input . isTouched ( 1 ) ; boolean left = ( touch0 && ( x0 < 60 ) ) || ( touch1 && ( x1 < 60 ) ) ; boolean right = ( touch0 && ( ( x0 > 80 ) && ( x0 < 128 ) ) ) || ( touch1 && ( ( x1 > 80 ) && ( x1 < 128 ) ) ) ; boolean down = ( touch0 && ( y0 < 60 ) ) || ( touch1 && ( y1 < 60 ) ) ; boolean up = ( touch0 && ( ( y0 > 80 ) && ( x0 < 128 ) ) ) || ( touch1 && ( ( y1 > 80 ) && ( y1 < 128 ) ) ) ; if ( ( state ) == ( Cube . CONTROLLED ) ) { if ( input . isKeyPressed ( A ) ) { accel . x = - ( Cube . ACCELERATION ) ; } else if ( ( input . isKeyPressed ( D ) ) || right ) { accel . x = Cube . ACCELERATION ; } else { accel . x = 0 ; } if ( ( input . isKeyPressed ( W ) ) || up ) { accel . y = Cube . ACCELERATION ; } else if ( ( input . isKeyPressed ( S ) ) || down ) { accel . y = - ( Cube . ACCELERATION ) ; } else { accel . y = 0 ; } if ( touch0 ) { if ( dpadRect . contains ( x0 , y0 ) ) { float x = ( x0 - 64 ) / 64 ; float y = ( y0 - 64 ) / 64 ; float len = ( ( float ) ( Math . sqrt ( ( ( x * x ) + ( y * y ) ) ) ) ) ; if ( len != 0 ) { x /= len ; y /= len ; } else { x = 0 ; y = 0 ; } vel . x = x * ( Cube . MAX_VELOCITY ) ; vel . y = y * ( Cube . MAX_VELOCITY ) ; }
public class DateFieldMapper extends NumberFieldMapper < Long > { public static final String CONTENT_TYPE = "date" ; public static class Defaults extends NumberFieldMapper . Defaults { public static final FormatDateTimeFormatter DATE_TIME_FORMATTER = Joda . forPattern ( "dateOptionalTime" ) ; public static final FieldType FIELD_TYPE = new FieldType ( NumberFieldMapper . Defaults . FIELD_TYPE ) ; public static final String NULL_VALUE = null ; public static final TimeUnit TIME_UNIT = TimeUnit . MILLISECONDS ; public static final boolean PARSE_UPPER_INCLUSIVE = true ; } public static class Builder extends NumberFieldMapper . Builder < DateFieldMapper . Builder , DateFieldMapper > { protected TimeUnit timeUnit = DateFieldMapper . Defaults . TIME_UNIT ; protected String nullValue = DateFieldMapper . Defaults . NULL_VALUE ; protected FormatDateTimeFormatter dateTimeFormatter = DateFieldMapper . Defaults . DATE_TIME_FORMATTER ; private Locale locale ; public Builder ( String name ) { } public DateFieldMapper . Builder timeUnit ( TimeUnit timeUnit ) { } public DateFieldMapper . Builder nullValue ( String nullValue ) { } public DateFieldMapper . Builder dateTimeFormatter ( FormatDateTimeFormatter dateTimeFormatter ) { } @ Override public DateFieldMapper build ( BuilderContext context ) { } public DateFieldMapper . Builder locale ( Locale locale ) { } } public static class TypeParser implements Mapper . TypeParser { @ Override public Mapper . Builder < ? , ? > parse ( String name , Map < String , Object > node , ParserContext parserContext ) throws MapperParsingException { } } public static Locale parseLocale ( String locale ) { } protected final FormatDateTimeFormatter dateTimeFormatter ; private final boolean parseUpperInclusive ; private final DateMathParser dateMathParser ; private String nullValue ; protected final TimeUnit timeUnit ; protected DateFieldMapper ( Names names , FormatDateTimeFormatter dateTimeFormatter , int precisionStep , float boost , FieldType fieldType , String nullValue , TimeUnit timeUnit , boolean parseUpperInclusive , Explicit < Boolean > ignoreMalformed , PostingsFormatProvider provider , SimilarityProvider similarity , @ Nullable Settings fieldDataSettings ) { } @ Override public FieldType defaultFieldType ( ) { } @ Override public FieldDataType defaultFieldDataType ( ) { } @ Override protected int maxPrecisionStep ( ) { } @ Override public Long value ( Object value ) { } @ Override public Object valueForSearch ( Object value ) { } @ Override public BytesRef indexedValueForSearch ( Object value ) { } private long parseValue ( Object value ) { } private String convertToString ( Object value ) { } @ Override public Query fuzzyQuery ( String value , String minSim , int prefixLength , int maxExpansions , boolean transpositions ) { } @ Override public Query termQuery ( Object value , @ Nullable QueryParseContext context ) { } @ Override public Filter termFilter ( Object value , @ Nullable QueryParseContext context ) { } @ Override public Query rangeQuery ( Object lowerTerm , Object upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter rangeFilter ( Object lowerTerm , Object upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter rangeFilter ( IndexFieldDataService fieldData , Object lowerTerm , Object upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter nullValueFilter ( ) { } @ Override protected boolean customBoost ( ) { } @ Override protected Field innerParseCreateField ( ParseContext context ) throws IOException { } @ Override protected String contentType ( ) { } @ Override public void merge ( Mapper mergeWith , MergeContext mergeContext ) throws MergeMappingException { } @ Override protected void doXContentBody ( XContentBuilder builder ) throws IOException { super . doXContentBody ( builder ) ; if ( ( precisionStep ) != ( PRECISION_STEP ) ) { builder . field ( "precision_step" , precisionStep ) ; } builder . field ( "format" , dateTimeFormatter . format ( ) ) ; if ( ( nullValue ) != null ) { builder . field ( "null_value" , nullValue ) ; } if ( ( includeInAll ) != null ) { builder . field ( "include_in_all" , includeInAll ) ; } if ( ( timeUnit ) != ( DateFieldMapper . Defaults . TIME_UNIT ) ) { builder . field ( "numeric_resolution" , timeUnit . name ( ) . toLowerCase ( Locale . ROOT ) ) ; } if ( ( dateTimeFormatter . locale ( ) ) != null ) { <START_BUG> builder . field ( "locale" , dateTimeFormatter . format ( ) ) ; <END_BUG> } } private long parseStringValue ( String value ) { } }
public class IndicesStore extends AbstractComponent implements ClusterStateListener { public static final String INDICES_STORE_THROTTLE_TYPE = "indices.store.throttle.type" ; public static final String INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC = "indices.store.throttle.max_bytes_per_sec" ; public static final String ACTION_SHARD_EXISTS = "internal:index/shard/exists" ; private static final EnumSet < IndexShardState > ACTIVE_STATES = EnumSet . of ( STARTED , RELOCATED ) ; class ApplySettings implements NodeSettingsService . Listener { @ Override public void onRefreshSettings ( Settings settings ) { } } private final NodeEnvironment nodeEnv ; private final NodeSettingsService nodeSettingsService ; private final IndicesService indicesService ; private final ClusterService clusterService ; private final TransportService transportService ; private volatile String rateLimitingType ; private volatile ByteSizeValue rateLimitingThrottle ; private final StoreRateLimiting rateLimiting = new StoreRateLimiting ( ) ; private final IndicesStore . ApplySettings applySettings = new IndicesStore . ApplySettings ( ) ; @ Inject public IndicesStore ( Settings settings , NodeEnvironment nodeEnv , NodeSettingsService nodeSettingsService , IndicesService indicesService , ClusterService clusterService , TransportService transportService ) { } IndicesStore ( ) { } public StoreRateLimiting rateLimiting ( ) { } public void close ( ) { } @ Override public void clusterChanged ( ClusterChangedEvent event ) { } boolean shardCanBeDeleted ( ClusterState state , IndexShardRoutingTable indexShardRoutingTable ) { } private void deleteShardIfExistElseWhere ( ClusterState state , IndexShardRoutingTable indexShardRoutingTable ) { } private class ShardActiveResponseHandler implements TransportResponseHandler < IndicesStore . ShardActiveResponse > { private final ShardId shardId ; private final int expectedActiveCopies ; private final ClusterState clusterState ; private final AtomicInteger awaitingResponses ; private final AtomicInteger activeCopies ; public ShardActiveResponseHandler ( ShardId shardId , ClusterState clusterState , int expectedActiveCopies ) { } @ Override public IndicesStore . ShardActiveResponse newInstance ( ) { } @ Override public void handleResponse ( IndicesStore . ShardActiveResponse response ) { } @ Override public void handleException ( TransportException exp ) { } @ Override public String executor ( ) { } private void allNodesResponded ( ) { if ( ( activeCopies . get ( ) ) != ( expectedActiveCopies ) ) { logger . trace ( "not<seq2seq4repair_space>deleting<seq2seq4repair_space>shard<seq2seq4repair_space>{},<seq2seq4repair_space>expected<seq2seq4repair_space>{}<seq2seq4repair_space>active<seq2seq4repair_space>copies,<seq2seq4repair_space>but<seq2seq4repair_space>only<seq2seq4repair_space>{}<seq2seq4repair_space>found<seq2seq4repair_space>active<seq2seq4repair_space>copies" , shardId , expectedActiveCopies , activeCopies . get ( ) ) ; return ; } ClusterState latestClusterState = clusterService . state ( ) ; if ( ( clusterState . getVersion ( ) ) != ( latestClusterState . getVersion ( ) ) ) { logger . trace ( "not<seq2seq4repair_space>deleting<seq2seq4repair_space>shard<seq2seq4repair_space>{},<seq2seq4repair_space>the<seq2seq4repair_space>latest<seq2seq4repair_space>cluster<seq2seq4repair_space>state<seq2seq4repair_space>version[{}]<seq2seq4repair_space>is<seq2seq4repair_space>not<seq2seq4repair_space>equal<seq2seq4repair_space>to<seq2seq4repair_space>cluster<seq2seq4repair_space>state<seq2seq4repair_space>before<seq2seq4repair_space>shard<seq2seq4repair_space>active<seq2seq4repair_space>api<seq2seq4repair_space>call<seq2seq4repair_space>[{}]" , shardId , latestClusterState . getVersion ( ) , clusterState . getVersion ( ) ) ; return ; } <START_BUG> clusterService . submitStateUpdateTask ( "indices_store" , new ClusterStateUpdateTask ( ) { <END_BUG> @ Override public ClusterState execute ( ClusterState currentState ) throws Exception { if ( ( clusterState . getVersion ( ) ) != ( currentState . getVersion ( ) ) ) { logger . trace ( "not<seq2seq4repair_space>deleting<seq2seq4repair_space>shard<seq2seq4repair_space>{},<seq2seq4repair_space>the<seq2seq4repair_space>update<seq2seq4repair_space>task<seq2seq4repair_space>state<seq2seq4repair_space>version[{}]<seq2seq4repair_space>is<seq2seq4repair_space>not<seq2seq4repair_space>equal<seq2seq4repair_space>to<seq2seq4repair_space>cluster<seq2seq4repair_space>state<seq2seq4repair_space>before<seq2seq4repair_space>shard<seq2seq4repair_space>active<seq2seq4repair_space>api<seq2seq4repair_space>call<seq2seq4repair_space>[{}]" , shardId , currentState . getVersion ( ) , clusterState . getVersion ( ) ) ; return currentState ; } IndexService indexService = indicesService . indexService ( shardId . getIndex ( ) ) ; if ( indexService == null ) { if ( nodeEnv . hasNodeFile ( ) ) { File [ ] shardLocations = nodeEnv . shardLocations ( shardId ) ; if ( FileSystemUtils . exists ( shardLocations ) ) { logger . debug ( "{}<seq2seq4repair_space>deleting<seq2seq4repair_space>shard<seq2seq4repair_space>that<seq2seq4repair_space>is<seq2seq4repair_space>no<seq2seq4repair_space>longer<seq2seq4repair_space>used" , shardId ) ; FileSystemUtils . deleteRecursively ( shardLocations ) ; } } } else { if ( ! ( indexService . hasShard ( shardId . id ( ) ) ) ) { if ( indexService . store ( ) . canDeleteUnallocated ( shardId ) ) { logger . debug ( "{}<seq2seq4repair_space>deleting<seq2seq4repair_space>shard<seq2seq4repair_space>that<seq2seq4repair_space>is<seq2seq4repair_space>no<seq2seq4repair_space>longer<seq2seq4repair_space>used" , shardId ) ; try { indexService . store ( ) . deleteUnallocated ( shardId ) ; } catch ( Exception e ) { logger . debug ( "{}<seq2seq4repair_space>failed<seq2seq4repair_space>to<seq2seq4repair_space>delete<seq2seq4repair_space>unallocated<seq2seq4repair_space>shard,<seq2seq4repair_space>ignoring" , e , shardId ) ; } } } else { } } return currentState ; } @ Override public void onFailure ( String source , Throwable t ) { logger . error ( "{}<seq2seq4repair_space>unexpected<seq2seq4repair_space>error<seq2seq4repair_space>during<seq2seq4repair_space>deletion<seq2seq4repair_space>of<seq2seq4repair_space>unallocated<seq2seq4repair_space>shard" , t , shardId ) ; } } ) ; } } private class ShardActiveRequestHandler extends BaseTransportRequestHandler < IndicesStore . ShardActiveRequest > { @ Override public IndicesStore . ShardActiveRequest newInstance ( ) { } @ Override public String executor ( ) { } @ Override public void messageReceived ( IndicesStore . ShardActiveRequest request , TransportChannel channel ) throws Exception { } private boolean shardActive ( IndicesStore . ShardActiveRequest request ) { } } private static class ShardActiveRequest extends TransportRequest { private ClusterName clusterName ; private String indexUUID ; private ShardId shardId ; ShardActiveRequest ( ) { } ShardActiveRequest ( ClusterName clusterName , String indexUUID , ShardId shardId ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } private static class ShardActiveResponse extends TransportResponse { private boolean shardActive ; private DiscoveryNode node ; ShardActiveResponse ( ) { } ShardActiveResponse ( boolean shardActive , DiscoveryNode node ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } }
public class TribeTests extends ElasticsearchIntegrationTest { private static InternalTestCluster cluster2 ; private Node tribeNode ; private Client tribeClient ; @ BeforeClass public static void setupSecondCluster ( ) throws Exception { ElasticsearchIntegrationTest . beforeClass ( ) ; <START_BUG> TribeTests . cluster2 = new InternalTestCluster ( randomLong ( ) , 2 , 2 , Strings . randomBase64UUID ( getRandom ( ) ) , 0 , false , CHILD_JVM_ID ) ; <END_BUG> TribeTests . cluster2 . beforeTest ( getRandom ( ) , 0.1 ) ; TribeTests . cluster2 . ensureAtLeastNumDataNodes ( 2 ) ; } @ AfterClass public static void tearDownSecondCluster ( ) { } @ After public void tearDownTribeNode ( ) throws IOException { } private void setupTribeNode ( Settings settings ) { } @ Test public void testGlobalReadWriteBlocks ( ) throws Exception { } @ Test public void testIndexWriteBlocks ( ) throws Exception { } @ Test public void testOnConflictDrop ( ) throws Exception { } @ Test public void testOnConflictPrefer ( ) throws Exception { } private void testOnConflictPrefer ( String tribe ) throws Exception { } @ Test public void testTribeOnOneCluster ( ) throws Exception { } private void awaitIndicesInClusterState ( final String ... indices ) throws Exception { } private void awaitSameNodeCounts ( ) throws Exception { } private int countDataNodesForTribe ( String tribeName , DiscoveryNodes nodes ) { } }
public class PrototypeRendererGL20 implements ModelRenderer { static final int SIZE = 256 ; private final Array < Model > modelQueue = new Array < Model > ( false , PrototypeRendererGL20 . SIZE ) ; private final Array < StillModelInstance > modelInstances = new Array < StillModelInstance > ( false , PrototypeRendererGL20 . SIZE ) ; final MaterialShaderHandler materialShaderHandler ; private LightManager lightManager ; private boolean drawing ; private final Matrix3 normalMatrix = new Matrix3 ( ) ; public Camera cam ; PrototypeRendererGL20 . DrawableManager drawableManager = new PrototypeRendererGL20 . DrawableManager ( ) ; public PrototypeRendererGL20 ( LightManager lightManager ) { } @ Override public void begin ( ) { } @ Override public void draw ( StillModel model , StillModelInstance instance ) { } @ Override public void draw ( AnimatedModel model , AnimatedModelInstance instance ) { } @ Override public void end ( ) { } private ShaderProgram currentShader ; private final TextureAttribute [ ] lastTexture = new TextureAttribute [ TextureAttribute . MAX_TEXTURE_UNITS ] ; private void flush ( ) { } boolean bindShader ( Material material ) { ShaderProgram shader = material . getShader ( ) ; if ( shader == ( currentShader ) ) return false ; currentShader = shader ; currentShader . begin ( ) ; lightManager . applyGlobalLights ( currentShader ) ; lightManager . applyLights ( currentShader ) ; currentShader . setUniformMatrix ( "u_projectionViewMatrix" , cam . combined ) ; <START_BUG> currentShader . setUniformf ( "camPos" , cam . position . x , cam . position . y , cam . position . z ) ; <END_BUG> currentShader . setUniformf ( "camDir" , cam . direction . x , cam . direction . y , cam . direction . z ) ; return true ; } public void dispose ( ) { } private void renderBlended ( ) { } class DrawableManager { Pool < PrototypeRendererGL20 . DrawableManager . Drawable > drawablePool = new Pool < PrototypeRendererGL20 . DrawableManager . Drawable > ( ) { @ Override protected PrototypeRendererGL20 . DrawableManager . Drawable newObject ( ) { } } ; Pool < Material > materialPool = new Pool < Material > ( ) { @ Override protected Material newObject ( ) { } } ; Array < PrototypeRendererGL20 . DrawableManager . Drawable > drawables = new Array < PrototypeRendererGL20 . DrawableManager . Drawable > ( ) ; Array < PrototypeRendererGL20 . DrawableManager . Drawable > drawablesBlended = new Array < PrototypeRendererGL20 . DrawableManager . Drawable > ( ) ; public void add ( StillModel model , StillModelInstance instance ) { } public void add ( AnimatedModel model , AnimatedModelInstance instance ) { } public void clear ( ) { } private void clear ( Array < PrototypeRendererGL20 . DrawableManager . Drawable > drawables ) { } class Drawable implements Comparable { private static final int PRIORITY_DISCRETE_STEPS = 256 ; Model model ; final Matrix4 transform = new Matrix4 ( ) ; final Vector3 sortCenter = new Vector3 ( ) ; float boundingSphereRadius ; final Array < Material > materials = new Array < Material > ( ) ; boolean isAnimated ; String animation ; float animationTime ; boolean isLooping ; boolean blending ; int distance ; public void set ( StillModel model , StillModelInstance instance ) { } public void set ( AnimatedModel model , AnimatedModelInstance instance ) { } private void setCommon ( Model model , StillModelInstance instance ) { } @ Override public int compareTo ( Object other ) { } } } }
public class TransportSearchQueryAndFetchAction extends TransportSearchTypeAction { @ Inject public TransportSearchQueryAndFetchAction ( Settings settings , ThreadPool threadPool , ClusterService clusterService , TransportSearchCache transportSearchCache , SearchServiceTransportAction searchService , SearchPhaseController searchPhaseController ) { } @ Override protected void doExecute ( SearchRequest searchRequest , ActionListener < SearchResponse > listener ) { } private class AsyncAction extends BaseAsyncAction < QueryFetchSearchResult > { private final Map < SearchShardTarget , QueryFetchSearchResult > queryFetchResults = searchCache . obtainQueryFetchResults ( ) ; private AsyncAction ( SearchRequest request , ActionListener < SearchResponse > listener ) { } @ Override protected String firstPhaseName ( ) { } @ Override protected void sendExecuteFirstPhase ( DiscoveryNode node , InternalSearchRequest request , SearchServiceListener < QueryFetchSearchResult > listener ) { } @ Override protected void processFirstPhaseResult ( ShardRouting shard , QueryFetchSearchResult result ) { } @ Override protected void moveToSecondPhase ( ) throws Exception { sortedShardList = searchPhaseController . sortDocs ( queryFetchResults . values ( ) ) ; final InternalSearchResponse internalResponse = searchPhaseController . merge ( sortedShardList , queryFetchResults , queryFetchResults ) ; String scrollId = null ; if ( ( request . scroll ( ) ) != null ) { scrollId = buildScrollId ( request . searchType ( ) , queryFetchResults . values ( ) ) ; } <START_BUG> invokeListener ( new SearchResponse ( internalResponse , scrollId , expectedSuccessfulOps , successulOps . get ( ) , buildTookInMillis ( ) , buildShardFailures ( ) ) ) ; <END_BUG> searchCache . releaseQueryFetchResults ( queryFetchResults ) ; } } }
public class ShortFieldMapper extends NumberFieldMapper < Short > { public static final String CONTENT_TYPE = "short" ; public static class Defaults extends NumberFieldMapper . Defaults { public static final FieldType FIELD_TYPE = new FieldType ( NumberFieldMapper . Defaults . FIELD_TYPE ) ; public static final Short NULL_VALUE = null ; } public static class Builder extends NumberFieldMapper . Builder < ShortFieldMapper . Builder , ShortFieldMapper > { protected Short nullValue = ShortFieldMapper . Defaults . NULL_VALUE ; public Builder ( String name ) { } public ShortFieldMapper . Builder nullValue ( short nullValue ) { } @ Override public ShortFieldMapper build ( BuilderContext context ) { } } public static class TypeParser implements Mapper . TypeParser { @ Override public Mapper . Builder parse ( String name , Map < String , Object > node , ParserContext parserContext ) throws MapperParsingException { } } private Short nullValue ; private String nullValueAsString ; protected ShortFieldMapper ( Names names , int precisionStep , float boost , FieldType fieldType , Short nullValue , Explicit < Boolean > ignoreMalformed , PostingsFormatProvider provider , SimilarityProvider similarity , @ Nullable Settings fieldDataSettings ) { } @ Override public FieldType defaultFieldType ( ) { } @ Override public FieldDataType defaultFieldDataType ( ) { } @ Override protected int maxPrecisionStep ( ) { } @ Override public Short value ( Object value ) { } @ Override public BytesRef indexedValueForSearch ( Object value ) { BytesRef bytesRef = new BytesRef ( ) ; <START_BUG> NumericUtils . intToPrefixCoded ( parseValue ( value ) , precisionStep ( ) , bytesRef ) ; <END_BUG> return bytesRef ; } private short parseValue ( Object value ) { } private int parseValueAsInt ( Object value ) { } @ Override public Query fuzzyQuery ( String value , String minSim , int prefixLength , int maxExpansions , boolean transpositions ) { } @ Override public Query termQuery ( Object value , @ Nullable QueryParseContext context ) { } @ Override public Query rangeQuery ( Object lowerTerm , Object upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter termFilter ( Object value , @ Nullable QueryParseContext context ) { } @ Override public Filter rangeFilter ( Object lowerTerm , Object upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter rangeFilter ( IndexFieldDataService fieldData , Object lowerTerm , Object upperTerm , boolean includeLower , boolean includeUpper , @ Nullable QueryParseContext context ) { } @ Override public Filter nullValueFilter ( ) { } @ Override protected boolean customBoost ( ) { } @ Override protected Field innerParseCreateField ( ParseContext context ) throws IOException { } @ Override protected String contentType ( ) { } @ Override public void merge ( Mapper mergeWith , MergeContext mergeContext ) throws MergeMappingException { } @ Override protected void doXContentBody ( XContentBuilder builder ) throws IOException { } public static class CustomShortNumericField extends CustomNumericField { private final short number ; private final NumberFieldMapper mapper ; public CustomShortNumericField ( NumberFieldMapper mapper , short number , FieldType fieldType ) { } @ Override public TokenStream tokenStream ( Analyzer analyzer ) throws IOException { } @ Override public String numericAsString ( ) { } } }
@ Test public class SimpleAttachmentIntegrationTests { private final Logger logger = Loggers . getLogger ( getClass ( ) ) ; private Node node ; @ BeforeClass public void setupServer ( ) { } @ AfterClass public void closeServer ( ) { } @ BeforeMethod public void createIndex ( ) { } @ AfterMethod public void deleteIndex ( ) { } @ Test public void testSimpleAttachment ( ) throws Exception { String mapping = copyToStringFromClasspath ( "/org/elasticsearch/plugin/attachments/index/mapper/test-mapping.json" ) ; <START_BUG> node . client ( ) . admin ( ) . indices ( ) . putMapping ( putMappingRequest ( "test" ) . mappingSource ( mapping ) ) . actionGet ( ) ; <END_BUG> node . client ( ) . index ( indexRequest ( "test" ) . type ( "person" ) . source ( jsonBuilder ( ) . startObject ( ) . field ( "file" , copyToBytesFromClasspath ( "/org/elasticsearch/plugin/attachments/index/mapper/testXHTML.html" ) ) . endObject ( ) ) ) . actionGet ( ) ; node . client ( ) . admin ( ) . indices ( ) . refresh ( refreshRequest ( ) ) . actionGet ( ) ; CountResponse countResponse = node . client ( ) . count ( countRequest ( "test" ) . query ( fieldQuery ( "file.title" , "test<seq2seq4repair_space>document" ) ) ) . actionGet ( ) ; assertThat ( countResponse . count ( ) , equalTo ( 1L ) ) ; countResponse = node . client ( ) . count ( countRequest ( "test" ) . query ( fieldQuery ( "file" , "tests<seq2seq4repair_space>the<seq2seq4repair_space>ability" ) ) ) . actionGet ( ) ; assertThat ( countResponse . count ( ) , equalTo ( 1L ) ) ; } }
public class DashboardIssueFragment extends PagedItemFragment < RepositoryIssue > { public static final String ARG_FILTER = "filter" ; @ Inject private IssueService service ; @ Inject private IssueStore store ; @ Inject private AvatarLoader avatarHelper ; private Map < String , String > filterData ; @ Override public void onCreate ( Bundle savedInstanceState ) { } @ SuppressWarnings ( "unchecked" ) @ Override public void onActivityCreated ( Bundle savedInstanceState ) { } @ Override public void onListItemClick ( ListView l , View v , int position , long id ) { <START_BUG> startActivity ( ViewIssuesActivity . createIntent ( items , ( position - ( l . getHeaderViewsCount ( ) ) ) ) ) ; <END_BUG> } @ Override protected ResourcePager < RepositoryIssue > createPager ( ) { } @ Override protected int getLoadingMessage ( ) { } @ Override protected ItemListAdapter < RepositoryIssue , ? extends ItemView > createAdapter ( List < RepositoryIssue > items ) { } }
public class UnicastZenPing extends AbstractLifecycleComponent < ZenPing > implements ZenPing { public static final int LIMIT_PORTS_COUNT = 1 ; private final ThreadPool threadPool ; private final TransportService transportService ; private final ClusterName clusterName ; private final DiscoveryNode [ ] nodes ; private volatile DiscoveryNodesProvider nodesProvider ; private final AtomicInteger pingIdGenerator = new AtomicInteger ( ) ; private final Map < Integer , ConcurrentMap < DiscoveryNode , PingResponse > > receivedResponses = newConcurrentMap ( ) ; private final Queue < PingResponse > temporalResponses = new org . elasticsearch . common . util . concurrent . jsr166y . LinkedTransferQueue < PingResponse > ( ) ; private final CopyOnWriteArrayList < UnicastHostsProvider > hostsProviders = new CopyOnWriteArrayList < UnicastHostsProvider > ( ) ; public UnicastZenPing ( ThreadPool threadPool , TransportService transportService , ClusterName clusterName ) { } public UnicastZenPing ( Settings settings , ThreadPool threadPool , TransportService transportService , ClusterName clusterName ) { } @ Override protected void doStart ( ) throws ElasticSearchException { } @ Override protected void doStop ( ) throws ElasticSearchException { } @ Override protected void doClose ( ) throws ElasticSearchException { } public void addHostsProvider ( UnicastHostsProvider provider ) { } public void removeHostsProvider ( UnicastHostsProvider provider ) { } @ Override public void setNodesProvider ( DiscoveryNodesProvider nodesProvider ) { } public PingResponse [ ] pingAndWait ( TimeValue timeout ) { } @ Override public void ping ( final PingListener listener , final TimeValue timeout ) throws ElasticSearchException { } private void sendPings ( final int id , final TimeValue timeout , boolean wait ) { } private void sendPingRequestToNode ( final int id , TimeValue timeout , UnicastZenPing . UnicastPingRequest pingRequest , final CountDownLatch latch , final DiscoveryNode node , final boolean disconnect , final DiscoveryNode nodeToSend ) { logger . trace ( "[{}]<seq2seq4repair_space>connecting<seq2seq4repair_space>to<seq2seq4repair_space>{},<seq2seq4repair_space>disconnect[{}]" , id , nodeToSend , disconnect ) ; transportService . sendRequest ( nodeToSend , UnicastZenPing . UnicastPingRequestHandler . ACTION , pingRequest , TransportRequestOptions . options ( ) . withTimeout ( ( ( long ) ( ( timeout . millis ( ) ) * 1.25 ) ) ) , new org . elasticsearch . transport . BaseTransportResponseHandler < UnicastZenPing . UnicastPingResponse > ( ) { @ Override public UnicastZenPing . UnicastPingResponse newInstance ( ) { return new UnicastZenPing . UnicastPingResponse ( ) ; } @ Override public String executor ( ) { return Names . SAME ; } @ Override public void handleResponse ( UnicastZenPing . UnicastPingResponse response ) { logger . trace ( "[{}]<seq2seq4repair_space>received<seq2seq4repair_space>response<seq2seq4repair_space>from<seq2seq4repair_space>{}:<seq2seq4repair_space>{}" , id , nodeToSend , Arrays . toString ( response . pingResponses ) ) ; try { DiscoveryNodes discoveryNodes = nodesProvider . nodes ( ) ; for ( PingResponse pingResponse : response . pingResponses ) { if ( disconnect ) { transportService . disconnectFromNode ( nodeToSend ) ; } if ( pingResponse . target ( ) . id ( ) . equals ( discoveryNodes . localNodeId ( ) ) ) { continue ; } if ( ! ( pingResponse . clusterName ( ) . equals ( clusterName ) ) ) { logger . debug ( "[{}]<seq2seq4repair_space>filtering<seq2seq4repair_space>out<seq2seq4repair_space>response<seq2seq4repair_space>from<seq2seq4repair_space>{},<seq2seq4repair_space>not<seq2seq4repair_space>same<seq2seq4repair_space>cluster_name<seq2seq4repair_space>[{}]" , pingResponse . target ( ) , pingResponse . clusterName ( ) . value ( ) ) ; return ; } ConcurrentMap < DiscoveryNode , PingResponse > responses = receivedResponses . get ( response . id ) ; if ( responses == null ) { <START_BUG> logger . warn ( "received<seq2seq4repair_space>ping<seq2seq4repair_space>response<seq2seq4repair_space>with<seq2seq4repair_space>no<seq2seq4repair_space>matching<seq2seq4repair_space>id<seq2seq4repair_space>[{}]" , response . id ) ; <END_BUG> } else { responses . put ( pingResponse . target ( ) , pingResponse ) ; } } } finally { latch . countDown ( ) ; } } @ Override public void handleException ( TransportException exp ) { latch . countDown ( ) ; if ( exp instanceof ConnectTransportException ) { logger . trace ( "failed<seq2seq4repair_space>to<seq2seq4repair_space>connect<seq2seq4repair_space>to<seq2seq4repair_space>{}" , exp , nodeToSend ) ; } else { if ( disconnect ) { transportService . disconnectFromNode ( nodeToSend ) ; } logger . warn ( "failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>ping<seq2seq4repair_space>to<seq2seq4repair_space>[{}]" , exp , node ) ; } } } ) ; } private UnicastZenPing . UnicastPingResponse handlePingRequest ( final UnicastZenPing . UnicastPingRequest request ) { } class UnicastPingRequestHandler extends BaseTransportRequestHandler < UnicastZenPing . UnicastPingRequest > { static final String ACTION = "discovery/zen/unicast" ; @ Override public UnicastZenPing . UnicastPingRequest newInstance ( ) { } @ Override public String executor ( ) { } @ Override public void messageReceived ( UnicastZenPing . UnicastPingRequest request , TransportChannel channel ) throws Exception { } } static class UnicastPingRequest implements Streamable { int id ; TimeValue timeout ; PingResponse pingResponse ; UnicastPingRequest ( ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } static class UnicastPingResponse implements Streamable { int id ; PingResponse [ ] pingResponses ; UnicastPingResponse ( ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { } @ Override public void writeTo ( StreamOutput out ) throws IOException { } } }
public class Loggers { private static final String commonPrefix = System . getProperty ( "es.logger.prefix" , "org.elasticsearch." ) ; public static final String SPACE = "<seq2seq4repair_space>" ; private static boolean consoleLoggingEnabled = true ; public static void disableConsoleLogging ( ) { } public static void enableConsoleLogging ( ) { } public static boolean consoleLoggingEnabled ( ) { } public static ESLogger getLogger ( Class clazz , Settings settings , ShardId shardId , String ... prefixes ) { } public static ESLogger getLogger ( Class clazz , Settings settings , Index index , String ... prefixes ) { } public static ESLogger getLogger ( Class clazz , Settings settings , RiverName riverName , String ... prefixes ) { } public static ESLogger getLogger ( Class clazz , Settings settings , String ... prefixes ) { } public static ESLogger getLogger ( String loggerName , Settings settings , String ... prefixes ) { } public static ESLogger getLogger ( ESLogger parentLogger , String s ) { } public static ESLogger getLogger ( String s ) { <START_BUG> return ESLoggerFactory . getLogger ( s ) ; <END_BUG> } public static ESLogger getLogger ( Class clazz ) { } public static ESLogger getLogger ( Class clazz , String ... prefixes ) { } public static ESLogger getLogger ( String name , String ... prefixes ) { } private static String buildClassLoggerName ( Class clazz ) { } private static String getLoggerName ( String name ) { } }
public class Bootstrap { private Server server ; private void setup ( boolean addShutdownHook ) throws Exception { Tuple < Settings , Environment > tuple = InternalSettingsPerparer . prepareSettings ( EMPTY_SETTINGS , true ) ; try { Classes . getDefaultClassLoader ( ) . loadClass ( "org.apache.log4j.Logger" ) ; LogConfigurator . configure ( tuple . v1 ( ) ) ; } catch ( ClassNotFoundException e ) { } catch ( NoClassDefFoundError e ) { } catch ( Exception e ) { System . err . println ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>configure<seq2seq4repair_space>logging..." ) ; e . printStackTrace ( ) ; } if ( ( tuple . v1 ( ) . get ( CREATE_CONNECTOR ) ) == null ) { <START_BUG> Settings updated = settingsBuilder ( ) . putAll ( tuple . v1 ( ) ) . putBoolean ( CREATE_CONNECTOR , true ) . build ( ) ; <END_BUG> tuple = new Tuple < Settings , Environment > ( updated , tuple . v2 ( ) ) ; } ServerBuilder serverBuilder = ServerBuilder . serverBuilder ( ) . settings ( tuple . v1 ( ) ) . loadConfigSettings ( false ) ; server = serverBuilder . build ( ) ; if ( addShutdownHook ) { Runtime . getRuntime ( ) . addShutdownHook ( new Thread ( ) { @ Override public void run ( ) { server . close ( ) ; } } ) ; } } public void init ( String [ ] args ) throws Exception { } public void start ( ) { } public void stop ( ) { } public void destroy ( ) { } public static void main ( String [ ] args ) { } }
public class TransportNodesShutdownAction extends TransportMasterNodeOperationAction < NodesShutdownRequest , NodesShutdownResponse > { private final Node node ; private final ClusterName clusterName ; private final boolean disabled ; private final TimeValue delay ; @ Inject public TransportNodesShutdownAction ( Settings settings , TransportService transportService , ClusterService clusterService , ThreadPool threadPool , Node node , ClusterName clusterName ) { } @ Override protected String transportAction ( ) { } @ Override protected NodesShutdownRequest newRequest ( ) { } @ Override protected NodesShutdownResponse newResponse ( ) { } @ Override protected void processBeforeDelegationToMaster ( NodesShutdownRequest request , ClusterState state ) { } @ Override protected NodesShutdownResponse masterOperation ( final NodesShutdownRequest request , final ClusterState state ) throws ElasticSearchException { if ( disabled ) { throw new ElasticSearchIllegalStateException ( "Shutdown<seq2seq4repair_space>is<seq2seq4repair_space>disabled" ) ; } Set < DiscoveryNode > nodes = Sets . newHashSet ( ) ; if ( Actions . isAllNodes ( request . nodesIds ) ) { logger . info ( "[cluster_shutdown]:<seq2seq4repair_space>requested,<seq2seq4repair_space>shutting<seq2seq4repair_space>down<seq2seq4repair_space>in<seq2seq4repair_space>[{}]" , request . delay ) ; nodes . addAll ( state . nodes ( ) . nodes ( ) . values ( ) ) ; Thread t = new Thread ( new Runnable ( ) { @ Override public void run ( ) { try { Thread . sleep ( request . delay . millis ( ) ) ; } catch ( InterruptedException e ) { } logger . trace ( "[cluster_shutdown]:<seq2seq4repair_space>stopping<seq2seq4repair_space>the<seq2seq4repair_space>cluster<seq2seq4repair_space>service<seq2seq4repair_space>so<seq2seq4repair_space>no<seq2seq4repair_space>re-routing<seq2seq4repair_space>will<seq2seq4repair_space>occur" ) ; clusterService . stop ( ) ; final CountDownLatch latch = new CountDownLatch ( state . nodes ( ) . size ( ) ) ; for ( final DiscoveryNode node : state . nodes ( ) ) { if ( node . id ( ) . equals ( state . nodes ( ) . masterNodeId ( ) ) ) { latch . countDown ( ) ; } else { logger . trace ( "[cluster_shutdown]:<seq2seq4repair_space>sending<seq2seq4repair_space>shutdown<seq2seq4repair_space>request<seq2seq4repair_space>to<seq2seq4repair_space>[{}]" , node ) ; transportService . sendRequest ( node , TransportNodesShutdownAction . NodeShutdownRequestHandler . ACTION , INSTANCE , new VoidTransportResponseHandler ( ) { @ Override public void handleResponse ( VoidStreamable response ) { logger . trace ( "[cluster_shutdown]:<seq2seq4repair_space>received<seq2seq4repair_space>shutdown<seq2seq4repair_space>response<seq2seq4repair_space>from<seq2seq4repair_space>[{}]" , node ) ; latch . countDown ( ) ; } @ Override public void handleException ( RemoteTransportException exp ) { logger . warn ( "[cluster_shutdown]:<seq2seq4repair_space>received<seq2seq4repair_space>failed<seq2seq4repair_space>shutdown<seq2seq4repair_space>response<seq2seq4repair_space>from<seq2seq4repair_space>[{}]" , exp , node ) ; latch . countDown ( ) ; } } ) ; } } try { latch . await ( ) ; } catch ( InterruptedException e ) { } <START_BUG> logger . info ( "[cluster_shutdown]:<seq2seq4repair_space>done<seq2seq4repair_space>shutting<seq2seq4repair_space>done<seq2seq4repair_space>all<seq2seq4repair_space>nodes<seq2seq4repair_space>except<seq2seq4repair_space>master,<seq2seq4repair_space>proceeding<seq2seq4repair_space>to<seq2seq4repair_space>master" ) ; <END_BUG> logger . trace ( "[cluster_shutdown]:<seq2seq4repair_space>shutting<seq2seq4repair_space>down<seq2seq4repair_space>the<seq2seq4repair_space>master<seq2seq4repair_space>[{}]" , state . nodes ( ) . masterNode ( ) ) ; transportService . sendRequest ( state . nodes ( ) . masterNode ( ) , TransportNodesShutdownAction . NodeShutdownRequestHandler . ACTION , INSTANCE , new VoidTransportResponseHandler ( ) { @ Override public void handleResponse ( VoidStreamable response ) { logger . trace ( "[cluster_shutdown]:<seq2seq4repair_space>received<seq2seq4repair_space>shutdown<seq2seq4repair_space>response<seq2seq4repair_space>from<seq2seq4repair_space>master" ) ; } @ Override public void handleException ( RemoteTransportException exp ) { logger . warn ( "[cluster_shutdown]:<seq2seq4repair_space>received<seq2seq4repair_space>failed<seq2seq4repair_space>shutdown<seq2seq4repair_space>response<seq2seq4repair_space>master" , exp ) ; } } ) ; } } ) ; t . start ( ) ; } else { final String [ ] nodesIds = Actions . buildNodesIds ( state . nodes ( ) , request . nodesIds ) ; logger . info ( "[partial_cluster_shutdown]:<seq2seq4repair_space>requested,<seq2seq4repair_space>shutting<seq2seq4repair_space>down<seq2seq4repair_space>[{}]<seq2seq4repair_space>in<seq2seq4repair_space>[{}]" , nodesIds , request . delay ) ; for ( String nodeId : nodesIds ) { final DiscoveryNode node = state . nodes ( ) . get ( nodeId ) ; if ( node != null ) { nodes . add ( node ) ; } } Thread t = new Thread ( new Runnable ( ) { @ Override public void run ( ) { try { Thread . sleep ( request . delay . millis ( ) ) ; } catch ( InterruptedException e ) { } final CountDownLatch latch = new CountDownLatch ( nodesIds . length ) ; for ( String nodeId : nodesIds ) { final DiscoveryNode node = state . nodes ( ) . get ( nodeId ) ; if ( node == null ) { logger . warn ( "[partial_cluster_shutdown]:<seq2seq4repair_space>no<seq2seq4repair_space>node<seq2seq4repair_space>to<seq2seq4repair_space>shutdown<seq2seq4repair_space>for<seq2seq4repair_space>node_id<seq2seq4repair_space>[{}]" , nodeId ) ; latch . countDown ( ) ; continue ; } logger . trace ( "[partial_cluster_shutdown]:<seq2seq4repair_space>sending<seq2seq4repair_space>shutdown<seq2seq4repair_space>request<seq2seq4repair_space>to<seq2seq4repair_space>[{}]" , node ) ; transportService . sendRequest ( node , TransportNodesShutdownAction . NodeShutdownRequestHandler . ACTION , INSTANCE , new VoidTransportResponseHandler ( ) { @ Override public void handleResponse ( VoidStreamable response ) { logger . trace ( "[partial_cluster_shutdown]:<seq2seq4repair_space>received<seq2seq4repair_space>shutdown<seq2seq4repair_space>response<seq2seq4repair_space>from<seq2seq4repair_space>[{}]" , node ) ; latch . countDown ( ) ; } @ Override public void handleException ( RemoteTransportException exp ) { logger . warn ( "[partial_cluster_shutdown]:<seq2seq4repair_space>received<seq2seq4repair_space>failed<seq2seq4repair_space>shutdown<seq2seq4repair_space>response<seq2seq4repair_space>from<seq2seq4repair_space>[{}]" , exp , node ) ; latch . countDown ( ) ; } } ) ; } try { latch . await ( ) ; } catch ( InterruptedException e ) { } logger . info ( "[partial_cluster_shutdown]:<seq2seq4repair_space>done<seq2seq4repair_space>shutting<seq2seq4repair_space>down<seq2seq4repair_space>[{}]" , nodesIds ) ; } } ) ; t . start ( ) ; } return new NodesShutdownResponse ( clusterName , nodes . toArray ( new DiscoveryNode [ nodes . size ( ) ] ) ) ; } private class NodeShutdownRequestHandler extends BaseTransportRequestHandler < VoidStreamable > { static final String ACTION = "/cluster/nodes/shutdown/node" ; @ Override public VoidStreamable newInstance ( ) { } @ Override public void messageReceived ( VoidStreamable request , TransportChannel channel ) throws Exception { } } }
protected Element root ; protected boolean yUp ; protected boolean convertObjectToTileSpace ; protected int mapTileWidth ; protected int mapTileHeight ; protected int mapWidthInPixels ; protected int mapHeightInPixels ; protected TiledMap map ; protected Array < Texture > trackedTextures = new Array < Texture > ( ) ; private interface AtlasResolver { public TextureAtlas getAtlas ( String name ) { } public static class DirectAtlasResolver implements AtlasTmxMapLoader . AtlasResolver { private final ObjectMap < String , TextureAtlas > atlases ; public DirectAtlasResolver ( ObjectMap < String , TextureAtlas > atlases ) { } @ Override public TextureAtlas getAtlas ( String name ) { } } public static class AssetManagerAtlasResolver implements AtlasTmxMapLoader . AtlasResolver { private final AssetManager assetManager ; public AssetManagerAtlasResolver ( AssetManager assetManager ) { } @ Override public TextureAtlas getAtlas ( String name ) { } } } public AtlasTmxMapLoader ( ) { } public AtlasTmxMapLoader ( FileHandleResolver resolver ) { } public TiledMap load ( String fileName ) { } @ Override public Array < AssetDescriptor > getDependencies ( String fileName , FileHandle tmxFile , AtlasTmxMapLoader . AtlasTiledMapLoaderParameters parameter ) { } public TiledMap load ( String fileName , AtlasTmxMapLoader . AtlasTiledMapLoaderParameters parameter ) { } protected FileHandle loadAtlas ( Element root , FileHandle tmxFile ) throws IOException { } private void setTextureFilters ( TextureFilter min , TextureFilter mag ) { } @ Override public void loadAsync ( AssetManager manager , String fileName , FileHandle tmxFile , AtlasTmxMapLoader . AtlasTiledMapLoaderParameters parameter ) { } @ Override public TiledMap loadSync ( AssetManager manager , String fileName , FileHandle file , AtlasTmxMapLoader . AtlasTiledMapLoaderParameters parameter ) { } protected TiledMap loadMap ( Element root , FileHandle tmxFile , AtlasTmxMapLoader . AtlasResolver resolver , AtlasTmxMapLoader . AtlasTiledMapLoaderParameters parameter ) { } protected void loadTileset ( TiledMap map , Element element , FileHandle tmxFile , AtlasTmxMapLoader . AtlasResolver resolver , AtlasTmxMapLoader . AtlasTiledMapLoaderParameters parameter ) { if ( element . getName ( ) . equals ( "tileset" ) ) { String name = element . get ( "name" , null ) ; int firstgid = element . getIntAttribute ( "firstgid" , 1 ) ; int tilewidth = element . getIntAttribute ( "tilewidth" , 0 ) ; int tileheight = element . getIntAttribute ( "tileheight" , 0 ) ; int spacing = element . getIntAttribute ( "spacing" , 0 ) ; int margin = element . getIntAttribute ( "margin" , 0 ) ; String source = element . getAttribute ( "source" , null ) ; String imageSource = "" ; int imageWidth = 0 ; int imageHeight = 0 ; FileHandle image = null ; if ( source != null ) { FileHandle tsx = AtlasTmxMapLoader . getRelativeFileHandle ( tmxFile , source ) ; try { element = xml . parse ( tsx ) ; name = element . get ( "name" , null ) ; tilewidth = element . getIntAttribute ( "tilewidth" , 0 ) ; tileheight = element . getIntAttribute ( "tileheight" , 0 ) ; spacing = element . getIntAttribute ( "spacing" , 0 ) ; margin = element . getIntAttribute ( "margin" , 0 ) ; Element imageElement = element . getChildByName ( "image" ) ; imageSource = imageElement . getAttribute ( "source" ) ; imageWidth = imageElement . getIntAttribute ( "width" , 0 ) ; imageHeight = imageElement . getIntAttribute ( "height" , 0 ) ; } catch ( IOException e ) { throw new GdxRuntimeException ( "Error<seq2seq4repair_space>parsing<seq2seq4repair_space>external<seq2seq4repair_space>tileset." ) ; } } else { Element imageElement = element . getChildByName ( "image" ) ; if ( imageElement != null ) { imageSource = imageElement . getAttribute ( "source" ) ; imageWidth = imageElement . getIntAttribute ( "width" , 0 ) ; imageHeight = imageElement . getIntAttribute ( "height" , 0 ) ; } } String atlasFilePath = map . getProperties ( ) . get ( "atlas" , String . class ) ; if ( atlasFilePath == null ) { FileHandle atlasFile = tmxFile . sibling ( ( ( tmxFile . nameWithoutExtension ( ) ) + ".atlas" ) ) ; if ( atlasFile . exists ( ) ) <START_BUG> atlasFilePath = atlasFile . path ( ) ; <END_BUG> } if ( atlasFilePath == null ) { throw new GdxRuntimeException ( "The<seq2seq4repair_space>map<seq2seq4repair_space>is<seq2seq4repair_space>missing<seq2seq4repair_space>the<seq2seq4repair_space>'atlas'<seq2seq4repair_space>property" ) ; } FileHandle atlasHandle = AtlasTmxMapLoader . getRelativeFileHandle ( tmxFile , atlasFilePath ) ; atlasHandle = resolve ( atlasHandle . path ( ) ) ; TextureAtlas atlas = resolver . getAtlas ( atlasHandle . path ( ) ) ; String regionsName = atlasHandle . nameWithoutExtension ( ) ; if ( ( parameter != null ) && ( parameter . forceTextureFilters ) ) { for ( Texture texture : atlas . getTextures ( ) ) { trackedTextures . add ( texture ) ; } } TiledMapTileSet tileset = new TiledMapTileSet ( ) ; MapProperties props = tileset . getProperties ( ) ; tileset . setName ( name ) ; props . put ( "firstgid" , firstgid ) ; props . put ( "imagesource" , imageSource ) ; props . put ( "imagewidth" , imageWidth ) ; props . put ( "imageheight" , imageHeight ) ; props . put ( "tilewidth" , tilewidth ) ; props . put ( "tileheight" , tileheight ) ; props . put ( "margin" , margin ) ; props . put ( "spacing" , spacing ) ; for ( AtlasRegion region : atlas . findRegions ( regionsName ) ) { if ( region != null ) { StaticTiledMapTile tile = new StaticTiledMapTile ( region ) ; if ( ! ( yUp ) ) { region . flip ( false , true ) ; } int tileid = firstgid + ( region . index ) ; tile . setId ( tileid ) ; tileset . putTile ( tileid , tile ) ; } } for ( Element tileElement : element . getChildrenByName ( "tile" ) ) { int tileid = firstgid + ( tileElement . getIntAttribute ( "id" , 0 ) ) ; TiledMapTile tile = tileset . getTile ( tileid ) ;
public void cancelRecovery ( IndexShard indexShard ) { } public void startRecovery ( final StartRecoveryRequest request , final InternalIndexShard indexShard , final RecoveryTarget . RecoveryListener listener ) { } public void retryRecovery ( final StartRecoveryRequest request , TimeValue retryAfter , final RecoveryStatus status , final RecoveryTarget . RecoveryListener listener ) { } private void doRecovery ( final StartRecoveryRequest request , final RecoveryStatus recoveryStatus , final RecoveryTarget . RecoveryListener listener ) { assert ( request . sourceNode ( ) ) != null : "can't<seq2seq4repair_space>do<seq2seq4repair_space>a<seq2seq4repair_space>recovery<seq2seq4repair_space>without<seq2seq4repair_space>a<seq2seq4repair_space>source<seq2seq4repair_space>node" ; final InternalIndexShard shard = recoveryStatus . indexShard ; if ( shard == null ) { listener . onIgnoreRecovery ( false , "shard<seq2seq4repair_space>missing<seq2seq4repair_space>locally,<seq2seq4repair_space>stop<seq2seq4repair_space>recovery" ) ; return ; } if ( ( shard . state ( ) ) == ( IndexShardState . CLOSED ) ) { listener . onIgnoreRecovery ( false , "local<seq2seq4repair_space>shard<seq2seq4repair_space>closed,<seq2seq4repair_space>stop<seq2seq4repair_space>recovery" ) ; return ; } if ( recoveryStatus . isCanceled ( ) ) { listener . onIgnoreRecovery ( false , "canceled<seq2seq4repair_space>recovery" ) ; return ; } recoveryStatus . recoveryThread = Thread . currentThread ( ) ; try { logger . trace ( "[{}][{}]<seq2seq4repair_space>starting<seq2seq4repair_space>recovery<seq2seq4repair_space>from<seq2seq4repair_space>{}" , request . shardId ( ) . index ( ) . name ( ) , request . shardId ( ) . id ( ) , request . sourceNode ( ) ) ; StopWatch stopWatch = new StopWatch ( ) . start ( ) ; RecoveryResponse recoveryResponse = transportService . submitRequest ( request . sourceNode ( ) , START_RECOVERY , request , new FutureTransportResponseHandler < RecoveryResponse > ( ) { @ Override public RecoveryResponse newInstance ( ) { return new RecoveryResponse ( ) ; } } ) . txGet ( ) ; if ( ( shard . state ( ) ) == ( IndexShardState . CLOSED ) ) { removeAndCleanOnGoingRecovery ( recoveryStatus ) ; listener . onIgnoreRecovery ( false , "local<seq2seq4repair_space>shard<seq2seq4repair_space>closed,<seq2seq4repair_space>stop<seq2seq4repair_space>recovery" ) ; return ; } stopWatch . stop ( ) ; if ( logger . isTraceEnabled ( ) ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( '[' ) . append ( request . shardId ( ) . index ( ) . name ( ) ) . append ( ']' ) . append ( '[' ) . append ( request . shardId ( ) . id ( ) ) . append ( "]<seq2seq4repair_space>" ) ; sb . append ( "recovery<seq2seq4repair_space>completed<seq2seq4repair_space>from<seq2seq4repair_space>" ) . append ( request . sourceNode ( ) ) . append ( ",<seq2seq4repair_space>took[" ) . append ( stopWatch . totalTime ( ) ) . append ( "]\n" ) ; sb . append ( "<seq2seq4repair_space>phase1:<seq2seq4repair_space>recovered_files<seq2seq4repair_space>[" ) . append ( recoveryResponse . phase1FileNames . size ( ) ) . append ( "]" ) . append ( "<seq2seq4repair_space>with<seq2seq4repair_space>total_size<seq2seq4repair_space>of<seq2seq4repair_space>[" ) . append ( new org . elasticsearch . common . unit . ByteSizeValue ( recoveryResponse . phase1TotalSize ) ) . append ( "]" ) . append ( ",<seq2seq4repair_space>took<seq2seq4repair_space>[" ) . append ( timeValueMillis ( recoveryResponse . phase1Time ) ) . append ( "],<seq2seq4repair_space>throttling_wait<seq2seq4repair_space>[" ) . append ( timeValueMillis ( recoveryResponse . phase1ThrottlingWaitTime ) ) . append ( ']' ) . append ( "\n" ) ; sb . append ( "<seq2seq4repair_space>:<seq2seq4repair_space>reusing_files<seq2seq4repair_space>[" ) . append ( recoveryResponse . phase1ExistingFileNames . size ( ) ) . append ( "]<seq2seq4repair_space>with<seq2seq4repair_space>total_size<seq2seq4repair_space>of<seq2seq4repair_space>[" ) . append ( new org . elasticsearch . common . unit . ByteSizeValue ( recoveryResponse . phase1ExistingTotalSize ) ) . append ( "]\n" ) ; sb . append ( "<seq2seq4repair_space>phase2:<seq2seq4repair_space>start<seq2seq4repair_space>took<seq2seq4repair_space>[" ) . append ( timeValueMillis ( recoveryResponse . startTime ) ) . append ( "]\n" ) ; sb . append ( "<seq2seq4repair_space>:<seq2seq4repair_space>recovered<seq2seq4repair_space>[" ) . append ( recoveryResponse . phase2Operations ) . append ( "]" ) . append ( "<seq2seq4repair_space>transaction<seq2seq4repair_space>log<seq2seq4repair_space>operations" ) . append ( ",<seq2seq4repair_space>took<seq2seq4repair_space>[" ) . append ( timeValueMillis ( recoveryResponse . phase2Time ) ) . append ( "]" ) . append ( "\n" ) ; sb . append ( "<seq2seq4repair_space>phase3:<seq2seq4repair_space>recovered<seq2seq4repair_space>[" ) . append ( recoveryResponse . phase3Operations ) . append ( "]" ) . append ( "<seq2seq4repair_space>transaction<seq2seq4repair_space>log<seq2seq4repair_space>operations" ) . append ( ",<seq2seq4repair_space>took<seq2seq4repair_space>[" ) . append ( timeValueMillis ( recoveryResponse . phase3Time ) ) . append ( "]" ) ; logger . trace ( sb . toString ( ) ) ; } else if ( logger . isDebugEnabled ( ) ) { <START_BUG> logger . debug ( "recovery<seq2seq4repair_space>completed<seq2seq4repair_space>from<seq2seq4repair_space>[{}],<seq2seq4repair_space>took<seq2seq4repair_space>[{}]" , request . shardId ( ) , request . sourceNode ( ) , stopWatch . totalTime ( ) ) ; <END_BUG> } removeAndCleanOnGoingRecovery ( recoveryStatus ) ; listener . onRecoveryDone ( ) ; } catch ( Throwable e ) { if ( recoveryStatus . isCanceled ( ) ) { listener . onIgnoreRecovery ( false , "canceled<seq2seq4repair_space>recovery" ) ; return ; } if ( ( shard . state ( ) ) == ( IndexShardState . CLOSED ) ) { removeAndCleanOnGoingRecovery ( recoveryStatus ) ; listener . onIgnoreRecovery ( false , "local<seq2seq4repair_space>shard<seq2seq4repair_space>closed,<seq2seq4repair_space>stop<seq2seq4repair_space>recovery" ) ; return ; } Throwable cause = ExceptionsHelper . unwrapCause ( e ) ; if ( cause instanceof RecoveryEngineException ) { cause = cause . getCause ( ) ; } cause = ExceptionsHelper . unwrapCause ( cause ) ; if ( cause instanceof RecoveryEngineException ) { cause = cause . getCause ( ) ; } if ( ( ( cause instanceof IndexShardNotStartedException ) || ( cause instanceof IndexMissingException ) ) || ( cause instanceof IndexShardMissingException ) ) { listener . onRetryRecovery ( TimeValue . timeValueMillis ( 500 ) , recoveryStatus ) ; return ; } if ( cause instanceof DelayRecoveryException ) { listener . onRetryRecovery ( TimeValue . timeValueMillis ( 500 ) , recoveryStatus ) ; return ; } removeAndCleanOnGoingRecovery ( recoveryStatus ) ; if ( cause instanceof ConnectTransportException ) { listener . onIgnoreRecovery ( true , ( ( "source<seq2seq4repair_space>node<seq2seq4repair_space>disconnected<seq2seq4repair_space>(" + ( request . sourceNode ( ) ) ) + ")" ) ) ; return ; } if ( cause instanceof IndexShardClosedException ) { listener . onIgnoreRecovery ( true , ( ( "source<seq2seq4repair_space>shard<seq2seq4repair_space>is<seq2seq4repair_space>closed<seq2seq4repair_space>(" + ( request . sourceNode ( ) ) ) + ")" ) ) ; return ; } if ( cause instanceof AlreadyClosedException ) { listener . onIgnoreRecovery ( true , ( ( "source<seq2seq4repair_space>shard<seq2seq4repair_space>is<seq2seq4repair_space>closed<seq2seq4repair_space>(" + ( request . sourceNode ( ) ) ) + ")" ) ) ; return ; } logger . warn ( "[{}][{}]<seq2seq4repair_space>recovery<seq2seq4repair_space>from<seq2seq4repair_space>[{}]<seq2seq4repair_space>failed"
if ( cause instanceof ConnectTransportException ) { continue ; } sb . append ( "\n<seq2seq4repair_space>-><seq2seq4repair_space>" ) . append ( nodesStoreFilesMetaData . failures ( ) [ i ] . getDetailedMessage ( ) ) ; } logger . debug ( sb . toString ( ) ) ; } } long lastSizeMatched = 0 ; DiscoveryNode lastDiscoNodeMatched = null ; RoutingNode lastNodeMatched = null ; for ( TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData nodeStoreFilesMetaData : nodesStoreFilesMetaData ) { DiscoveryNode discoNode = nodeStoreFilesMetaData . node ( ) ; logger . trace ( "{}:<seq2seq4repair_space>checking<seq2seq4repair_space>node<seq2seq4repair_space>[{}]" , shard , discoNode ) ; IndexStore . StoreFilesMetaData storeFilesMetaData = nodeStoreFilesMetaData . storeFilesMetaData ( ) ; if ( storeFilesMetaData == null ) { continue ; } RoutingNode node = routingNodes . node ( discoNode . id ( ) ) ; if ( node == null ) { continue ; } if ( ( nodeAllocations . canAllocate ( shard , node , routingNodes ) ) == ( Decision . NO ) ) { continue ; } if ( storeFilesMetaData . allocated ( ) ) { continue ; } if ( ( shard . primary ( ) ) && ( ( indexService . gateway ( ) ) instanceof BlobStoreIndexGateway ) ) { BlobStoreIndexGateway indexGateway = ( ( BlobStoreIndexGateway ) ( indexService . gateway ( ) ) ) ; try { CommitPoint commitPoint = indexGateway . findCommitPoint ( shard . id ( ) ) ; if ( logger . isTraceEnabled ( ) ) { StringBuilder sb = new StringBuilder ( ( ( ( shard + ":<seq2seq4repair_space>checking<seq2seq4repair_space>for<seq2seq4repair_space>pre_allocation<seq2seq4repair_space>(gateway)<seq2seq4repair_space>on<seq2seq4repair_space>node<seq2seq4repair_space>" ) + discoNode ) + "\n" ) ) ; sb . append ( "<seq2seq4repair_space>gateway_files:\n" ) ; for ( CommitPoint . FileInfo fileInfo : commitPoint . indexFiles ( ) ) { sb . append ( "<seq2seq4repair_space>[" ) . append ( fileInfo . name ( ) ) . append ( "]/[" ) . append ( fileInfo . physicalName ( ) ) . append ( "],<seq2seq4repair_space>size<seq2seq4repair_space>[" ) . append ( new ByteSizeValue ( fileInfo . length ( ) ) ) . append ( "]\n" ) ; } sb . append ( "<seq2seq4repair_space>node_files:\n" ) ; for ( StoreFileMetaData md : storeFilesMetaData ) { sb . append ( "<seq2seq4repair_space>[" ) . append ( md . name ( ) ) . append ( "],<seq2seq4repair_space>size<seq2seq4repair_space>[" ) . append ( new ByteSizeValue ( md . length ( ) ) ) . append ( "]\n" ) ; } logger . trace ( sb . toString ( ) ) ; } long sizeMatched = 0 ; for ( StoreFileMetaData storeFileMetaData : storeFilesMetaData ) { CommitPoint . FileInfo fileInfo = commitPoint . findPhysicalIndexFile ( storeFileMetaData . name ( ) ) ; if ( fileInfo != null ) { if ( ( fileInfo . length ( ) ) == ( storeFileMetaData . length ( ) ) ) { logger . trace ( "{}:<seq2seq4repair_space>[{}]<seq2seq4repair_space>reusing<seq2seq4repair_space>file<seq2seq4repair_space>since<seq2seq4repair_space>it<seq2seq4repair_space>exists<seq2seq4repair_space>on<seq2seq4repair_space>remote<seq2seq4repair_space>node<seq2seq4repair_space>and<seq2seq4repair_space>on<seq2seq4repair_space>gateway<seq2seq4repair_space>with<seq2seq4repair_space>size<seq2seq4repair_space>[{}]" , shard , storeFileMetaData . name ( ) , new ByteSizeValue ( storeFileMetaData . length ( ) ) ) ; sizeMatched += storeFileMetaData . length ( ) ; } else { logger . trace ( "{}:<seq2seq4repair_space>[{}]<seq2seq4repair_space>ignore<seq2seq4repair_space>file<seq2seq4repair_space>since<seq2seq4repair_space>it<seq2seq4repair_space>exists<seq2seq4repair_space>on<seq2seq4repair_space>remote<seq2seq4repair_space>node<seq2seq4repair_space>and<seq2seq4repair_space>on<seq2seq4repair_space>gateway<seq2seq4repair_space>but<seq2seq4repair_space>has<seq2seq4repair_space>different<seq2seq4repair_space>size,<seq2seq4repair_space>remote<seq2seq4repair_space>node<seq2seq4repair_space>[{}],<seq2seq4repair_space>gateway<seq2seq4repair_space>[{}]" , shard , storeFileMetaData . name ( ) , storeFileMetaData . length ( ) , fileInfo . length ( ) ) ; } } else { logger . trace ( "{}:<seq2seq4repair_space>[{}]<seq2seq4repair_space>exists<seq2seq4repair_space>on<seq2seq4repair_space>remote<seq2seq4repair_space>node,<seq2seq4repair_space>does<seq2seq4repair_space>not<seq2seq4repair_space>exists<seq2seq4repair_space>on<seq2seq4repair_space>gateway" , shard , storeFileMetaData . name ( ) ) ; } } if ( sizeMatched > lastSizeMatched ) { lastSizeMatched = sizeMatched ; lastDiscoNodeMatched = discoNode ; lastNodeMatched = node ; logger . trace ( "{}:<seq2seq4repair_space>node<seq2seq4repair_space>elected<seq2seq4repair_space>for<seq2seq4repair_space>pre_allocation<seq2seq4repair_space>[{}],<seq2seq4repair_space>total_size_matched<seq2seq4repair_space>[{}]" , shard , discoNode , new ByteSizeValue ( sizeMatched ) ) ; } else { logger . trace ( "{}:<seq2seq4repair_space>node<seq2seq4repair_space>ignored<seq2seq4repair_space>for<seq2seq4repair_space>pre_allocation<seq2seq4repair_space>[{}],<seq2seq4repair_space>total_size_matched<seq2seq4repair_space>[{}]<seq2seq4repair_space>smaller<seq2seq4repair_space>than<seq2seq4repair_space>last_size_matched<seq2seq4repair_space>[{}]" , shard , discoNode , new ByteSizeValue ( sizeMatched ) , new ByteSizeValue ( lastSizeMatched ) ) ; } continue ; } catch ( Exception e ) { logger . debug ( ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>guess<seq2seq4repair_space>allocation<seq2seq4repair_space>of<seq2seq4repair_space>primary<seq2seq4repair_space>based<seq2seq4repair_space>on<seq2seq4repair_space>gateway<seq2seq4repair_space>for<seq2seq4repair_space>" + shard ) , e ) ; } } if ( ! ( shard . primary ( ) ) ) { MutableShardRouting primaryShard = routingNodes . findPrimaryForReplica ( shard ) ; if ( ( primaryShard != null ) && ( primaryShard . active ( ) ) ) { TransportNodesListShardStoreMetaData . NodeStoreFilesMetaData primaryNodeStoreFileMetaData = nodesStoreFilesMetaData . nodesMap ( ) . get ( primaryShard . currentNodeId ( ) ) ; if ( ( ( primaryNodeStoreFileMetaData != null ) && ( ( primaryNodeStoreFileMetaData . storeFilesMetaData ( ) ) != null ) ) && ( primaryNodeStoreFileMetaData . storeFilesMetaData ( ) . allocated ( ) ) ) { long sizeMatched = 0 ; IndexStore . StoreFilesMetaData primaryStoreFilesMetaData = primaryNodeStoreFileMetaData . storeFilesMetaData ( ) ; for ( StoreFileMetaData storeFileMetaData : storeFilesMetaData ) { if ( ( primaryStoreFilesMetaData . fileExists ( storeFileMetaData . name ( ) ) ) && ( ( primaryStoreFilesMetaData . file ( storeFileMetaData . name ( ) ) . length ( ) ) == ( storeFileMetaData . length ( ) ) ) ) { sizeMatched += storeFileMetaData . length ( ) ; } } if ( sizeMatched > lastSizeMatched ) { lastSizeMatched = sizeMatched ; lastDiscoNodeMatched = discoNode ; lastNodeMatched = node ; } continue ; } } } } if ( lastNodeMatched != null ) { if ( ( nodeAllocations . canAllocate ( shard , lastNodeMatched , routingNodes ) ) == ( NodeAllocation . Decision . THROTTLE ) ) { if ( logger . isTraceEnabled ( ) ) { <START_BUG> logger . trace ( "[{}][{}]:<seq2seq4repair_space>throttling<seq2seq4repair_space>allocation<seq2seq4repair_space>[{}]<seq2seq4repair_space>to<seq2seq4repair_space>[{}]<seq2seq4repair_space>in<seq2seq4repair_space>order<seq2seq4repair_space>to<seq2seq4repair_space>reuse<seq2seq4repair_space>its<seq2seq4repair_space>unallocated<seq2seq4repair_space>persistent<seq2seq4repair_space>store<seq2seq4repair_space>with<seq2seq4repair_space>total_size<seq2seq4repair_space>[{}]" , shard . index ( ) , shard . id ( ) , shard , lastDiscoNodeMatched , new ByteSizeValue ( lastSizeMatched ) ) ; <END_BUG> } unassignedIterator . remove ( ) ; routingNodes . ignoredUnassigned ( ) . add ( shard ) ; } else { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "[{}][{}]:<seq2seq4repair_space>allocating<seq2seq4repair_space>[{}]<seq2seq4repair_space>to<seq2seq4repair_space>[{}]<seq2seq4repair_space>in<seq2seq4repair_space>order<seq2seq4repair_space>to<seq2seq4repair_space>reuse<seq2seq4repair_space>its<seq2seq4repair_space>unallocated<seq2seq4repair_space>persistent<seq2seq4repair_space>store<seq2seq4repair_space>with<seq2seq4repair_space>total_size<seq2seq4repair_space>[{}]" , shard . index ( ) , shard . id ( ) , shard , lastDiscoNodeMatched , new ByteSizeValue ( lastSizeMatched ) ) ; } changed = true ; lastNodeMatched . add ( shard ) ; unassignedIterator . remove ( ) ; } } } return changed ; } @ Override public Decision canAllocate ( ShardRouting shardRouting , RoutingNode node , RoutingNodes routingNodes ) { } }
public . startObject ( ) . field ( "name" , "Times<seq2seq4repair_space>Square" ) . startObject ( "location" ) . field ( "lat" , 40.759011 ) . field ( "lon" , ( - 73.9844722 ) ) . endObject ( ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; client . prepareIndex ( "test" , "type1" , Long . toString ( ( i ++ ) ) ) . setSource ( jsonBuilder ( ) . startObject ( ) . field ( "name" , "Tribeca" ) . startObject ( "location" ) . field ( "lat" , 40.718266 ) . field ( "lon" , ( - 74.007819 ) ) . endObject ( ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; client . prepareIndex ( "test" , "type1" , Long . toString ( ( i ++ ) ) ) . setSource ( jsonBuilder ( ) . startObject ( ) . field ( "name" , "Soho" ) . startObject ( "location" ) . field ( "lat" , 40.7247222 ) . field ( "lon" , ( - 74 ) ) . endObject ( ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; client . prepareIndex ( "test" , "type1" , Long . toString ( ( i ++ ) ) ) . setSource ( jsonBuilder ( ) . startObject ( ) . field ( "name" , "Brooklyn" ) . startObject ( "location" ) . field ( "lat" , 40.65 ) . field ( "lon" , ( - 73.95 ) ) . endObject ( ) . endObject ( ) ) . execute ( ) . actionGet ( ) ; if ( ( i % 10000 ) == 0 ) { System . err . println ( ( "--><seq2seq4repair_space>indexed<seq2seq4repair_space>" + i ) ) ; } } System . err . println ( "Done<seq2seq4repair_space>indexed" ) ; client . admin ( ) . indices ( ) . prepareFlush ( "test" ) . execute ( ) . actionGet ( ) ; client . admin ( ) . indices ( ) . prepareRefresh ( ) . execute ( ) . actionGet ( ) ; } System . err . println ( "--><seq2seq4repair_space>Warming<seq2seq4repair_space>up<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox" ) ; long start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_WARM ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , ARC , "memory" ) ; } long totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Warmup<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>(memory)<seq2seq4repair_space>" + ( totalTime / NUM_WARM ) ) + "ms" ) ) ; System . err . println ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>(memory)" ) ; start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_RUNS ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , ARC , "memory" ) ; } totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>" + ( totalTime / NUM_RUNS ) ) + "ms" ) ) ; System . err . println ( "--><seq2seq4repair_space>Warming<seq2seq4repair_space>up<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>(indexed)" ) ; start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_WARM ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , ARC , "indexed" ) ; } totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Warmup<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>(indexed)<seq2seq4repair_space>" + ( totalTime / NUM_WARM ) ) + "ms" ) ) ; System . err . println ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>(indexed)" ) ; start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_RUNS ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , ARC , "indexed" ) ; } totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>(indexed)<seq2seq4repair_space>" + ( totalTime / NUM_RUNS ) ) + "ms" ) ) ; System . err . println ( "--><seq2seq4repair_space>Warming<seq2seq4repair_space>up<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>no<seq2seq4repair_space>optimize_bbox" ) ; start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_WARM ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , ARC , "none" ) ; } totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Warmup<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>no<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>" + ( totalTime / NUM_WARM ) ) + "ms" ) ) ; System . err . println ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>no<seq2seq4repair_space>optimize_bbox" ) ; start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_RUNS ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , ARC , "none" ) ; } totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(ARC)<seq2seq4repair_space>-<seq2seq4repair_space>no<seq2seq4repair_space>optimize_bbox<seq2seq4repair_space>" + ( totalTime / NUM_RUNS ) ) + "ms" ) ) ; System . err . println ( "--><seq2seq4repair_space>Warming<seq2seq4repair_space>up<seq2seq4repair_space>(PLANE)" ) ; start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_WARM ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , PLANE , "memory" ) ; } totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Warmup<seq2seq4repair_space>(PLANE)<seq2seq4repair_space>" + ( totalTime / NUM_WARM ) ) + "ms" ) ) ; System . err . println ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(PLANE)" ) ; start = System . currentTimeMillis ( ) ; for ( int i = 0 ; i < NUM_RUNS ; i ++ ) { GeoDistanceSearchBenchmark . run ( client , PLANE , "memory" ) ; } totalTime = ( System . currentTimeMillis ( ) ) - start ; System . err . println ( ( ( "--><seq2seq4repair_space>Perf<seq2seq4repair_space>(PLANE)<seq2seq4repair_space>" + ( totalTime / NUM_RUNS ) ) + "ms" ) ) ; node . close ( ) ; } public static void run ( Client client , GeoDistance geoDistance , String optimizeBbox ) { } }
public class TransportCountAction extends TransportBroadcastOperationAction < CountRequest , CountResponse , ShardCountRequest , ShardCountResponse > { @ Inject public TransportCountAction ( Settings settings , ThreadPool threadPool , ClusterService clusterService , TransportService transportService , IndicesService indicesService ) { } @ Override protected String transportAction ( ) { } @ Override protected String transportShardAction ( ) { } @ Override protected CountRequest newRequest ( ) { } @ Override protected ShardCountRequest newShardRequest ( ) { } @ Override protected ShardCountRequest newShardRequest ( ShardRouting shard , CountRequest request ) { } @ Override protected ShardCountResponse newShardResponse ( ) { } @ Override protected GroupShardsIterator shards ( CountRequest request , ClusterState clusterState ) { } @ Override protected CountResponse newResponse ( CountRequest request , AtomicReferenceArray shardsResponses , ClusterState clusterState ) { int successfulShards = 0 ; int failedShards = 0 ; long count = 0 ; List < ShardOperationFailedException > shardFailures = null ; for ( int i = 0 ; i < ( shardsResponses . length ( ) ) ; i ++ ) { Object shardResponse = shardsResponses . get ( i ) ; if ( shardResponse == null ) { failedShards ++ ; } else if ( shardResponse instanceof BroadcastShardOperationFailedException ) { failedShards ++ ; if ( shardFailures == null ) { shardFailures = Lists . newArrayList ( ) ; } shardFailures . add ( new DefaultShardOperationFailedException ( ( ( BroadcastShardOperationFailedException ) ( shardResponse ) ) ) ) ; } else { count += ( ( ShardCountResponse ) ( shardResponse ) ) . count ( ) ; successfulShards ++ ; } } <START_BUG> return new CountResponse ( count , successfulShards , failedShards , shardFailures ) ; <END_BUG> } @ Override protected ShardCountResponse shardOperation ( ShardCountRequest request ) throws ElasticSearchException { } }
public class ParentQuery extends Query implements ScopePhase . CollectorPhase { private final SearchContext searchContext ; private final Query parentQuery ; private final String parentType ; private final Filter childrenFilter ; private final List < String > childTypes ; private final String scope ; private TObjectFloatHashMap < HashedBytesArray > uidToScore ; public ParentQuery ( SearchContext searchContext , Query parentQuery , String parentType , List < String > childTypes , Filter childrenFilter , String scope ) { } private ParentQuery ( ParentQuery unwritten , Query rewrittenParentQuery ) { } @ Override public boolean requiresProcessing ( ) { } @ Override public Collector collector ( ) { } @ Override public void processCollector ( Collector collector ) { } @ Override public String scope ( ) { } @ Override public void clear ( ) { } @ Override public Query query ( ) { } @ Override public String toString ( String field ) { } @ Override public Query rewrite ( IndexReader reader ) throws IOException { } @ Override public void extractTerms ( Set < Term > terms ) { } @ Override public Weight createWeight ( IndexSearcher searcher ) throws IOException { } static class ParentUidCollector extends NoopCollector { final TObjectFloatHashMap < HashedBytesArray > uidToScore ; final SearchContext searchContext ; final String parentType ; Scorer scorer ; IdReaderTypeCache typeCache ; ParentUidCollector ( TObjectFloatHashMap < HashedBytesArray > uidToScore , SearchContext searchContext , String parentType ) { } @ Override public void collect ( int doc ) throws IOException { } @ Override public void setScorer ( Scorer scorer ) throws IOException { } @ Override public void setNextReader ( AtomicReaderContext context ) throws IOException { } } class ChildWeight extends Weight { private final Weight parentWeight ; ChildWeight ( Weight parentWeight ) { } @ Override public Explanation explain ( AtomicReaderContext context , int doc ) throws IOException { } @ Override public Query getQuery ( ) { } @ Override public float getValueForNormalization ( ) throws IOException { } @ Override public void normalize ( float norm , float topLevelBoost ) { } @ Override public Scorer scorer ( AtomicReaderContext context , boolean scoreDocsInOrder , boolean topScorer , Bits acceptDocs ) throws IOException { } } static class ChildScorer extends Scorer { final TObjectFloatHashMap < HashedBytesArray > uidToScore ; final DocIdSetIterator childrenIterator ; final IdReaderTypeCache typeCache ; int currentChildDoc = - 1 ; float currentScore ; ChildScorer ( Weight weight , TObjectFloatHashMap < HashedBytesArray > uidToScore , DocIdSetIterator childrenIterator , IdReaderTypeCache typeCache ) { } @ Override public float score ( ) throws IOException { } @ Override <START_BUG> public float freq ( ) throws IOException { <END_BUG> return 1 ; } @ Override public int docID ( ) { } @ Override public int nextDoc ( ) throws IOException { } @ Override public int advance ( int target ) throws IOException { } } }
@ Override public void run ( ) { if ( ! ( lifecycle . started ( ) ) ) { logger . debug ( "processing<seq2seq4repair_space>[{}]:<seq2seq4repair_space>ignoring,<seq2seq4repair_space>cluster_service<seq2seq4repair_space>not<seq2seq4repair_space>started" , source ) ; return ; } logger . debug ( "processing<seq2seq4repair_space>[{}]:<seq2seq4repair_space>execute" , source ) ; ClusterState previousClusterState = clusterState ; ClusterState newClusterState ; try { newClusterState = updateTask . execute ( previousClusterState ) ; } catch ( Throwable e ) { if ( logger . isTraceEnabled ( ) ) { StringBuilder sb = new StringBuilder ( "failed<seq2seq4repair_space>to<seq2seq4repair_space>execute<seq2seq4repair_space>cluster<seq2seq4repair_space>state<seq2seq4repair_space>update,<seq2seq4repair_space>state:\nversion<seq2seq4repair_space>[" ) . append ( previousClusterState . version ( ) ) . append ( "],<seq2seq4repair_space>source<seq2seq4repair_space>[" ) . append ( source ) . append ( "]\n" ) ; sb . append ( previousClusterState . nodes ( ) . prettyPrint ( ) ) ; sb . append ( previousClusterState . routingTable ( ) . prettyPrint ( ) ) ; sb . append ( previousClusterState . readOnlyRoutingNodes ( ) . prettyPrint ( ) ) ; logger . trace ( sb . toString ( ) , e ) ; } updateTask . onFailure ( source , e ) ; return ; } if ( previousClusterState == newClusterState ) { logger . debug ( "processing<seq2seq4repair_space>[{}]:<seq2seq4repair_space>no<seq2seq4repair_space>change<seq2seq4repair_space>in<seq2seq4repair_space>cluster_state" , source ) ; if ( ( updateTask ) instanceof ProcessedClusterStateUpdateTask ) { ( ( ProcessedClusterStateUpdateTask ) ( updateTask ) ) . clusterStateProcessed ( source , previousClusterState , newClusterState ) ; } return ; } try { if ( newClusterState . nodes ( ) . localNodeMaster ( ) ) { org . elasticsearch . cluster . ClusterState . Builder builder = ClusterState . builder ( ) . state ( newClusterState ) . version ( ( ( newClusterState . version ( ) ) + 1 ) ) ; if ( ( previousClusterState . routingTable ( ) ) != ( newClusterState . routingTable ( ) ) ) { builder . routingTable ( RoutingTable . builder ( ) . routingTable ( newClusterState . routingTable ( ) ) . version ( ( ( newClusterState . routingTable ( ) . version ( ) ) + 1 ) ) ) ; } if ( ( previousClusterState . metaData ( ) ) != ( newClusterState . metaData ( ) ) ) { builder . metaData ( MetaData . builder ( ) . metaData ( newClusterState . metaData ( ) ) . version ( ( ( newClusterState . metaData ( ) . version ( ) ) + 1 ) ) ) ; } newClusterState = builder . build ( ) ; } else { if ( ( previousClusterState . blocks ( ) . hasGlobalBlock ( NO_MASTER_BLOCK ) ) && ( ! ( newClusterState . blocks ( ) . hasGlobalBlock ( NO_MASTER_BLOCK ) ) ) ) { org . elasticsearch . cluster . ClusterState . Builder builder = ClusterState . builder ( ) . state ( newClusterState ) ; builder . routingTable ( RoutingTable . builder ( ) . routingTable ( newClusterState . routingTable ( ) ) ) ; builder . metaData ( MetaData . builder ( ) . metaData ( newClusterState . metaData ( ) ) ) ; newClusterState = builder . build ( ) ; logger . debug ( "got<seq2seq4repair_space>first<seq2seq4repair_space>state<seq2seq4repair_space>from<seq2seq4repair_space>fresh<seq2seq4repair_space>master<seq2seq4repair_space>[{}]" , newClusterState . nodes ( ) . masterNodeId ( ) ) ; } else if ( ( newClusterState . version ( ) ) < ( previousClusterState . version ( ) ) ) { logger . debug ( ( ( ( ( ( ( "got<seq2seq4repair_space>old<seq2seq4repair_space>cluster<seq2seq4repair_space>state<seq2seq4repair_space>[" + ( newClusterState . version ( ) ) ) + "<" ) + ( previousClusterState . version ( ) ) ) + "]<seq2seq4repair_space>from<seq2seq4repair_space>source<seq2seq4repair_space>[" ) + ( source ) ) + "],<seq2seq4repair_space>ignoring" ) ) ; return ; } } if ( logger . isTraceEnabled ( ) ) { StringBuilder sb = new StringBuilder ( "cluster<seq2seq4repair_space>state<seq2seq4repair_space>updated:\nversion<seq2seq4repair_space>[" ) . append ( newClusterState . version ( ) ) . append ( "],<seq2seq4repair_space>source<seq2seq4repair_space>[" ) . append ( source ) . append ( "]\n" ) ; sb . append ( newClusterState . nodes ( ) . prettyPrint ( ) ) ; sb . append ( newClusterState . routingTable ( ) . prettyPrint ( ) ) ; sb . append ( newClusterState . readOnlyRoutingNodes ( ) . prettyPrint ( ) ) ; logger . trace ( sb . toString ( ) ) ; } else if ( logger . isDebugEnabled ( ) ) { logger . debug ( "cluster<seq2seq4repair_space>state<seq2seq4repair_space>updated,<seq2seq4repair_space>version<seq2seq4repair_space>[{}],<seq2seq4repair_space>source<seq2seq4repair_space>[{}]" , newClusterState . version ( ) , source ) ; } ClusterChangedEvent clusterChangedEvent = new ClusterChangedEvent ( source , newClusterState , previousClusterState ) ; final DiscoveryNodes . Delta nodesDelta = clusterChangedEvent . nodesDelta ( ) ; if ( ( nodesDelta . hasChanges ( ) ) && ( logger . isInfoEnabled ( ) ) ) { String summary = nodesDelta . shortSummary ( ) ; if ( ( summary . length ( ) ) > 0 ) { logger . info ( "{},<seq2seq4repair_space>reason:<seq2seq4repair_space>{}" , summary , source ) ; } } for ( DiscoveryNode node : nodesDelta . addedNodes ( ) ) { if ( ! ( nodeRequiresConnection ( node ) ) ) { continue ; } try { transportService . connectToNode ( node ) ; } catch ( Exception e ) { logger . warn ( ( ( "failed<seq2seq4repair_space>to<seq2seq4repair_space>connect<seq2seq4repair_space>to<seq2seq4repair_space>node<seq2seq4repair_space>[" + node ) + "]" ) , e ) ; } } if ( newClusterState . nodes ( ) . localNodeMaster ( ) ) { logger . debug ( "Publishing<seq2seq4repair_space>cluster<seq2seq4repair_space>state<seq2seq4repair_space>version<seq2seq4repair_space>{}" , newClusterState . version ( ) ) ; discoveryService . publish ( newClusterState ) ; } <START_BUG> logger . debug ( "Updating<seq2seq4repair_space>cluster<seq2seq4repair_space>state<seq2seq4repair_space>version<seq2seq4repair_space>{}:<seq2seq4repair_space>{}" , newClusterState . version ( ) , newClusterState ) ; <END_BUG>
class ShardCountRequest extends BroadcastShardOperationRequest { private float minScore ; private byte [ ] querySource ; private int querySourceOffset ; private int querySourceLength ; private String [ ] types = Strings . EMPTY_ARRAY ; @ Nullable private String [ ] filteringAliases ; ShardCountRequest ( ) { } public ShardCountRequest ( String index , int shardId , @ Nullable String [ ] filteringAliases , CountRequest request ) { } public float minScore ( ) { } public byte [ ] querySource ( ) { } public int querySourceOffset ( ) { } public int querySourceLength ( ) { } public String [ ] types ( ) { } public String [ ] filteringAliases ( ) { } @ Override public void readFrom ( StreamInput in ) throws IOException { super . readFrom ( in ) ; minScore = in . readFloat ( ) ; <START_BUG> BytesHolder bytes = in . readBytesHolder ( ) ; <END_BUG> querySource = bytes . bytes ( ) ; querySourceOffset = bytes . offset ( ) ; querySourceLength = bytes . length ( ) ; int typesSize = in . readVInt ( ) ; if ( typesSize > 0 ) { types = new String [ typesSize ] ; for ( int i = 0 ; i < typesSize ; i ++ ) { types [ i ] = in . readUTF ( ) ; } } int aliasesSize = in . readVInt ( ) ; if ( aliasesSize > 0 ) { filteringAliases = new String [ aliasesSize ] ; for ( int i = 0 ; i < aliasesSize ; i ++ ) { filteringAliases [ i ] = in . readUTF ( ) ; } } } @ Override public void writeTo ( StreamOutput out ) throws IOException { } }
@ Test public void getFieldsWithDifferentTypes ( ) throws Exception { } @ Test public void testGetDocWithMultivaluedFields ( ) throws Exception { } @ Test public void testThatGetFromTranslogShouldWorkWithExclude ( ) throws Exception { } @ Test public void testThatGetFromTranslogShouldWorkWithInclude ( ) throws Exception { } @ SuppressWarnings ( "unchecked" ) @ Test public void testThatGetFromTranslogShouldWorkWithIncludeExcludeAndFields ( ) throws Exception { } @ Test public void testGetWithVersion ( ) { } @ Test public void testMultiGetWithVersion ( ) throws Exception { } @ Test public void testGetFields_metaData ( ) throws Exception { } @ Test public void testGetFields_nonLeafField ( ) throws Exception { } @ Test public void testGetFields_complexField ( ) throws Exception { client ( ) . admin ( ) . indices ( ) . prepareCreate ( "my-index" ) . setSettings ( ImmutableSettings . settingsBuilder ( ) . put ( "index.refresh_interval" , ( - 1 ) ) ) . addMapping ( "my-type2" , jsonBuilder ( ) . startObject ( ) . startObject ( "my-type2" ) . startObject ( "properties" ) . startObject ( "field1" ) . field ( "type" , "object" ) . startObject ( "field2" ) . field ( "type" , "object" ) . startObject ( "field3" ) . field ( "type" , "object" ) . startObject ( "field4" ) . field ( "type" , "string" ) . field ( "store" , "yes" ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) ) . get ( ) ; BytesReference source = jsonBuilder ( ) . startObject ( ) . startArray ( "field1" ) . startObject ( ) . startObject ( "field2" ) . startArray ( "field3" ) . startObject ( ) . field ( "field4" , "value1" ) . endObject ( ) . endArray ( ) . endObject ( ) . endObject ( ) . startObject ( ) . startObject ( "field2" ) . startArray ( "field3" ) . startObject ( ) . field ( "field4" , "value2" ) . endObject ( ) . endArray ( ) . endObject ( ) . endObject ( ) . endArray ( ) . endObject ( ) . bytes ( ) ; client ( ) . prepareIndex ( "my-index" , "my-type1" , "1" ) . setSource ( source ) . get ( ) ; client ( ) . prepareIndex ( "my-index" , "my-type2" , "1" ) . setSource ( source ) . get ( ) ; String field = "field1.field2.field3.field4" ; GetResponse getResponse = client ( ) . prepareGet ( "my-index" , "my-type1" , "1" ) . setFields ( field ) . get ( ) ; assertThat ( getResponse . isExists ( ) , equalTo ( true ) ) ; assertThat ( getResponse . getField ( field ) . isMetadataField ( ) , equalTo ( false ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . size ( ) , equalTo ( 2 ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 0 ) . toString ( ) , equalTo ( "value1" ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 1 ) . toString ( ) , equalTo ( "value2" ) ) ; getResponse = client ( ) . prepareGet ( "my-index" , "my-type2" , "1" ) . setFields ( field ) . get ( ) ; assertThat ( getResponse . isExists ( ) , equalTo ( true ) ) ; assertThat ( getResponse . getField ( field ) . isMetadataField ( ) , equalTo ( false ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . size ( ) , equalTo ( 2 ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 0 ) . toString ( ) , equalTo ( "value1" ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 1 ) . toString ( ) , equalTo ( "value2" ) ) ; <START_BUG> FlushResponse flushResponse = client ( ) . admin ( ) . indices ( ) . prepareFlush ( "my-index" ) . get ( ) ; <END_BUG> assertThat ( flushResponse . getSuccessfulShards ( ) , greaterThanOrEqualTo ( 1 ) ) ; getResponse = client ( ) . prepareGet ( "my-index" , "my-type1" , "1" ) . setFields ( field ) . get ( ) ; assertThat ( getResponse . isExists ( ) , equalTo ( true ) ) ; assertThat ( getResponse . getField ( field ) . isMetadataField ( ) , equalTo ( false ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . size ( ) , equalTo ( 2 ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 0 ) . toString ( ) , equalTo ( "value1" ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 1 ) . toString ( ) , equalTo ( "value2" ) ) ; getResponse = client ( ) . prepareGet ( "my-index" , "my-type2" , "1" ) . setFields ( field ) . get ( ) ; assertThat ( getResponse . isExists ( ) , equalTo ( true ) ) ; assertThat ( getResponse . getField ( field ) . isMetadataField ( ) , equalTo ( false ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . size ( ) , equalTo ( 2 ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 0 ) . toString ( ) , equalTo ( "value1" ) ) ; assertThat ( getResponse . getField ( field ) . getValues ( ) . get ( 1 ) . toString ( ) , equalTo ( "value2" ) ) ;
public class LoginActivity extends RoboSherlockAccountAuthenticatorActivity { public static final String PARAM_AUTHTOKEN_TYPE = "authtokenType" ; private static final String PARAM_CONFIRMCREDENTIALS = "confirmCredentials" ; private static final String PARAM_USERNAME = "username" ; private static final String TAG = "LoginActivity" ; private static final long SYNC_PERIOD = ( 8L * 60L ) * 60L ; private static void configureSyncFor ( Account account ) { } private static class AccountLoader extends AuthenticatedUserTask < List < User > > { @ Inject private AccountDataManager cache ; protected AccountLoader ( Context context ) { } @ Override protected List < User > run ( ) throws Exception { } } private AccountManager accountManager ; @ InjectView ( id . et_login ) private AutoCompleteTextView loginText ; @ InjectView ( id . et_password ) private EditText passwordText ; private RoboAsyncTask < User > authenticationTask ; private String authToken ; private String authTokenType ; private MenuItem loginItem ; private Boolean confirmCredentials = false ; private String password ; protected boolean requestNewAccount = false ; private String username ; @ Override public void onCreate ( Bundle savedInstanceState ) { } @ Override protected void onResume ( ) { } private boolean loginEnabled ( ) { <START_BUG> return ( ! ( TextUtils . isEmpty ( loginText . getText ( ) ) ) ) && ( ! ( TextUtils . isDigitsOnly ( passwordText . getText ( ) ) ) ) ; <END_BUG> } private void updateEnablement ( ) { } @ Override public void startActivity ( Intent intent ) { } public void handleLogin ( ) { } protected void finishConfirmCredentials ( boolean result ) { } protected void finishLogin ( ) { } public void onAuthenticationResult ( boolean result ) { } public boolean onOptionsItemSelected ( MenuItem item ) { } @ Override public boolean onCreateOptionsMenu ( Menu optionMenu ) { } private List < String > getEmailAddresses ( ) { } }
public class TransportMoreLikeThisAction extends HandledTransportAction < MoreLikeThisRequest , SearchResponse > { private final TransportSearchAction searchAction ; private final TransportGetAction getAction ; private final IndicesService indicesService ; private final ClusterService clusterService ; private final TransportService transportService ; @ Inject public TransportMoreLikeThisAction ( Settings settings , ThreadPool threadPool , TransportSearchAction searchAction , TransportGetAction getAction , ClusterService clusterService , IndicesService indicesService , TransportService transportService , ActionFilters actionFilters ) { } @ Override public MoreLikeThisRequest newRequestInstance ( ) { } @ Override protected void doExecute ( final MoreLikeThisRequest request , final ActionListener < SearchResponse > listener ) { } private void redirect ( MoreLikeThisRequest request , String concreteIndex , final ActionListener < SearchResponse > listener , ClusterState clusterState ) { } private void parseSource ( GetResponse getResponse , final BoolQueryBuilder boolBuilder , DocumentMapper docMapper , final Set < String > fields , final MoreLikeThisRequest request ) { } private Object convertField ( Field field ) { } private void addMoreLikeThis ( MoreLikeThisRequest request , BoolQueryBuilder boolBuilder , FieldMapper fieldMapper , Field field , boolean failOnUnsupportedField ) { } private void addMoreLikeThis ( MoreLikeThisRequest request , BoolQueryBuilder boolBuilder , String fieldName , String likeText , boolean failOnUnsupportedField ) { <START_BUG> MoreLikeThisFieldQueryBuilder mlt = moreLikeThisFieldQuery ( fieldName ) . likeText ( likeText ) . percentTermsToMatch ( request . percentTermsToMatch ( ) ) . boostTerms ( request . boostTerms ( ) ) . minDocFreq ( request . minDocFreq ( ) ) . maxDocFreq ( request . maxDocFreq ( ) ) . minWordLength ( request . minWordLength ( ) ) . maxWordLen ( request . maxWordLength ( ) ) . minTermFreq ( request . minTermFreq ( ) ) . maxQueryTerms ( request . maxQueryTerms ( ) ) . stopWords ( request . stopWords ( ) ) . failOnUnsupportedField ( failOnUnsupportedField ) ; <END_BUG> boolBuilder . should ( mlt ) ; } }
public class Texture implements Disposable { private static boolean enforcePotImages = true ; private static boolean useHWMipMap = true ; private static AssetManager assetManager ; private static final Map < Application , List < Texture > > managedTextures = new HashMap < Application , List < Texture > > ( ) ; public enum TextureFilter { Nearest ( GL10 . GL_NEAREST ) , Linear ( GL10 . GL_LINEAR ) , MipMap ( GL10 . GL_LINEAR_MIPMAP_LINEAR ) , MipMapNearestNearest ( GL10 . GL_NEAREST_MIPMAP_NEAREST ) , MipMapLinearNearest ( GL10 . GL_LINEAR_MIPMAP_NEAREST ) , MipMapNearestLinear ( GL10 . GL_NEAREST_MIPMAP_LINEAR ) , MipMapLinearLinear ( GL10 . GL_LINEAR_MIPMAP_LINEAR ) ; final int glEnum ; TextureFilter ( int glEnum ) { } public boolean isMipMap ( ) { } public int getGLEnum ( ) { } } public enum TextureWrap { ClampToEdge ( GL10 . GL_CLAMP_TO_EDGE ) , Repeat ( GL10 . GL_REPEAT ) ; final int glEnum ; TextureWrap ( int glEnum ) { } public int getGLEnum ( ) { } } private static final IntBuffer buffer = BufferUtils . newIntBuffer ( 1 ) ; Texture . TextureFilter minFilter = Texture . TextureFilter . Nearest ; Texture . TextureFilter magFilter = Texture . TextureFilter . Nearest ; Texture . TextureWrap uWrap = Texture . TextureWrap . ClampToEdge ; Texture . TextureWrap vWrap = Texture . TextureWrap . ClampToEdge ; int glHandle ; TextureData data ; public Texture ( String internalPath ) { } public Texture ( FileHandle file ) { } public Texture ( FileHandle file , boolean useMipMaps ) { } public Texture ( FileHandle file , Format format , boolean useMipMaps ) { } public Texture ( Pixmap pixmap ) { } public Texture ( Pixmap pixmap , boolean useMipMaps ) { } public Texture ( Pixmap pixmap , Format format , boolean useMipMaps ) { } public Texture ( int width , int height , Format format ) { } public Texture ( TextureData data ) { } public static int createGLHandle ( ) { } public void load ( TextureData data ) { } private void uploadImageData ( Pixmap pixmap ) { if ( ( ( Texture . enforcePotImages ) && ( ( Gdx . gl20 ) == null ) ) && ( ( ! ( MathUtils . isPowerOfTwo ( data . getWidth ( ) ) ) ) || ( ! ( MathUtils . isPowerOfTwo ( data . getHeight ( ) ) ) ) ) ) throw new GdxRuntimeException ( "texture<seq2seq4repair_space>width<seq2seq4repair_space>and<seq2seq4repair_space>height<seq2seq4repair_space>must<seq2seq4repair_space>be<seq2seq4repair_space>powers<seq2seq4repair_space>of<seq2seq4repair_space>two" ) ; boolean disposePixmap = false ; if ( ( data . getFormat ( ) ) != ( pixmap . getFormat ( ) ) ) { Pixmap tmp = new Pixmap ( pixmap . getWidth ( ) , pixmap . getHeight ( ) , data . getFormat ( ) ) ; Blending blend = Pixmap . getBlending ( ) ; Pixmap . setBlending ( None ) ; tmp . drawPixmap ( pixmap , 0 , 0 , 0 , 0 , pixmap . getWidth ( ) , pixmap . getHeight ( ) ) ; Pixmap . setBlending ( blend ) ; pixmap = tmp ; disposePixmap = true ; } gl . glBindTexture ( GL_TEXTURE_2D , glHandle ) ; if ( data . useMipMaps ( ) ) { <START_BUG> MipMapGenerator . generateMipMap ( pixmap , this , disposePixmap ) ; <END_BUG> } else { gl . glTexImage2D ( GL_TEXTURE_2D , 0 , pixmap . getGLInternalFormat ( ) , pixmap . getWidth ( ) , pixmap . getHeight ( ) , 0 , pixmap . getGLFormat ( ) , pixmap . getGLType ( ) , pixmap . getPixels ( ) ) ; if ( disposePixmap ) pixmap . dispose ( ) ; } } private void reload ( ) { } public void bind ( ) { } public void bind ( int unit ) { } public void draw ( Pixmap pixmap , int x , int y ) { } public int getWidth ( ) { } public int getHeight ( ) { } public Texture . TextureFilter getMinFilter ( ) { } public Texture . TextureFilter getMagFilter ( ) { } public Texture . TextureWrap getUWrap ( ) { } public Texture . TextureWrap getVWrap ( ) { } public TextureData getTextureData ( ) { } public boolean isManaged ( ) { } public int getTextureObjectHandle ( ) { } public void setWrap ( Texture . TextureWrap u , Texture . TextureWrap v ) { } public void setFilter ( Texture . TextureFilter minFilter , Texture . TextureFilter magFilter ) { } public void dispose ( ) { } public static void setEnforcePotImages ( boolean enforcePotImages ) { } private static void addManagedTexture ( Application app , Texture texture ) { } public static void clearAllTextures ( Application app ) { } public static void invalidateAllTextures ( Application app ) { } public static void setAssetManager ( AssetManager manager ) { } public static String getManagedStatus ( ) { } }
public class GeoPointDoubleArrayIndexFieldData extends AbstractIndexFieldData < GeoPointDoubleArrayAtomicFieldData > implements IndexGeoPointFieldData < GeoPointDoubleArrayAtomicFieldData > { public static class Builder implements IndexFieldData . Builder { @ Override public IndexFieldData build ( Index index , @ IndexSettings Settings indexSettings , FieldMapper . Names fieldNames , FieldDataType type , IndexFieldDataCache cache ) { } } public GeoPointDoubleArrayIndexFieldData ( Index index , @ IndexSettings Settings indexSettings , FieldMapper . Names fieldNames , FieldDataType fieldDataType , IndexFieldDataCache cache ) { } @ Override public boolean valuesOrdered ( ) { } @ Override public GeoPointDoubleArrayAtomicFieldData load ( AtomicReaderContext context ) { } @ Override public GeoPointDoubleArrayAtomicFieldData loadDirect ( AtomicReaderContext context ) throws Exception { AtomicReader reader = context . reader ( ) ; Terms terms = reader . terms ( getFieldNames ( ) . indexName ( ) ) ; if ( terms == null ) { return GeoPointDoubleArrayAtomicFieldData . EMPTY ; } final TDoubleArrayList lat = new TDoubleArrayList ( ) ; final TDoubleArrayList lon = new TDoubleArrayList ( ) ; lat . add ( 0 ) ; lon . add ( 0 ) ; OrdinalsBuilder builder = new OrdinalsBuilder ( terms , reader . maxDoc ( ) ) ; final CharsRef spare = new CharsRef ( ) ; try { BytesRefIterator iter = builder . buildFromTerms ( terms . iterator ( null ) , reader . getLiveDocs ( ) ) ; BytesRef term ; while ( ( term = iter . next ( ) ) != null ) { UnicodeUtil . UTF8toUTF16 ( term , spare ) ; boolean parsed = false ; for ( int i = spare . offset ; i < ( spare . length ) ; i ++ ) { if ( ( spare . chars [ i ] ) == ',' ) { lat . add ( Double . parseDouble ( new String ( spare . chars , spare . offset , ( i - ( spare . offset ) ) ) ) ) ; lon . add ( Double . parseDouble ( new String ( spare . chars , ( ( spare . offset ) + ( i + 1 ) ) , ( ( spare . length ) - ( ( i + 1 ) - ( spare . offset ) ) ) ) ) ) ; parsed = true ; break ; } } assert parsed ; } Ordinals build = builder . build ( fieldDataType . getSettings ( ) ) ; <START_BUG> if ( ! ( build . isMultiValued ( ) ) ) { <END_BUG> Docs ordinals = build . ordinals ( ) ; double [ ] sLat = new double [ reader . maxDoc ( ) ] ; double [ ] sLon = new double [ reader . maxDoc ( ) ] ; for ( int i = 0 ; i < ( sLat . length ) ; i ++ ) { int nativeOrdinal = ordinals . getOrd ( i ) ; sLat [ i ] = lat . get ( nativeOrdinal ) ; sLon [ i ] = lon . get ( nativeOrdinal ) ; } FixedBitSet set = builder . buildDocsWithValuesSet ( ) ; if ( set == null ) { return new GeoPointDoubleArrayAtomicFieldData . Single ( sLon , sLat , reader . maxDoc ( ) ) ; } else { return new GeoPointDoubleArrayAtomicFieldData . SingleFixedSet ( sLon , sLat , reader . maxDoc ( ) , set ) ; } } else { return new GeoPointDoubleArrayAtomicFieldData . WithOrdinals ( lon . toArray ( new double [ lon . size ( ) ] ) , lat . toArray ( new double [ lat . size ( ) ] ) , reader . maxDoc ( ) , build ) ; } } finally { builder . close ( ) ; } } @ Override public XFieldComparatorSource comparatorSource ( @ Nullable Object missingValue , SortMode sortMode ) { } }
public class SuperJumperDesktop { public static void main ( String [ ] argv ) { <START_BUG> new com . badlogic . gdx . backends . lwjgl . LwjglApplication ( new SuperJumper ( ) , "Super<seq2seq4repair_space>Jumper" , 320 , 480 , false ) ; <END_BUG> } }
public class BenchmarkNettyClient { public static void main ( String [ ] args ) { final SizeValue payloadSize = new SizeValue ( 100 , SizeUnit . BYTES ) ; final int NUMBER_OF_CLIENTS = 1 ; final int NUMBER_OF_ITERATIONS = 500000 ; final byte [ ] payload = new byte [ ( ( int ) ( payloadSize . bytes ( ) ) ) ] ; final AtomicLong idGenerator = new AtomicLong ( ) ; final boolean waitForRequest = false ; final boolean spawn = true ; Settings settings = ImmutableSettings . settingsBuilder ( ) . put ( "network.server" , false ) . put ( "transport.netty.connectionsPerNode" , 5 ) . build ( ) ; final ThreadPool threadPool = new CachedThreadPool ( ) ; <START_BUG> final TransportService transportService = new TransportService ( new org . elasticsearch . transport . netty . NettyTransport ( settings , threadPool ) ) . start ( ) ; <END_BUG> final Node node = new Node ( "server" , new InetSocketTransportAddress ( "localhost" , 9999 ) ) ; transportService . nodesAdded ( Lists . newArrayList ( node ) ) ; Thread [ ] clients = new Thread [ NUMBER_OF_CLIENTS ] ; final CountDownLatch latch = new CountDownLatch ( ( NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS ) ) ; for ( int i = 0 ; i < NUMBER_OF_CLIENTS ; i ++ ) { clients [ i ] = new Thread ( new Runnable ( ) { @ Override public void run ( ) { for ( int j = 0 ; j < NUMBER_OF_ITERATIONS ; j ++ ) { final long id = idGenerator . incrementAndGet ( ) ; BenchmarkMessage message = new BenchmarkMessage ( id , payload ) ; BaseTransportResponseHandler < BenchmarkMessage > handler = new BaseTransportResponseHandler < BenchmarkMessage > ( ) { @ Override public BenchmarkMessage newInstance ( ) { return new BenchmarkMessage ( ) ; } @ Override public void handleResponse ( BenchmarkMessage response ) { if ( ( response . id ) != id ) { System . out . println ( ( ( ( ( "NO<seq2seq4repair_space>ID<seq2seq4repair_space>MATCH<seq2seq4repair_space>[" + ( response . id ) ) + "]<seq2seq4repair_space>and<seq2seq4repair_space>[" ) + id ) + "]" ) ) ; } latch . countDown ( ) ; } @ Override public void handleException ( RemoteTransportException exp ) { exp . printStackTrace ( ) ; latch . countDown ( ) ; } @ Override public boolean spawn ( ) { return spawn ; } } ; if ( waitForRequest ) { transportService . submitRequest ( node , "benchmark" , message , handler ) . txGet ( ) ; } else { transportService . sendRequest ( node , "benchmark" , message , handler ) ; } } } } ) ; } StopWatch stopWatch = new StopWatch ( ) . start ( ) ; for ( int i = 0 ; i < NUMBER_OF_CLIENTS ; i ++ ) { clients [ i ] . start ( ) ; } try { latch . await ( ) ; } catch ( InterruptedException e ) { e . printStackTrace ( ) ; } stopWatch . stop ( ) ; System . out . println ( ( ( ( ( ( ( ( ( ( "Ran<seq2seq4repair_space>[" + NUMBER_OF_CLIENTS ) + "],<seq2seq4repair_space>each<seq2seq4repair_space>with<seq2seq4repair_space>[" ) + NUMBER_OF_ITERATIONS ) + "]<seq2seq4repair_space>iterations,<seq2seq4repair_space>payload<seq2seq4repair_space>[" ) + payloadSize ) + "]:<seq2seq4repair_space>took<seq2seq4repair_space>[" ) + ( stopWatch . totalTime ( ) ) ) + "],<seq2seq4repair_space>TPS:<seq2seq4repair_space>" ) + ( ( NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS ) / ( stopWatch . totalTime ( ) . secondsFrac ( ) ) ) ) ) ; transportService . close ( ) ; threadPool . shutdownNow ( ) ; } }
public final class DefaultTextureBinder implements TextureBinder { public static final int ROUNDROBIN = 0 ; public static final int WEIGHTED = 1 ; public static final int MAX_GLES_UNITS = 4 ; private final int offset ; private final int count ; private final int reuseWeight ; private final TextureDescriptor [ ] textures ; private final int [ ] weights ; private final int method ; private boolean reused ; private int reuseCount = 0 ; private int bindCount = 0 ; public DefaultTextureBinder ( final int method ) { } public DefaultTextureBinder ( final int method , final int offset ) { } public DefaultTextureBinder ( final int method , final int offset , final int count ) { } public DefaultTextureBinder ( final int method , final int offset , final int count , final int reuseWeight ) { } private static int getMaxTextureUnits ( ) { IntBuffer buffer = BufferUtils . newIntBuffer ( 16 ) ; <START_BUG> gl . glGetIntegerv ( GL_MAX_TEXTURE_IMAGE_UNITS , buffer ) ; <END_BUG> return buffer . get ( 0 ) ; } @ Override public void begin ( ) { } @ Override public void end ( ) { } @ Override public final int bind ( final TextureDescriptor textureDesc ) { } private final int bindTexture ( final TextureDescriptor textureDesc , final boolean rebind ) { } private int currentTexture = 0 ; private final int bindTextureRoundRobin ( final Texture texture ) { } private final int bindTextureWeighted ( final Texture texture ) { } @ Override public final int getBindCount ( ) { } @ Override public final int getReuseCount ( ) { } @ Override public final void resetCounts ( ) { } }
public interface ShardRouting extends Serializable , Streamable { ShardId shardId ( ) { } String index ( ) { } String getIndex ( ) { } int id ( ) { } int getId ( ) { } ShardRoutingState state ( ) { } boolean unassigned ( ) { } boolean initializing ( ) { } boolean started ( ) { } boolean relocating ( ) { } boolean active ( ) { } boolean assignedToNode ( ) { } String currentNodeId ( ) { } String relocatingNodeId ( ) { } boolean primary ( ) { } String shortSummary ( ) { } <START_BUG> ShardsIterator shardsIt ( ) ; <END_BUG> void writeToThin ( StreamOutput out ) throws IOException { } void readFromThin ( StreamInput in ) throws IOException , ClassNotFoundException { } }
public class LwjglDebugStarter { public static void main ( String [ ] argv ) { new SharedLibraryLoader ( "../../extensions/gdx-audio/libs/gdx-audio-natives.jar" ) . load ( "gdx-audio" ) ; new SharedLibraryLoader ( "../../extensions/gdx-image/libs/gdx-image-natives.jar" ) . load ( "gdx-image" ) ; new SharedLibraryLoader ( "../../extensions/gdx-freetype/libs/gdx-freetype-natives.jar" ) . load ( "gdx-freetype" ) ; new SharedLibraryLoader ( "../../extensions/gdx-controllers/gdx-controllers-desktop/libs/gdx-controllers-desktop-natives.jar" ) . load ( "gdx-controllers-desktop" ) ; new SharedLibraryLoader ( "../../gdx/libs/gdx-natives.jar" ) . load ( "gdx" ) ; <START_BUG> GdxTest test = new GleedTest ( ) ; <END_BUG> LwjglApplicationConfiguration config = new LwjglApplicationConfiguration ( ) ; config . useGL20 = test . needsGL20 ( ) ; new com . badlogic . gdx . backends . lwjgl . LwjglApplication ( test , config ) ; } }
public class RestGetAction extends BaseRestHandler { private static final Pattern fieldsPattern ; @ Inject public RestGetAction ( Settings settings , Client client , RestController controller ) { } @ Override public void handleRequest ( final RestRequest request , final RestChannel channel ) { final GetRequest getRequest = new GetRequest ( request . param ( "index" ) , request . param ( "type" ) , request . param ( "id" ) ) ; getRequest . listenerThreaded ( false ) ; <START_BUG> getRequest . threadedOperation ( true ) ; <END_BUG> List < String > fields = request . params ( "field" ) ; String sField = request . param ( "fields" ) ; if ( sField != null ) { String [ ] sFields = RestGetAction . fieldsPattern . split ( sField ) ; if ( sFields != null ) { if ( fields == null ) { fields = Lists . newArrayListWithExpectedSize ( sField . length ( ) ) ; } for ( String field : sFields ) { fields . add ( field ) ; } } } if ( fields != null ) { getRequest . fields ( fields . toArray ( new String [ fields . size ( ) ] ) ) ; } client . get ( getRequest , new org . elasticsearch . action . ActionListener < GetResponse > ( ) { @ Override public void onResponse ( GetResponse response ) { try { if ( ! ( response . exists ( ) ) ) { JsonBuilder builder = restJsonBuilder ( request ) ; builder . startObject ( ) ; builder . field ( "_index" , response . index ( ) ) ; builder . field ( "_type" , response . type ( ) ) ; builder . field ( "_id" , response . id ( ) ) ; builder . endObject ( ) ; channel . sendResponse ( new JsonRestResponse ( request , NOT_FOUND , builder ) ) ; } else { JsonBuilder builder = restJsonBuilder ( request ) ; builder . startObject ( ) ; builder . field ( "_index" , response . index ( ) ) ; builder . field ( "_type" , response . type ( ) ) ; builder . field ( "_id" , response . id ( ) ) ; if ( ( response . source ( ) ) != null ) { builder . raw ( ",<seq2seq4repair_space>\"_source\"<seq2seq4repair_space>:<seq2seq4repair_space>" ) ; builder . raw ( response . source ( ) ) ; } if ( ( ( response . fields ( ) ) != null ) && ( ! ( response . fields ( ) . isEmpty ( ) ) ) ) { builder . startObject ( "fields" ) ; for ( GetField field : response . fields ( ) . values ( ) ) { if ( field . values ( ) . isEmpty ( ) ) { continue ; } if ( ( field . values ( ) . size ( ) ) == 1 ) { builder . field ( field . name ( ) , field . values ( ) . get ( 0 ) ) ; } else { builder . field ( field . name ( ) ) ; builder . startArray ( ) ; for ( Object value : field . values ( ) ) { builder . value ( value ) ; } builder . endArray ( ) ; } } builder . endObject ( ) ; } builder . endObject ( ) ; channel . sendResponse ( new JsonRestResponse ( request , OK , builder ) ) ; } } catch ( Exception e ) { onFailure ( e ) ; } } @ Override public void onFailure ( Throwable e ) { try { channel . sendResponse ( new JsonThrowableRestResponse ( request , e ) ) ; } catch ( IOException e1 ) { logger . error ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>failure<seq2seq4repair_space>response" , e1 ) ; } } } ) ; } }
public class TransportClusterStateAction extends TransportMasterNodeOperationAction < ClusterStateRequest , ClusterStateResponse > { private final ClusterName clusterName ; @ Inject public TransportClusterStateAction ( Settings settings , TransportService transportService , ClusterService clusterService , ThreadPool threadPool , ClusterName clusterName ) { } @ Override protected String transportAction ( ) { } @ Override protected ClusterStateRequest newRequest ( ) { } @ Override protected ClusterStateResponse newResponse ( ) { } @ Override protected boolean localExecute ( ClusterStateRequest request ) { } @ Override protected ClusterStateResponse masterOperation ( ClusterStateRequest request , ClusterState state ) throws ElasticSearchException { ClusterState currentState = clusterService . state ( ) ; ClusterState . Builder builder = newClusterStateBuilder ( ) ; if ( ! ( request . filterNodes ( ) ) ) { builder . nodes ( currentState . nodes ( ) ) ; } if ( ! ( request . filterRoutingTable ( ) ) ) { builder . routingTable ( currentState . routingTable ( ) ) ; builder . allocationExplanation ( currentState . allocationExplanation ( ) ) ; } if ( ! ( request . filterBlocks ( ) ) ) { builder . blocks ( currentState . blocks ( ) ) ; } if ( ! ( request . filterMetaData ( ) ) ) { MetaData . Builder mdBuilder = newMetaDataBuilder ( ) ; if ( ( ( request . filteredIndices ( ) . length ) == 0 ) && ( ( request . filteredIndexTemplates ( ) . length ) == 0 ) ) { mdBuilder . metaData ( currentState . metaData ( ) ) ; } if ( ( request . filteredIndices ( ) . length ) > 0 ) { <START_BUG> String [ ] indices = currentState . metaData ( ) . concreteIndices ( request . filteredIndices ( ) , true ) ; <END_BUG> for ( String filteredIndex : indices ) { IndexMetaData indexMetaData = currentState . metaData ( ) . index ( filteredIndex ) ; if ( indexMetaData != null ) { mdBuilder . put ( indexMetaData ) ; } } } if ( ( request . filteredIndexTemplates ( ) . length ) > 0 ) { for ( String templateName : request . filteredIndexTemplates ( ) ) { IndexTemplateMetaData indexTemplateMetaData = currentState . metaData ( ) . templates ( ) . get ( templateName ) ; if ( indexTemplateMetaData != null ) { mdBuilder . put ( indexTemplateMetaData ) ; } } } builder . metaData ( mdBuilder ) ; } return new ClusterStateResponse ( clusterName , builder . build ( ) ) ; } }
public class ScriptHistogramFacetExecutor extends FacetExecutor { final SearchScript keyScript ; final SearchScript valueScript ; final long interval ; private final ComparatorType comparatorType ; final Recycler . V < LongObjectOpenHashMap < InternalFullHistogramFacet . FullEntry > > entries ; public ScriptHistogramFacetExecutor ( String scriptLang , String keyScript , String valueScript , Map < String , Object > params , long interval , HistogramFacet . ComparatorType comparatorType , SearchContext context ) { } @ Override public ScriptHistogramFacetExecutor . Collector collector ( ) { } @ Override public InternalFacet buildFacet ( String facetName ) { List < InternalFullHistogramFacet . FullEntry > entries1 = new java . util . ArrayList ( entries . v ( ) . size ( ) ) ; final boolean [ ] states = entries . v ( ) . allocated ; final Object [ ] values = entries . v ( ) . values ; for ( int i = 0 ; i < ( states . length ) ; i ++ ) { if ( states [ i ] ) { InternalFullHistogramFacet . FullEntry value = ( ( InternalFullHistogramFacet . FullEntry ) ( values [ i ] ) ) ; entries1 . add ( value ) ; } } <START_BUG> entries . release ( ) ; <END_BUG> return new InternalFullHistogramFacet ( facetName , comparatorType , entries1 ) ; } public static long bucket ( double value , long interval ) { } class Collector extends FacetExecutor . Collector { final LongObjectOpenHashMap < InternalFullHistogramFacet . FullEntry > entries ; Collector ( LongObjectOpenHashMap < InternalFullHistogramFacet . FullEntry > entries ) { } @ Override public void setScorer ( Scorer scorer ) throws IOException { } @ Override public void setNextReader ( AtomicReaderContext context ) throws IOException { } @ Override public void collect ( int doc ) throws IOException { } @ Override public void postCollection ( ) { } } }
public class RestMainAction extends BaseRestHandler { private final JsonNode rootNode ; private final int quotesSize ; @ Inject public RestMainAction ( Settings settings , Client client , RestController controller ) { } @ Override public void handleRequest ( RestRequest request , RestChannel channel ) { try { JsonBuilder builder = RestJsonBuilder . restJsonBuilder ( request ) . prettyPrint ( ) ; builder . startObject ( ) ; builder . field ( "ok" , true ) ; if ( ( settings . get ( "name" ) ) != null ) { builder . field ( "name" , settings . get ( "name" ) ) ; } <START_BUG> builder . startObject ( "version" ) . field ( "number" , Version . number ( ) ) . field ( "date" , Version . date ( ) ) . field ( "snapshotBuild" , Version . snapshotBuild ( ) ) . endObject ( ) ; <END_BUG> builder . field ( "version" , Version . number ( ) ) ; builder . field ( "tagline" , "You<seq2seq4repair_space>Know,<seq2seq4repair_space>for<seq2seq4repair_space>Search" ) ; builder . field ( "cover" , "DON'T<seq2seq4repair_space>PANIC" ) ; if ( ( rootNode ) != null ) { builder . startObject ( "quote" ) ; ArrayNode arrayNode = ( ( ArrayNode ) ( rootNode . get ( "quotes" ) ) ) ; JsonNode quoteNode = arrayNode . get ( ThreadLocalRandom . current ( ) . nextInt ( quotesSize ) ) ; builder . field ( "book" , quoteNode . get ( "book" ) . getValueAsText ( ) ) ; builder . field ( "chapter" , quoteNode . get ( "chapter" ) . getValueAsText ( ) ) ; ArrayNode textNodes = ( ( ArrayNode ) ( quoteNode . get ( "text" ) ) ) ; int index = 0 ; for ( JsonNode textNode : textNodes ) { builder . field ( ( "text" + ( ++ index ) ) , textNode . getValueAsText ( ) ) ; } builder . endObject ( ) ; } builder . endObject ( ) ; channel . sendResponse ( new JsonRestResponse ( request , Status . OK , builder ) ) ; } catch ( Exception e ) { try { channel . sendResponse ( new JsonThrowableRestResponse ( request , e ) ) ; } catch ( IOException e1 ) { logger . warn ( "Failed<seq2seq4repair_space>to<seq2seq4repair_space>send<seq2seq4repair_space>response" , e ) ; } } } }
public class TransportPutMappingAction extends TransportMasterNodeOperationAction < PutMappingRequest , PutMappingResponse > { private final MetaDataService metaDataService ; private final TransportCreateIndexAction createIndexAction ; private final boolean autoCreateIndex ; @ Inject public TransportPutMappingAction ( Settings settings , TransportService transportService , ClusterService clusterService , ThreadPool threadPool , MetaDataService metaDataService , TransportCreateIndexAction createIndexAction ) { } @ Override protected String transportAction ( ) { } @ Override protected PutMappingRequest newRequest ( ) { } @ Override protected PutMappingResponse newResponse ( ) { } @ Override protected PutMappingResponse masterOperation ( PutMappingRequest request ) throws ElasticSearchException { final String [ ] indices = processIndices ( clusterService . state ( ) , request . indices ( ) ) ; <START_BUG> MetaDataService . PutMappingResult result = metaDataService . putMapping ( indices , request . type ( ) , request . mappingSource ( ) , request . ignoreDuplicates ( ) , request . timeout ( ) ) ; <END_BUG> return new PutMappingResponse ( result . acknowledged ( ) ) ; } @ Override protected void doExecute ( final PutMappingRequest request , final ActionListener < PutMappingResponse > listener ) { } }
public class BufferUtils { static Array < ByteBuffer > unsafeBuffers = new Array < ByteBuffer > ( ) ; static int allocatedUnsafe = 0 ; public static void copy ( float [ ] src , Buffer dst , int numFloats , int offset ) { } public static void copy ( byte [ ] src , int srcOffset , Buffer dst , int numElements ) { } public static void copy ( short [ ] src , int srcOffset , Buffer dst , int numElements ) { } public static void copy ( char [ ] src , int srcOffset , Buffer dst , int numElements ) { } public static void copy ( int [ ] src , int srcOffset , Buffer dst , int numElements ) { } public static void copy ( long [ ] src , int srcOffset , Buffer dst , int numElements ) { } public static void copy ( float [ ] src , int srcOffset , Buffer dst , int numElements ) { } public static void copy ( double [ ] src , int srcOffset , Buffer dst , int numElements ) { } public static void copy ( Buffer src , Buffer dst , int numElements ) { } private static int positionInBytes ( Buffer dst ) { } private static int bytesToElements ( Buffer dst , int bytes ) { } private static int elementsToBytes ( Buffer dst , int elements ) { } public static FloatBuffer newFloatBuffer ( int numFloats ) { } public static DoubleBuffer newDoubleBuffer ( int numDoubles ) { } public static ByteBuffer newByteBuffer ( int numBytes ) { } public static ShortBuffer newShortBuffer ( int numShorts ) { } public static CharBuffer newCharBuffer ( int numChars ) { } public static IntBuffer newIntBuffer ( int numInts ) { } public static LongBuffer newLongBuffer ( int numLongs ) { } public static void disposeUnsafeByteBuffer ( ByteBuffer buffer ) { <START_BUG> int size = buffer . capacity ( ) ; <END_BUG> synchronized ( BufferUtils . unsafeBuffers ) { if ( ! ( BufferUtils . unsafeBuffers . removeValue ( buffer , true ) ) ) throw new IllegalArgumentException ( "buffer<seq2seq4repair_space>not<seq2seq4repair_space>allocated<seq2seq4repair_space>with<seq2seq4repair_space>newUnsafeByteBuffer<seq2seq4repair_space>or<seq2seq4repair_space>already<seq2seq4repair_space>disposed" ) ; } BufferUtils . allocatedUnsafe -= size ; BufferUtils . freeMemory ( buffer ) ; } public static ByteBuffer newUnsafeByteBuffer ( int numBytes ) { } public static int getAllocatedBytesUnsafe ( ) { } private static native void freeMemory ( ByteBuffer buffer ) { } private static native ByteBuffer newDisposableByteBuffer ( int numBytes ) { } public static native void clear ( ByteBuffer buffer , int numBytes ) { } private static native void copyJni ( float [ ] src , Buffer dst , int numFloats , int offset ) { } private static native void copyJni ( byte [ ] src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } private static native void copyJni ( char [ ] src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } private static native void copyJni ( short [ ] src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } private static native void copyJni ( int [ ] src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } private static native void copyJni ( long [ ] src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } private static native void copyJni ( float [ ] src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } private static native void copyJni ( double [ ] src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } private static native void copyJni ( Buffer src , int srcOffset , Buffer dst , int dstOffset , int numBytes ) { } }
