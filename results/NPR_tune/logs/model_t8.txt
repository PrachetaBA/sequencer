[2019-11-28 01:17:10,032 INFO]  * src vocab size = 3004
[2019-11-28 01:17:10,059 INFO]  * tgt vocab size = 3004
[2019-11-28 01:17:10,059 INFO] Building model...
[2019-11-28 01:17:15,151 INFO] NMTModel(
  (encoder): CNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(3004, 256, padding_idx=1)
        )
      )
    )
    (linear): Linear(in_features=256, out_features=500, bias=True)
    (cnn): StackedCNN(
      (layers): ModuleList(
        (0): GatedConv(
          (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
          (dropout): Dropout(p=0.22665518436479926)
        )
        (1): GatedConv(
          (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
          (dropout): Dropout(p=0.22665518436479926)
        )
        (2): GatedConv(
          (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
          (dropout): Dropout(p=0.22665518436479926)
        )
        (3): GatedConv(
          (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
          (dropout): Dropout(p=0.22665518436479926)
        )
        (4): GatedConv(
          (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
          (dropout): Dropout(p=0.22665518436479926)
        )
        (5): GatedConv(
          (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
          (dropout): Dropout(p=0.22665518436479926)
        )
        (6): GatedConv(
          (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
          (dropout): Dropout(p=0.22665518436479926)
        )
      )
    )
  )
  (decoder): CNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(3004, 256, padding_idx=1)
        )
      )
    )
    (linear): Linear(in_features=256, out_features=500, bias=True)
    (conv_layers): ModuleList(
      (0): GatedConv(
        (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1))
        (dropout): Dropout(p=0.22665518436479926)
      )
      (1): GatedConv(
        (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1))
        (dropout): Dropout(p=0.22665518436479926)
      )
      (2): GatedConv(
        (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1))
        (dropout): Dropout(p=0.22665518436479926)
      )
      (3): GatedConv(
        (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1))
        (dropout): Dropout(p=0.22665518436479926)
      )
      (4): GatedConv(
        (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1))
        (dropout): Dropout(p=0.22665518436479926)
      )
      (5): GatedConv(
        (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1))
        (dropout): Dropout(p=0.22665518436479926)
      )
      (6): GatedConv(
        (conv): WeightNormConv2d(500, 1000, kernel_size=(3, 1), stride=(1, 1))
        (dropout): Dropout(p=0.22665518436479926)
      )
    )
    (attn_layers): ModuleList(
      (0): ConvMultiStepAttention(
        (linear_in): Linear(in_features=500, out_features=500, bias=True)
      )
      (1): ConvMultiStepAttention(
        (linear_in): Linear(in_features=500, out_features=500, bias=True)
      )
      (2): ConvMultiStepAttention(
        (linear_in): Linear(in_features=500, out_features=500, bias=True)
      )
      (3): ConvMultiStepAttention(
        (linear_in): Linear(in_features=500, out_features=500, bias=True)
      )
      (4): ConvMultiStepAttention(
        (linear_in): Linear(in_features=500, out_features=500, bias=True)
      )
      (5): ConvMultiStepAttention(
        (linear_in): Linear(in_features=500, out_features=500, bias=True)
      )
      (6): ConvMultiStepAttention(
        (linear_in): Linear(in_features=500, out_features=500, bias=True)
      )
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=500, out_features=3004, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2019-11-28 01:17:15,153 INFO] encoder: 11411524
[2019-11-28 01:17:15,153 INFO] decoder: 14670028
[2019-11-28 01:17:15,153 INFO] * number of parameters: 26081552
[2019-11-28 01:17:15,249 INFO] Starting training on GPU: [0]
[2019-11-28 01:17:15,249 INFO] Start training loop and validate every 10000 steps...
[2019-11-28 01:17:15,250 INFO] Loading dataset from /home/mmotwani/myworkdir/sequencer/results/NPR_tune/models/model_p8.train.0.pt
[2019-11-28 01:17:24,329 INFO] number of examples: 33755
/home/mmotwani/anaconda3/envs/npr-py36/lib/python3.6/site-packages/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  var = torch.tensor(arr, dtype=self.dtype, device=device)
Traceback (most recent call last):
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/trainer.py", line 372, in _gradient_accumulation
    trunc_size=trunc_size)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/utils/loss.py", line 164, in __call__
    for shard in shards(shard_state, shard_size):
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/utils/loss.py", line 338, in shards
    torch.autograd.backward(inputs, grads)
  File "/home/mmotwani/anaconda3/envs/npr-py36/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.93 GiB total capacity; 2.02 GiB already allocated; 37.81 MiB free; 171.08 MiB cached)
[2019-11-28 01:17:28,114 INFO] At step 1, we removed a batch - accum 0
Traceback (most recent call last):
  File "train.py", line 6, in <module>
    main()
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/bin/train.py", line 200, in main
    train(opt)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/bin/train.py", line 86, in train
    single_main(opt, 0)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/train_single.py", line 143, in main
    valid_steps=opt.valid_steps)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/trainer.py", line 242, in train
    report_stats)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/trainer.py", line 360, in _gradient_accumulation
    outputs, attns = self.model(src, tgt, src_lengths, bptt=bptt)
  File "/home/mmotwani/anaconda3/envs/npr-py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/models/model.py", line 42, in forward
    enc_state, memory_bank, lengths = self.encoder(src, lengths)
  File "/home/mmotwani/anaconda3/envs/npr-py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/encoders/cnn_encoder.py", line 49, in forward
    out = self.cnn(emb_remap)
  File "/home/mmotwani/anaconda3/envs/npr-py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/utils/cnn_factory.py", line 52, in forward
    x = x + conv(x)
  File "/home/mmotwani/anaconda3/envs/npr-py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/utils/cnn_factory.py", line 31, in forward
    x_var = self.conv(x_var)
  File "/home/mmotwani/anaconda3/envs/npr-py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/mnt/nfs/work1/brun/mmotwani/sequencer/src/lib/OpenNMT-py/onmt/modules/weight_norm.py", line 168, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 11.93 GiB total capacity; 2.05 GiB already allocated; 78.81 MiB free; 78.16 MiB cached)
